{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hexo • All posts by \"大数据\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2024/05/01/hive_flink/",
            "url": "http://example.com/2024/05/01/hive_flink/",
            "title": "hive_flink",
            "date_published": "2024-05-01T05:38:45.000Z",
            "content_html": "<h2 id=\"hive\"><a class=\"markdownIt-Anchor\" href=\"#hive\">#</a> hive</h2>\n<p>Hive 是由 Facebook 开源，基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。</p>\n<p>Hive 是一个 Hadoop 客户端，用于将 HQL (HiveSQL) 转化成 MapfReduce 程序。</p>\n<p>(1) Hive 中每张表的数据存储在 HDFS</p>\n<p>(2) Hive 分析数据底层的实现是 MapReduce (也可配置为 Sparl 或者 Tez)</p>\n<p>(3) 执行程序运行在 Yarn 上</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224128675.png\" alt=\"image-20240804224128675\"></p>\n<h3 id=\"最小化安装\"><a class=\"markdownIt-Anchor\" href=\"#最小化安装\">#</a> 最小化安装</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http:<span class=\"comment\">//apache.mirror.iweb.ca/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz</span></span><br><span class=\"line\">tar xzvf apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin.tar.gz</span><br><span class=\"line\">cd apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/</span><br><span class=\"line\"></span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">export HIVE_HOME=/root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin</span><br><span class=\"line\">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class=\"line\"></span><br><span class=\"line\">./schematool -dbType derby -initSchema</span><br><span class=\"line\"></span><br><span class=\"line\">hive </span><br><span class=\"line\">&gt; show databases;</span><br></pre></td></tr></table></figure>\n<p>路径同一时刻，只能允许一个 derby 客户端使用</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL2FwYWNoZS5taXJyb3IuaXdlYi5jYS9oaXZlL2hpdmUtMy4xLjMv\">http://apache.mirror.iweb.ca/hive/hive-3.1.3/</span></p>\n<h3 id=\"mysql-hive\"><a class=\"markdownIt-Anchor\" href=\"#mysql-hive\">#</a> mysql-hive</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 卸载mariadb</span><br><span class=\"line\"></span><br><span class=\"line\"># 安装mysql</span><br><span class=\"line\">apt install mysql-server</span><br><span class=\"line\">systemctl enable mysql --now</span><br><span class=\"line\">sudo mysql_secure_installation</span><br><span class=\"line\">ALTER USER <span class=\"string\">&#x27;root&#x27;</span>@<span class=\"string\">&#x27;localhost&#x27;</span> IDENTIFIED BY <span class=\"string\">&#x27;123456&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># hive存储元数据到mysql</span><br><span class=\"line\">create database metastore;</span><br><span class=\"line\"></span><br><span class=\"line\"># 修改配置文件 hive-site.xml</span><br><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;jdbc:mysql:<span class=\"comment\">//hadoop100:3306/metastore?useSSL=false&lt;/value&gt;</span></span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      JDBC connect string <span class=\"keyword\">for</span> a JDBC metastore.</span><br><span class=\"line\">      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class=\"line\">      For example, jdbc:postgresql:<span class=\"comment\">//myhost/db?ssl=true for postgres database.</span></span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt; </span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;root&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">123456</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Driver <span class=\"keyword\">class</span> <span class=\"title class_\">name</span> <span class=\"keyword\">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;location of <span class=\"keyword\">default</span> database <span class=\"keyword\">for</span> the warehouse&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/lib/log4j-slf4j-impl-<span class=\"number\">2.17</span><span class=\"number\">.1</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/share/hadoop/common/lib/slf4j-reload4j-<span class=\"number\">1.7</span><span class=\"number\">.36</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: See http:<span class=\"comment\">//www.slf4j.org/codes.html#multiple_bindings for an explanation.</span></span><br><span class=\"line\">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class=\"line\">Metastore connection URL:        jdbc:mysql:<span class=\"comment\">//hadoop100:3306/metastore?useSSL=false</span></span><br><span class=\"line\">Metastore Connection Driver :    com.mysql.cj.jdbc.Driver</span><br><span class=\"line\">Metastore connection User:       root</span><br><span class=\"line\">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.</span><br><span class=\"line\">Underlying cause: com.mysql.cj.jdbc.exceptions.CommunicationsException : Communications link failure</span><br><span class=\"line\"></span><br><span class=\"line\">The last packet sent successfully to the server was <span class=\"number\">0</span> milliseconds ago. The driver has not received any packets from the server.</span><br><span class=\"line\">SQL Error code: <span class=\"number\">0</span></span><br><span class=\"line\">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.</span><br><span class=\"line\">        at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:<span class=\"number\">94</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:<span class=\"number\">169</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:<span class=\"number\">475</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:<span class=\"number\">581</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:<span class=\"number\">567</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:<span class=\"number\">1517</span>)</span><br><span class=\"line\">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class=\"line\">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class=\"number\">43</span>)</span><br><span class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:<span class=\"number\">498</span>)</span><br><span class=\"line\">        at org.apache.hadoop.util.RunJar.run(RunJar.java:<span class=\"number\">328</span>)</span><br><span class=\"line\">        at org.apache.hadoop.util.RunJar.main(RunJar.java:<span class=\"number\">241</span>)</span><br><span class=\"line\">Caused by: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure</span><br><span class=\"line\"></span><br><span class=\"line\">The last packet sent successfully to the server was <span class=\"number\">0</span> milliseconds ago. The driver has not received any packets from the server.</span><br><span class=\"line\">        at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:<span class=\"number\">175</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:<span class=\"number\">64</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:<span class=\"number\">825</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:<span class=\"number\">446</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:<span class=\"number\">239</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:<span class=\"number\">188</span>)</span><br><span class=\"line\">        at java.sql.DriverManager.getConnection(DriverManager.java:<span class=\"number\">664</span>)</span><br><span class=\"line\">        at java.sql.DriverManager.getConnection(DriverManager.java:<span class=\"number\">247</span>)</span><br><span class=\"line\">        at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:<span class=\"number\">88</span>)</span><br><span class=\"line\">        ... <span class=\"number\">11</span> more</span><br><span class=\"line\">Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure</span><br><span class=\"line\"></span><br><span class=\"line\">The last packet sent successfully to the server was <span class=\"number\">0</span> milliseconds ago. The driver has not received any packets from the server.</span><br><span class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class=\"number\">45</span>)</span><br><span class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class=\"number\">423</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:<span class=\"number\">105</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:<span class=\"number\">150</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:<span class=\"number\">166</span>)</span><br><span class=\"line\">        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:<span class=\"number\">89</span>)</span><br><span class=\"line\">        at com.mysql.cj.NativeSession.connect(NativeSession.java:<span class=\"number\">121</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:<span class=\"number\">945</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:<span class=\"number\">815</span>)</span><br><span class=\"line\">        ... <span class=\"number\">17</span> more</span><br><span class=\"line\">Caused by: java.net.ConnectException: Connection <span class=\"title function_\">refused</span> <span class=\"params\">(Connection refused)</span></span><br><span class=\"line\">        at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class=\"line\">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:<span class=\"number\">350</span>)</span><br><span class=\"line\">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:<span class=\"number\">206</span>)</span><br><span class=\"line\">        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:<span class=\"number\">188</span>)</span><br><span class=\"line\">        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:<span class=\"number\">392</span>)</span><br><span class=\"line\">        at java.net.Socket.connect(Socket.java:<span class=\"number\">607</span>)</span><br><span class=\"line\">        at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:<span class=\"number\">153</span>)</span><br><span class=\"line\">        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:<span class=\"number\">63</span>)</span><br><span class=\"line\">        ... <span class=\"number\">20</span> more</span><br><span class=\"line\">*** schemaTool failed ***</span><br><span class=\"line\">如果报错，注意，当前用户不是localhost</span><br><span class=\"line\">mysql&gt; use mysql;</span><br><span class=\"line\">Reading table information <span class=\"keyword\">for</span> completion of table and column names</span><br><span class=\"line\">You can turn off <span class=\"built_in\">this</span> feature to get a quicker startup with -A</span><br><span class=\"line\"></span><br><span class=\"line\">Database changed</span><br><span class=\"line\">mysql&gt; select host,user from user where user=<span class=\"string\">&#x27;root&#x27;</span>;</span><br><span class=\"line\">+-----------+------+</span><br><span class=\"line\">| host      | user |</span><br><span class=\"line\">+-----------+------+</span><br><span class=\"line\">| localhost | root |</span><br><span class=\"line\">+-----------+------+</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">配置文件中不是<span class=\"number\">127.0</span><span class=\"number\">.0</span><span class=\"number\">.1</span></span><br><span class=\"line\">通常情况下，这个配置文件位于/etc/mysql/mysql.conf.d/mysqld.cnf，找到bind-address这一项并将其更改为<span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span>。然后重启mysql服务：sudo systemctl restart mysql。</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">配置文件中账号密码正确</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">已经下载mysql连接java驱动，并且复制到hive/lib下面</span><br><span class=\"line\">https:<span class=\"comment\">//downloads.mysql.com/archives/c-j/</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Relative path in absolute URI: $&#123;system:java.io.tmpdir%<span class=\"number\">7D</span>/$%7Bsystem:user.name%<span class=\"number\">7D</span></span><br><span class=\"line\">https:<span class=\"comment\">//www.cnblogs.com/qxyy/articles/5247933.html</span></span><br><span class=\"line\">把上面文章中所有 $HIVE_HOME/iotmp 改为绝对路径，比如 /root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/iotmp</span><br><span class=\"line\">验证：</span><br><span class=\"line\">hive</span><br><span class=\"line\">show databases；</span><br></pre></td></tr></table></figure>\n<h3 id=\"hiveserver2\"><a class=\"markdownIt-Anchor\" href=\"#hiveserver2\">#</a> hiveserver2</h3>\n<p>Hive 的 hiveserver2 服务的作用是提供 jdbc/odbc 接口，为用户提供远程访问 Hive 数据的功能，例如用户期望在个人电脑中访问远程服务中的 Hive 数据，就需要用到 Hiveserver2</p>\n<h4 id=\"用户模拟功能\"><a class=\"markdownIt-Anchor\" href=\"#用户模拟功能\">#</a> 用户模拟功能</h4>\n<p>在远程访问 Hive 数据时，客户端并未直接访问 Hadoop 集群，而是由 Hivesever2 代理访问。由于 Hadoop 集群中的数据具备访问权限控制，所以此时需考虑一个问题：那就是访问 Hadoop 集群的用户身份是谁？是 Hiveserver2 的启动用户？还是客户端的登录用户？</p>\n<p>答案是都有可能，具体是谁，由 Hiveserver2 的 hive.server2.enable.doAs 参数决定，该参数的含义是是否启用 Hiveserver2 用户模拟的功能。若启用，则 Hiveserver2 会模拟成客户端的登录用户去访问 Hadoop 集群的数据，不启用，则 Hivesever2 会直接使用启动用户访问 Hadoop 集群数据。模拟用户的功能，默认是开启的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim core-site.yaml</span><br><span class=\"line\">&lt;!--配置所有节点的atguigu用户都可作为代理用户--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;!--配置atguigu用户能够代理的用户组为任意组--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;!--配置atguigu用户能够代理的用户为任意用户--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hadoop.proxyuser.root.users&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">scp core-site.xml root@<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.191</span>:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/etc/hadoop</span><br><span class=\"line\">scp core-site.xml root@<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.192</span>:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/etc/hadoop</span><br><span class=\"line\">hadoop-start.sh</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim hive-site.yaml</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 指定hiveserver2连接的host --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class=\"line\">   &lt;value&gt;hadoop100&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class=\"line\">   &lt;value&gt;<span class=\"number\">10000</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">bin/hiveserver2</span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin# nohup hiveserver2 &gt;/dev/<span class=\"literal\">null</span> <span class=\"number\">2</span>&gt;&amp;<span class=\"number\">1</span>  &amp; </span><br><span class=\"line\">[<span class=\"number\">1</span>] <span class=\"number\">124804</span></span><br><span class=\"line\"></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin# jps</span><br><span class=\"line\"><span class=\"number\">123633</span> NameNode</span><br><span class=\"line\"><span class=\"number\">124131</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">124804</span> RunJar</span><br></pre></td></tr></table></figure>\n<p>测试</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin# beeline </span><br><span class=\"line\">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/lib/log4j-slf4j-impl-<span class=\"number\">2.17</span><span class=\"number\">.1</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/share/hadoop/common/lib/slf4j-reload4j-<span class=\"number\">1.7</span><span class=\"number\">.36</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: See http:<span class=\"comment\">//www.slf4j.org/codes.html#multiple_bindings for an explanation.</span></span><br><span class=\"line\">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class=\"line\">Beeline version <span class=\"number\">3.1</span><span class=\"number\">.3</span> by Apache Hive</span><br><span class=\"line\">beeline&gt; !connect jdbc:hive2:<span class=\"comment\">//hadoop100:10000</span></span><br><span class=\"line\">Connecting to jdbc:hive2:<span class=\"comment\">//hadoop100:10000</span></span><br><span class=\"line\">Enter username <span class=\"keyword\">for</span> jdbc:hive2:<span class=\"comment\">//hadoop100:10000: root</span></span><br><span class=\"line\">Enter password <span class=\"keyword\">for</span> jdbc:hive2:<span class=\"comment\">//hadoop100:10000: </span></span><br><span class=\"line\">Connected to: Apache <span class=\"title function_\">Hive</span> <span class=\"params\">(version <span class=\"number\">3.1</span><span class=\"number\">.3</span>)</span></span><br><span class=\"line\">Driver: Hive <span class=\"title function_\">JDBC</span> <span class=\"params\">(version <span class=\"number\">3.1</span><span class=\"number\">.3</span>)</span></span><br><span class=\"line\">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class=\"line\"><span class=\"number\">0</span>: jdbc:hive2:<span class=\"comment\">//hadoop100:10000&gt; </span></span><br></pre></td></tr></table></figure>\n<p>使用 datagrip 连接</p>\n<h3 id=\"metastore\"><a class=\"markdownIt-Anchor\" href=\"#metastore\">#</a> metastore</h3>\n<p>Hive 的 metastore 服务的作用是为 Hive CLI 或者 Hiveserver2 提供元数据访问接口。</p>\n<p><strong>metastore 运行模式</strong></p>\n<p>metastore 有两种运行模式，分别为嵌入式模式和独立服务模式。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224157870.png\" alt=\"image-20240804224157870\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224212449.png\" alt=\"image-20240804224212449\"></p>\n<p>生产环境中，不推荐使用嵌入式模式。因为其存在以下两个问题：</p>\n<p>（1）嵌入式模式下，每个 Hive CLI 都需要直接连接元数据库，当 Hive CLI 较多时，数据库压力会比较大。</p>\n<p>（2）每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证。</p>\n<h4 id=\"独立模式部署\"><a class=\"markdownIt-Anchor\" href=\"#独立模式部署\">#</a> 独立模式部署</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hadoop100</span><br><span class=\"line\">nohup hive --service metastore &amp;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Hadoop101</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;thrift:<span class=\"comment\">//hadoop100:9083 &lt;/value&gt;</span></span><br><span class=\"line\">    &lt;description/&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">需要删除jdbc相关参数</span><br><span class=\"line\">hive -e <span class=\"string\">&quot;insert into stu values(2,&#x27;aaa&#x27;)&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">hive -f stu.sql</span><br><span class=\"line\"></span><br><span class=\"line\">hive -hiveconf mapreduce.job.x=<span class=\"number\">10</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">hive&gt; set mapreduce.x.x=<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">hive&gt; set mapreduce.x.x </span><br></pre></td></tr></table></figure>\n<p>配置文件 &lt; 命令行参数 &lt; 参数声明。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Whether to print the names of the columns in query output.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Whether to include the current database in the Hive prompt.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">修改日志路径</span><br><span class=\"line\"></span><br><span class=\"line\">vim hive-log4j2.properties</span><br><span class=\"line\"># list of properties</span><br><span class=\"line\">property.hive.log.level = INFO</span><br><span class=\"line\">property.hive.root.logger = DRFA</span><br><span class=\"line\">property.hive.log.dir = $&#123;sys:java.io.tmpdir&#125;/$&#123;sys:user.name&#125;</span><br><span class=\"line\">property.hive.log.file = hive.log</span><br><span class=\"line\">property.hive.perflogger.log.level = INFO</span><br><span class=\"line\"># 调整堆内存</span><br><span class=\"line\"></span><br><span class=\"line\">vim hive-env.sh</span><br><span class=\"line\">export HADOOP_HEAPSIZE=<span class=\"number\">2048</span></span><br><span class=\"line\">在yarn-site.xml中关闭虚拟内存检查(虚拟内存校验</span><br><span class=\"line\"></span><br><span class=\"line\">yarn-nodemanager.vmem-check-enabled </span><br><span class=\"line\"><span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"ddl\"><a class=\"markdownIt-Anchor\" href=\"#ddl\">#</a> DDL</h3>\n<h4 id=\"库操作\"><a class=\"markdownIt-Anchor\" href=\"#库操作\">#</a> 库操作</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">### databases</span><br><span class=\"line\">（<span class=\"number\">1</span>）创建一个数据库，不指定路径</span><br><span class=\"line\">hive (<span class=\"keyword\">default</span>)&gt; create database db_hive1;</span><br><span class=\"line\">注：若不指定路径，其默认路径为$&#123;hive.metastore.warehouse.dir&#125;/database_name.db</span><br><span class=\"line\">（<span class=\"number\">2</span>）创建一个数据库，指定路径</span><br><span class=\"line\">hive (<span class=\"keyword\">default</span>)&gt; create database db_hive2 location <span class=\"string\">&#x27;/db_hive2&#x27;</span>;</span><br><span class=\"line\">（<span class=\"number\">2</span>）创建一个数据库，带有dbproperties</span><br><span class=\"line\"><span class=\"title function_\">hive</span> <span class=\"params\">(<span class=\"keyword\">default</span>)</span>&gt; create database db_hive3 with <span class=\"title function_\">dbproperties</span><span class=\"params\">(<span class=\"string\">&#x27;create_date&#x27;</span>=<span class=\"string\">&#x27;2022-11-18&#x27;</span>)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">show databases like <span class=\"string\">&#x27;db_hive*&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">desc database db_hive3;</span><br><span class=\"line\">desc database extended db_hive3;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录。</span><br><span class=\"line\">ALTER DATABASE db_hive3 SET <span class=\"title function_\">DBPROPERTIES</span> <span class=\"params\">(<span class=\"string\">&#x27;create_date&#x27;</span>=<span class=\"string\">&#x27;2022-11-20&#x27;</span>)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE];</span><br><span class=\"line\">注：RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。</span><br><span class=\"line\">    CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。</span><br><span class=\"line\">    </span><br><span class=\"line\">use db1;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">### tables</span><br></pre></td></tr></table></figure>\n<h4 id=\"建表\"><a class=\"markdownIt-Anchor\" href=\"#建表\">#</a> 建表</h4>\n<p><strong>1TEMPORARY</strong></p>\n<p>临时表，该表只在当前会话可见，会话结束，表会被删除。</p>\n<p><strong>2EXTERNAL（重点）</strong></p>\n<p>外部表，与之相对应的是内部表（管理表）。管理表意味着 Hive 会完全接管该表，包括元数据和 HDFS 中的数据。而外部表则意味着 Hive 只接管元数据，而不完全接管 HDFS 中的数据。</p>\n<p><strong>3data_type（重点）</strong></p>\n<p>Hive 中的字段类型可分为基本数据类型和复杂数据类型。</p>\n<p>基本数据类型如下：</p>\n<table>\n<thead>\n<tr>\n<th>Hive</th>\n<th>说明</th>\n<th>定义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>tinyint</td>\n<td>1byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>smallint</td>\n<td>2byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>int</td>\n<td>4byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>bigint</td>\n<td>8byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>boolean</td>\n<td>布尔类型，true 或者 false</td>\n<td></td>\n</tr>\n<tr>\n<td>float</td>\n<td>单精度浮点数</td>\n<td></td>\n</tr>\n<tr>\n<td>double</td>\n<td>双精度浮点数</td>\n<td></td>\n</tr>\n<tr>\n<td>decimal</td>\n<td>十进制精准数字类型</td>\n<td>decimal(16,2)</td>\n</tr>\n<tr>\n<td>varchar</td>\n<td>字符序列，需指定最大长度，最大长度的范围是 [1,65535]</td>\n<td>varchar(32)</td>\n</tr>\n<tr>\n<td>string</td>\n<td>字符串，无需指定最大长度</td>\n<td></td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>时间类型</td>\n<td></td>\n</tr>\n<tr>\n<td>binary</td>\n<td>二进制数据</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>方式一：隐式转换</strong></p>\n<p>具体规则如下：</p>\n<p>a. 任何整数类型都可以隐式地转换为一个范围更广的类型，如 tinyint 可以转换成 int，int 可以转换成 bigint。</p>\n<p>b. 所有整数类型、float 和 string 类型都可以隐式地转换成 double。</p>\n<p>c. tinyint、smallint、int 都可以转换为 float。</p>\n<p>d. boolean 类型不可以转换为任何其它的类型。</p>\n<p><strong>方式二：显示转换</strong></p>\n<p>可以借助 cast 函数完成显示的类型转换</p>\n<p>a. 语法</p>\n<p>cast(expr as <type>)</p>\n<p>b. 案例</p>\n<p>hive (default)&gt; select ‘1’ + 2, cast(‘1’ as int) + 2;</p>\n<p>数据仓库 (英语：Data Warehouse, 简称数仓、DW), 是一个月用于存储、分析、报告的数据系统。</p>\n<p>数据仓库的目的是构建面向分析的集成化数据环境，分析结果为企业提供决策支持 (Decision Support)。</p>\n<p>关系型数据库 (RDBMS) 是 OLTP 典型应用，比如：Oracle、MySQL、SQL Server 等。</p>\n<p>面向分析、支持分析的系统称之为 OLAP (联机分析处理) 系统。数据仓库是 OLAP 一种。</p>\n<p>联机事务处理 OLTP (On-Line Transaction Processing)。</p>\n<p>传统 RDBMS，mysql，oracle，sqlserver</p>\n<p>联机分析处理 OLAP (On-LineAnalytical Processing)。</p>\n<p>数据仓库，主要用于开展数据分析</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224237355.png\" alt=\"image-20240804224237355\"></p>\n<p>数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。</p>\n<p>OLTP 系统的典型应用就是 RDBMS, 也就是我们俗称的数据库，当然这里要特别强调此数据库表示的是关系型数据库，</p>\n<p>Nosql 数据库并不在讨论范围内。</p>\n<p>OLAP 系统的典型应用就是 DW, 也就是我们俗称的数据仓库。</p>\n<p>数据仓库，数据集市</p>\n<p>数据仓库 (Data Warehouse) 是面向整个集团组织的数据，数据集市 (Data Mart) 是面向单个部门使用的。</p>\n<p>可以认为数据集市是数据仓库的子集，也有人把数据集市叫做小型数据仓库。数据集市通常只涉及一个主题领域，例如市场营销或销售。因为它们较小且更具体，所以它们通常更易于管理和维护，并具有更灵活的结构。</p>\n<p>下图中，各种操作型系统数据和包括文件在内的等其他数据作为数据源，经过 ETL (抽取转换加载) 填充到数据仓库中；数据仓库中有不同主题数据，数据集市则根据部门特点面向指定主题，比如 Purchasing (采购)、Sales 售)、Inventory (库存);</p>\n<p>用户可以基于主题数据开展各种应用：数据分析、数据报表、数据挖掘。</p>\n<p>数据仓库分层思想</p>\n<p>ETL ELT</p>\n<h2 id=\"flink\"><a class=\"markdownIt-Anchor\" href=\"#flink\">#</a> flink</h2>\n<p>Apache Flink 是一个框架和分布式处理引擎，用于对无异和有界数据流进行有状态计算。</p>\n<p>1) 无界数据流:</p>\n<p>有定义流的开始，但没有定义流的结束；</p>\n<p>它们会无休止的产生数据；</p>\n<p>无界流的数据必须持续处理，即数据被摄取后需要立刻处上理。</p>\n<p>我们不能等到所有数据都到达再处理，因为输入是无限的。</p>\n<p>2) 有界数据流:</p>\n<p>有定义流的开始，也有定义流的结束；</p>\n<p>有界流可以在摄取所有数据后再进行计算；</p>\n<p>有界流所有数据可以被排序，所以并不需要有序摄取；</p>\n<p>有界流处理通常被称为批处理。</p>\n<p>把流处理需要的额外数据保存成一个 &quot;状态&quot;, 然户后针对这条数据进行处理，并且更新状态。这就是所谓的 &quot;有状态的流处理&quot;。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224253261.png\" alt=\"image-20240804224253261\"></p>\n<h3 id=\"wordcount\"><a class=\"markdownIt-Anchor\" href=\"#wordcount\">#</a> wordcount</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt;</span><br><span class=\"line\">        &lt;flink.version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/flink.version&gt;</span><br><span class=\"line\">&lt;/properties&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;flink-streaming-java&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">     &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;flink-clients&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class=\"line\">     &lt;/dependency&gt;</span><br><span class=\"line\">&lt;/dependencies&gt;</span><br><span class=\"line\"><span class=\"comment\">// 批处理</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.AggregateOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.FlatMapOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.UnsortedGrouping;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">count</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">ExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        DataSource&lt;String&gt; stringDataSource = env.readTextFile(<span class=\"string\">&quot;data/input&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        FlatMapOperator&lt;String, Tuple2&lt;String, Integer&gt;&gt; stringTuple2FlatMapOperator = stringDataSource.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">flatMap</span><span class=\"params\">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                String[] s1 = s.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (String ss : s1) &#123;</span><br><span class=\"line\">                    Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = Tuple2.of(ss, <span class=\"number\">1</span>);</span><br><span class=\"line\">                    collector.collect(stringIntegerTuple2);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        UnsortedGrouping&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2UnsortedGrouping = stringTuple2FlatMapOperator.groupBy(<span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2UnsortedGrouping.sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 流处理</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.KeyedStream;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">count2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">executionEnvironment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.readTextFile(<span class=\"string\">&quot;data/input&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2SingleOutputStreamOperator = stringDataStreamSource.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">flatMap</span><span class=\"params\">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                String[] s1 = s.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (String string : s1) &#123;</span><br><span class=\"line\">                    Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = Tuple2.of(string, <span class=\"number\">1</span>);</span><br><span class=\"line\">                    collector.collect(stringIntegerTuple2);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2StringKeyedStream = tuple2SingleOutputStreamOperator.keyBy(<span class=\"keyword\">new</span> <span class=\"title class_\">KeySelector</span>&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">getKey</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> stringIntegerTuple2.f0;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2StringKeyedStream.sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\">        </span><br><span class=\"line\">        executionEnvironment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// socket</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"flink-部署\"><a class=\"markdownIt-Anchor\" href=\"#flink-部署\">#</a> flink 部署</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">客户端(Client):代码由客户端获取并做转换,之后提交给JolManger</span><br><span class=\"line\">JobManager就是Flink集群里的<span class=\"string\">&quot;管事人&quot;</span>,对作业进行中央调度管理;而它获取到要执行的作业后,会进一步处理转换,然后分发任务给众多的TaskManager。</span><br><span class=\"line\">TaskManager,就是真正<span class=\"string\">&quot;干活的人&quot;</span>,数据的处理操作都是它它们来做的。</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224309842.png\" alt=\"image-20240804224309842\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget <span class=\"string\">&quot;https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz&quot;</span></span><br><span class=\"line\">tar xzvf flink-<span class=\"number\">1.17</span><span class=\"number\">.2</span>-bin-scala_2<span class=\"number\">.12</span>.tgz</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim flink-conf.yaml</span><br><span class=\"line\">jobmanager.rpc.address: hadoop100</span><br><span class=\"line\">jobmanager.rpc.port: <span class=\"number\">6123</span></span><br><span class=\"line\">jobmanager.bind-host: <span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">rest.address: hadoop100</span><br><span class=\"line\">rest.bind-address: <span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">taskmanager.bind-host: <span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">taskmanager.host: hadoop100</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim worker</span><br><span class=\"line\">hadoop100</span><br><span class=\"line\">hadoop101</span><br><span class=\"line\">hadoop102</span><br><span class=\"line\"></span><br><span class=\"line\">vim master</span><br><span class=\"line\">hadoop100:<span class=\"number\">8081</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">分发到其他节点，修改flink-conf</span><br><span class=\"line\"></span><br><span class=\"line\">taskmanager.host: hadoop10x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 启动</span><br><span class=\"line\"> start-cluster.sh </span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# jps </span><br><span class=\"line\"><span class=\"number\">140450</span> StandaloneSessionClusterEntrypoint</span><br><span class=\"line\"><span class=\"number\">140861</span> TaskManagerRunner</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">http:<span class=\"comment\">//hadoop100:8081/#/overview</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"web提交任务\"><a class=\"markdownIt-Anchor\" href=\"#web提交任务\">#</a> web 提交任务</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Arrays;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SocketStreamWordCount</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 1. 创建流式执行环境</span></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 2. 读取文本流：hadoop102表示发送端主机名、7777表示端口号</span></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; lineStream = env.socketTextStream(<span class=\"string\">&quot;hadoop100&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 3. 转换、分组、求和，得到统计结果</span></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = lineStream.flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">                    String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                        out.collect(Tuple2.of(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;).returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class=\"line\">                .keyBy(data -&gt; data.f0)</span><br><span class=\"line\">                .sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 4. 打印</span></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 5. 执行</span></span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"># 增加依赖</span><br><span class=\"line\"> &lt;build&gt;</span><br><span class=\"line\">        &lt;plugins&gt;</span><br><span class=\"line\">            &lt;plugin&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;<span class=\"number\">3.2</span><span class=\"number\">.4</span>&lt;/version&gt;</span><br><span class=\"line\">                &lt;executions&gt;</span><br><span class=\"line\">                    &lt;execution&gt;</span><br><span class=\"line\">                        &lt;phase&gt;<span class=\"keyword\">package</span>&lt;/phase&gt;</span><br><span class=\"line\">                        &lt;goals&gt;</span><br><span class=\"line\">                            &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class=\"line\">                        &lt;/goals&gt;</span><br><span class=\"line\">                        &lt;configuration&gt;</span><br><span class=\"line\">                            &lt;artifactSet&gt;</span><br><span class=\"line\">                                &lt;excludes&gt;</span><br><span class=\"line\">                                    &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt;</span><br><span class=\"line\">                                    &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt;</span><br><span class=\"line\">                                    &lt;exclude&gt;log4j:*&lt;/exclude&gt;</span><br><span class=\"line\">                                &lt;/excludes&gt;</span><br><span class=\"line\">                            &lt;/artifactSet&gt;</span><br><span class=\"line\">                            &lt;filters&gt;</span><br><span class=\"line\">                                &lt;filter&gt;</span><br><span class=\"line\">                                    &lt;!-- Do not copy the signatures in the META-INF folder.</span><br><span class=\"line\">                                    Otherwise, <span class=\"built_in\">this</span> might cause SecurityExceptions when using the JAR. --&gt;</span><br><span class=\"line\">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class=\"line\">                                    &lt;excludes&gt;</span><br><span class=\"line\">                                        &lt;exclude&gt;META-INF<span class=\"comment\">/*.SF&lt;/exclude&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                    &lt;/excludes&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                &lt;/filter&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                            &lt;/filters&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                            &lt;transformers combine.children=&quot;append&quot;&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                &lt;transformer</span></span><br><span class=\"line\"><span class=\"comment\">                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                &lt;/transformer&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                            &lt;/transformers&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                        &lt;/configuration&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                    &lt;/execution&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                &lt;/executions&gt;</span></span><br><span class=\"line\"><span class=\"comment\">            &lt;/plugin&gt;</span></span><br><span class=\"line\"><span class=\"comment\">        &lt;/plugins&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;/build&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  可以在前面的依赖中添加</span><br><span class=\"line\">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class=\"line\">  来不打包依赖</span><br><span class=\"line\">  </span><br><span class=\"line\">  添加后需要在edit configuration中include provided，否则idea中无法编译</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224322601.png\" alt=\"image-20240804224322601\"></p>\n<p>创建任务，submit</p>\n<p>nc -lvvp 7777 发送数据查看效果</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224334662.png\" alt=\"image-20240804224334662\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224343308.png\" alt=\"image-20240804224343308\"></p>\n<p>命令行提交作业</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run -m hadoop100:<span class=\"number\">8081</span> -c wordcount.SocketStreamWordCount ./xxx.jar</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署模式\"><a class=\"markdownIt-Anchor\" href=\"#部署模式\">#</a> 部署模式</h3>\n<p>Flink 为各种场景提供了不同的部署模式，主要有以下三种：会话模式（Session Mode）、单作业模式（Per-Job Mode）、应用模式（Application Mode）。</p>\n<p>它们的区别主要在于：集群的生命周期以及资源的分配方式；以及应用的 main 方法到底在哪里执行 —— 客户端（Client）还是 JobManager。</p>\n<h4 id=\"会话模式\"><a class=\"markdownIt-Anchor\" href=\"#会话模式\">#</a> 会话模式</h4>\n<p>会话模式其实最符合常规思维。我们需要先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源就都已经确定，所以所有提交的作业会竞争集群中的资源。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224355062.png\" alt=\"image-20240804224355062\"></p>\n<h4 id=\"单作业模式\"><a class=\"markdownIt-Anchor\" href=\"#单作业模式\">#</a> 单作业模式</h4>\n<p>会话模式因为资源共享会导致很多问题，所以为了更好地隔离资源，我们可以考虑为每个提交的作业启动一个集群，这就是所谓的单作业 (Per-Job) 模式。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224407159.png\" alt=\"image-20240804224407159\"></p>\n<p>需要注意的是，Flink 本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理框架来启动集群，比如 YARN、Kubernetes (K8S)。</p>\n<h4 id=\"应用模式\"><a class=\"markdownIt-Anchor\" href=\"#应用模式\">#</a> 应用模式</h4>\n<p>直接把应用提交到 JobManger 上运行。而这也就代表着，我们需要为每一个提交的应用单独启动一个 JobManager, 也就是创建一个集群。这个 JobManager 只为执行这一个应用而存在，执行结束之后 JobManager 也就关闭了，这就是所谓的应用模式。</p>\n<p>应用模式与单作业模式，都是提交作业之后才创建集群；单单作业模式是通过客户端来提交的，客户端解析出的每一个作业对应一个集群；而应用模式下，是直接由 JobManager 执行应用程序的。</p>\n<h4 id=\"standlone-模式\"><a class=\"markdownIt-Anchor\" href=\"#standlone-模式\">#</a> standlone 模式</h4>\n<p>独立模式是独立运行的，不依赖任何外部的资源管理平台；当然独立也是有代价的：如果资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理。所以独立模式一般只用在开发测试或作业非常少的场景下。</p>\n<p>会话模式部署</p>\n<p>提前启动集群，并通过 Web 页面客户端提交任务（可以多个任务，但是集群资源固定）。</p>\n<p>standlone 不支持单作业部署</p>\n<p>应用模式部署</p>\n<p>启动 jobmanager</p>\n<p>bin/standalone-job.sh start --job-classname com.atguigu.wc.SocketStreamWordCount</p>\n<p>启动 TaskManager, 需要在每个 taskmanager 上启动</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3Rhc2ttYW5hZ2VyLnNo\">taskmanager.sh</span> start</p>\n<p>停止</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3Rhc2ttYW5hZ2VyLnNo\">taskmanager.sh</span> stop</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3N0YW5kYWxvbmUtam9iLnNo\">standalone-job.sh</span> stop</p>\n<h4 id=\"yarn模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn模式\">#</a> yarn 模式</h4>\n<p>客户端把 Flink 应用提交给 Yarn 的 ResourceManager，Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署 JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配 TaskManager 资源。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim /etc/profile</span><br><span class=\"line\"># hadoop</span><br><span class=\"line\">export HADOOP_HOME=/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span></span><br><span class=\"line\">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class=\"line\">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class=\"line\">export HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure>\n<h5 id=\"yarn-会话模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn-会话模式\">#</a> yarn - 会话模式</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yarn-session.sh -d -nm name111</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 提交任务</span><br><span class=\"line\">flink run -c com.wordcount lib/<span class=\"number\">1.</span>jar </span><br></pre></td></tr></table></figure>\n<h5 id=\"yarn-单作业模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn-单作业模式\">#</a> yarn - 单作业模式</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run -d -t yarn-per-job -c com.wordcount lib/<span class=\"number\">1.</span>jar</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/// 停止</span></span><br><span class=\"line\">bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class=\"line\"> </span><br><span class=\"line\">bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br><span class=\"line\">消除classloader告警</span><br><span class=\"line\">vim flink-conf.yaml</span><br><span class=\"line\"> </span><br><span class=\"line\">classloader.check-leaked-classloader: <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<h5 id=\"yarn-应用模式部署\"><a class=\"markdownIt-Anchor\" href=\"#yarn-应用模式部署\">#</a> yarn - 应用模式部署</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-<span class=\"number\">1.0</span>-SNAPSHOT.jar </span><br><span class=\"line\">上传HDFS提交</span><br><span class=\"line\">可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。</span><br><span class=\"line\">（<span class=\"number\">1</span>）上传flink的lib和plugins到HDFS上</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -mkdir /flink-dist</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -put lib/ /flink-dist</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -put plugins/ /flink-dist</span><br><span class=\"line\">（<span class=\"number\">2</span>）上传自己的jar包到HDFS</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -mkdir /flink-jars</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -put FlinkTutorial-<span class=\"number\">1.0</span>-SNAPSHOT.jar /flink-jars</span><br><span class=\"line\">（<span class=\"number\">3</span>）提交作业</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ bin/flink run-application -t yarn-application        -Dyarn.provided.lib.dirs=<span class=\"string\">&quot;hdfs://hadoop102:8020/flink-dist&quot;</span>        -c com.atguigu.wc.SocketStreamWordCount  hdfs:<span class=\"comment\">//hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span></span><br></pre></td></tr></table></figure>\n<p>配置历史服务器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jobmanager.archive.fs.dir: hdfs:<span class=\"comment\">//hadoop100:8020/logs</span></span><br><span class=\"line\"></span><br><span class=\"line\"># The address under which the web-based HistoryServer listens.</span><br><span class=\"line\">historyserver.web.address: hadoop100</span><br><span class=\"line\"></span><br><span class=\"line\"># The port under which the web-based HistoryServer listens.</span><br><span class=\"line\">historyserver.web.port: <span class=\"number\">8082</span></span><br><span class=\"line\"></span><br><span class=\"line\"># Comma separated list of directories to monitor <span class=\"keyword\">for</span> completed jobs.</span><br><span class=\"line\">historyserver.archive.fs.dir: hdfs:<span class=\"comment\">//hadoop100:8020/logs</span></span><br><span class=\"line\"></span><br><span class=\"line\"># Interval in milliseconds <span class=\"keyword\">for</span> refreshing the monitored directories.</span><br><span class=\"line\">historyserver.archive.fs.refresh-interval: <span class=\"number\">10000</span></span><br><span class=\"line\"></span><br><span class=\"line\">historyserver.sh start </span><br></pre></td></tr></table></figure>\n<h3 id=\"系统架构\"><a class=\"markdownIt-Anchor\" href=\"#系统架构\">#</a> 系统架构</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224432790.png\" alt=\"image-20240804224432790\"></p>\n<h4 id=\"并行度\"><a class=\"markdownIt-Anchor\" href=\"#并行度\">#</a> 并行度</h4>\n<p>一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。这样，包含并行子任务的数据流，就是并行数据流，它需要多个分区（stream partition）来分配并行任务。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224445616.png\" alt=\"image-20240804224445616\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;flink-runtime-web&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SocketStreamWordCount</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 1. 创建流式执行环境</span></span><br><span class=\"line\">        <span class=\"comment\">//StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>());</span><br><span class=\"line\">        <span class=\"comment\">// 2. 读取文本流：hadoop102表示发送端主机名、7777表示端口号</span></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; lineStream = env.socketTextStream(<span class=\"string\">&quot;hadoop100&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 3. 转换、分组、求和，得到统计结果</span></span><br><span class=\"line\">        elineStream.flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">                    String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                        out.collect(Tuple2.of(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;).setParallelism(<span class=\"number\">2</span>)</span><br><span class=\"line\">                .returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class=\"line\">                .keyBy(data -&gt; data.f0)</span><br><span class=\"line\">                .sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 4. 打印</span></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 5. 执行</span></span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// idea中，不指定并行度，默认就是电脑的线程数</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 全局设置并行度</span></span><br><span class=\"line\">        env.setParallelism(<span class=\"number\">3</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">算子优先级更高 &gt;  env   &gt;  -p  &gt; flink-conf</span><br><span class=\"line\"></span><br><span class=\"line\">提交作业时指定 命令行启动 参数 -p <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">配置文件</span><br><span class=\"line\">flink-conf.yaml</span><br><span class=\"line\">parallelism.<span class=\"keyword\">default</span>: <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"算子链\"><a class=\"markdownIt-Anchor\" href=\"#算子链\">#</a> 算子链</h4>\n<p><strong>（1）一对一（One-to-one，forwarding）</strong></p>\n<p>这种模式下，数据流维护着分区以及元素的顺序。比如图中的 source 和 map 算子，source 算子读取数据之后，可以直接发送给 map 算子做处理，它们之间不需要重新分区，也不需要调整数据的顺序。这就意味着 map 算子的子任务，看到的元素个数和顺序跟 source 算子的子任务产生的完全一样，保证着 “一对一” 的关系。map、filter、flatMap 等算子都是这种 one-to-one 的对应关系。这种关系类似于 Spark 中的窄依赖。</p>\n<p><strong>（2）重分区（Redistributing）</strong></p>\n<p>在这种模式下，数据流的分区会发生改变。比如图中的 map 和后面的 keyBy/window 算子之间，以及 keyBy/window 算子和 Sink 算子之间，都是这样的关系。</p>\n<p>每一个算子的子任务，会根据数据传输的策略，把数据发送到不同的下游目标任务。这些传输方式都会引起重分区的过程，这一过程类似于 Spark 中的 shuffle。</p>\n<p>在 Flink 中，并行度相同的一对一（one to one）算子操作，可以直接链接在一起形成一个 “大” 的任务（task）</p>\n<p>将算子链接成 task 是非常有效的优化：可以减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 全局禁用算子链</span><br><span class=\"line\">env.disableOperatorChaining();</span><br><span class=\"line\"></span><br><span class=\"line\"># 对某个算子禁用算子链</span><br><span class=\"line\">.sum(<span class=\"number\">1</span>).disableChaining();</span><br><span class=\"line\"></span><br><span class=\"line\">禁用算子链之后，只有这个算子相邻算子不能和自己链</span><br><span class=\"line\"></span><br><span class=\"line\"># 从某个算子开启新链条</span><br><span class=\"line\">.sum(<span class=\"number\">1</span>).startNewChain();</span><br><span class=\"line\">算子不与前面链，从当前开始正常链</span><br></pre></td></tr></table></figure>\n<h4 id=\"任务槽\"><a class=\"markdownIt-Anchor\" href=\"#任务槽\">#</a> 任务槽</h4>\n<p>每个任务槽（task slot）其实表示了 TaskManager 拥有计算资源的一个固定大小的子集。这些资源就是用来独立执行一个子任务的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 每个tm slot数，默认是<span class=\"number\">1</span></span><br><span class=\"line\">taskmanager.numberOfTaskSlots: <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>slot 目前仅仅用来隔离内存，不会涉及 CPU 的隔离。在具体应用时，可以将 slot 数量配置为机器的 CPU 核心数，尽量避免不同任务之间对 CPU 的竞争。这也是开发环境默认并行度设为机器 CPU 数量的原因。</p>\n<p>同一个 job 中，不同算子的子任务，才可以共享一个 slot，slot 内是同时在运行的</p>\n<p>属于同一个 slot 共享组，默认是 default</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224459615.png\" alt=\"image-20240804224459615\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.sum(<span class=\"number\">1</span>).slotSharingGroup(<span class=\"string\">&quot;aaa&quot;</span>);</span><br></pre></td></tr></table></figure>\n<p>slot 数量 与 并行度 的关系</p>\n<p>1) slot 是一种静态的概念，表示最大的并发上限</p>\n<p>并行度是一种动态的概念，表示实际运行 占用了几个</p>\n<p>2) 要求:slot 数量 &gt;=job 并行度 (算子最大并行度),job 才能运行</p>\n<p>yarn 模式是动态申请</p>\n<p>申请的 TM 数量 = job 并行度 / 每个 TM 的 slot 数，向上取整</p>\n<h4 id=\"作业提交流程\"><a class=\"markdownIt-Anchor\" href=\"#作业提交流程\">#</a> 作业提交流程</h4>\n<h5 id=\"standalone\"><a class=\"markdownIt-Anchor\" href=\"#standalone\">#</a> standalone</h5>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224512232.png\" alt=\"image-20240804224512232\"></p>\n<p>逻辑流图（StreamGraph）→ 作业图（JobGraph）→ 执行图（ExecutionGraph）→ 物理图（Physical Graph）</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224530055.png\" alt=\"image-20240804224530055\"></p>\n<p><img data-src=\"https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ODUxMzFkZWQ3OWQyMTFjMjhlZWY1NWM3MWQzNzk0YWZfVXM3M2gxRkdSeEFvMWQ4ZWJ6N1pscHplUFZFOTVJVWhfVG9rZW46VWR4WmJTUUJzb0c0MFp4eTdxeWNzVGhrbllnXzE3MjI3ODE5MDk6MTcyMjc4NTUwOV9WNA\" alt=\"img\"></p>\n<h5 id=\"yarn\"><a class=\"markdownIt-Anchor\" href=\"#yarn\">#</a> yarn</h5>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224551577.png\" alt=\"image-20240804224551577\"></p>\n<h4 id=\"datastream-api\"><a class=\"markdownIt-Anchor\" href=\"#datastream-api\">#</a> datastream api</h4>\n<h5 id=\"执行环境\"><a class=\"markdownIt-Anchor\" href=\"#执行环境\">#</a> 执行环境</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">Configuration</span> <span class=\"variable\">configuration</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">    configuration.set(RestOptions.BIND_PORT, <span class=\"string\">&quot;8082&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 自动识别环境是远程还是本地</span></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 流批一体，同一套API，默认STREAMING</span></span><br><span class=\"line\">    <span class=\"comment\">// 一般不在代码写死，再命令行参数指定 -Dexecution.runtime-mode=BATCH</span></span><br><span class=\"line\">    environment.setRuntimeMode(RuntimeExecutionMode.STREAMING);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = environment.socketTextStream(<span class=\"string\">&quot;127.0.0.1&quot;</span>, <span class=\"number\">7777</span>).flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">                String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                    out.collect(Tuple2.of(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;)</span><br><span class=\"line\">            .returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class=\"line\">            .keyBy(data -&gt; data.f0)</span><br><span class=\"line\">            .sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 触发执行 flink job</span></span><br><span class=\"line\">    <span class=\"comment\">// Flink是由事件驱动的,只有等等到数据到来,才会触发真正的计算,这也被称为&quot;延迟执行&quot;或&quot;懒执行&quot;。</span></span><br><span class=\"line\">    <span class=\"comment\">// 一个main调用多个exec，会在第一个阻塞住</span></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 异步执行，不会阻塞，execsync个数 = 生成flink job数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// yarn-application集群,提交一次,集群里会有几个flink job?</span></span><br><span class=\"line\">    <span class=\"comment\">//取决于调用了n个executeAsync()</span></span><br><span class=\"line\">    <span class=\"comment\">//对应application集群里,会有n个job</span></span><br><span class=\"line\">    <span class=\"comment\">//对应Jobmanager当中,会有 n个JobMaster</span></span><br><span class=\"line\">    environment.executeAsync();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"读取数据\"><a class=\"markdownIt-Anchor\" href=\"#读取数据\">#</a> 读取数据</h5>\n<h6 id=\"从集合读取数据\"><a class=\"markdownIt-Anchor\" href=\"#从集合读取数据\">#</a> 从集合读取数据</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">Configuration</span> <span class=\"variable\">configuration</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">    configuration.set(RestOptions.BIND_PORT, <span class=\"string\">&quot;8082&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 自动识别环境是远程还是本地</span></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromCollection(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>));</span><br><span class=\"line\">    DataStreamSource&lt;Integer&gt; dataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    integerDataStreamSource.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"文件\"><a class=\"markdownIt-Anchor\" href=\"#文件\">#</a> 文件</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-files&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">FileSource&lt;String&gt; build = FileSource.forRecordStreamFormat(<span class=\"keyword\">new</span> <span class=\"title class_\">TextLineInputFormat</span>(), <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(<span class=\"string\">&quot;input/1.txt&quot;</span>)).build();</span><br><span class=\"line\"></span><br><span class=\"line\">DataStreamSink&lt;String&gt; aaaa = environment.fromSource(build, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;aaaa&quot;</span>).print();</span><br></pre></td></tr></table></figure>\n<h6 id=\"kafka\"><a class=\"markdownIt-Anchor\" href=\"#kafka\">#</a> kafka</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">KafkaSource&lt;String&gt; build = KafkaSource.&lt;String&gt;builder().setBootstrapServers(<span class=\"string\">&quot;hadoop100:9092&quot;</span>).setGroupId(<span class=\"string\">&quot;aaa&quot;</span>).setTopics(<span class=\"string\">&quot;topic_haha&quot;</span>).setValueOnlyDeserializer(<span class=\"keyword\">new</span> <span class=\"title class_\">SimpleStringSchema</span>()).setStartingOffsets(OffsetsInitializer.latest()).build();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">environment.fromSource(build, WatermarkStrategy.noWatermarks(),<span class=\"string\">&quot;kafka&quot;</span>);</span><br></pre></td></tr></table></figure>\n<h6 id=\"数据生成器读取\"><a class=\"markdownIt-Anchor\" href=\"#数据生成器读取\">#</a> 数据生成器读取</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\">environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// GeneratorFunction接口重新long方法</span></span><br><span class=\"line\"><span class=\"comment\">// Long 类型 自动生成数字序列的最大值</span></span><br><span class=\"line\"><span class=\"comment\">// 限速策略</span></span><br><span class=\"line\"><span class=\"comment\">// 返回类型</span></span><br><span class=\"line\">DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 如果有n个并行度,最大值设为a</span></span><br><span class=\"line\"><span class=\"comment\">//将数值均分成n份,</span></span><br><span class=\"line\"><span class=\"comment\">//其中一个是0-49</span></span><br><span class=\"line\"><span class=\"comment\">//a/n,比如,最大100,并行度2,每个并行度生成50个</span></span><br><span class=\"line\">environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(),<span class=\"string\">&quot;datagen&quot;</span>).print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">environment.execute();</span><br></pre></td></tr></table></figure>\n<h5 id=\"flink-数据类型\"><a class=\"markdownIt-Anchor\" href=\"#flink-数据类型\">#</a> flink 数据类型</h5>\n<p>Flink 使用 “类型信息”（TypeInformation）来统一表示数据类型。TypeInformation 类是 Flink 中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p>\n<blockquote>\n<p>泛型类型（GENERIC）</p>\n<p>Flink 支持所有的 Java 类和 Scala 类。不过如果没有按照上面 POJO 类型的要求来定义，就会被 Flink 当作泛型类来处理。Flink 会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由 Flink 本身序列化的，而是由 Kryo 序列化的。</p>\n<p>在这些类型中，元组类型和 POJO 类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO 还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为 Flink 的 POJO 类型。</p>\n<p>Flink 对 POJO 类型的要求如下：</p>\n<ul>\n<li>类是公有（public）的</li>\n<li>有一个无参的构造方法</li>\n<li>所有属性都是公有（public）的</li>\n<li>所有属性的类型都是可以序列化的</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>类型提示（Type Hints）</strong></p>\n<p>Flink 还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于 Java 中泛型擦除的存在，在某些特殊情况下（比如 Lambda 表达式中），自动提取的信息是不够精细的 —— 只告诉 Flink 当前的元素由 “船头、船身、船尾” 构成，根本无法重建出 “大船” 的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p>\n<p>为了解决这类问题，Java API 提供了专门的 “类型提示”（type hints）。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.returns(<span class=\"keyword\">new</span> <span class=\"title class_\">TypeHint</span>&lt;Tuple2&lt;Integer, SomeType&gt;&gt;()&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">.returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"转换算子\"><a class=\"markdownIt-Anchor\" href=\"#转换算子\">#</a> 转换算子</h4>\n<h5 id=\"map\"><a class=\"markdownIt-Anchor\" href=\"#map\">#</a> map</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\">environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Object&gt; map = integerDataStreamSource.map(<span class=\"keyword\">new</span> <span class=\"title class_\">MapFunction</span>&lt;Integer, Object&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Object <span class=\"title function_\">map</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> integer;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">map.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Integer&gt; map1 = integerDataStreamSource.map(num -&gt; num * <span class=\"number\">2</span>);</span><br></pre></td></tr></table></figure>\n<h5 id=\"fliter-flatmap\"><a class=\"markdownIt-Anchor\" href=\"#fliter-flatmap\">#</a> fliter \\ flatmap</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Integer&gt; filter = integerDataStreamSource.filter(<span class=\"keyword\">new</span> <span class=\"title class_\">FilterFunction</span>&lt;Integer&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">filter</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// flatmap 可以一进一出，一进多出，一进0出</span></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Object&gt; objectSingleOutputStreamOperator = integerDataStreamSource.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;Integer, Object&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">flatMap</span><span class=\"params\">(Integer integer, Collector&lt;Object&gt; collector)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        collector.collect(integer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">objectSingleOutputStreamOperator.print();</span><br></pre></td></tr></table></figure>\n<script>alert(1)</script>\n<h5 id=\"keyby\"><a class=\"markdownIt-Anchor\" href=\"#keyby\">#</a> keyby</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// keyby 按照id分组，，返回键控流，不是转换算子，只是对数据重分区，不能设置并行度</span></span><br><span class=\"line\"><span class=\"comment\">// 3、keyby分组与分组 与 分区 的关系:</span></span><br><span class=\"line\"><span class=\"comment\">//1) keyby是对数据分组,保证 相同key的数据在同一个分区</span></span><br><span class=\"line\"><span class=\"comment\">//2)分区:一个子任务可以理解为一个分区,一个分区(子任务)中可以存在多个分组(key)</span></span><br><span class=\"line\">KeyedStream&lt;Integer, Tuple&gt; integerObjectKeyedStream = integerDataStreamSource.keyBy(String.valueOf(<span class=\"keyword\">new</span> <span class=\"title class_\">KeySelector</span>&lt;Integer, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">getKey</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;));</span><br></pre></td></tr></table></figure>\n<h5 id=\"简单聚合算子\"><a class=\"markdownIt-Anchor\" href=\"#简单聚合算子\">#</a> 简单聚合算子</h5>\n<p>keyby 之后才能调用</p>\n<p>做分组内聚合，对同一个 key 数据聚合</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum(<span class=\"string\">&quot;title&quot;</span>);  </span><br><span class=\"line\"><span class=\"comment\">// 传位置索引适用于tuple类型</span></span><br><span class=\"line\">min(<span class=\"string\">&quot;vc&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// max 只回取比较字段最大值，非比较字段取第一次的值</span></span><br><span class=\"line\"><span class=\"comment\">// maxby只回取比较字段最大值，非比较字段取当前字段的值</span></span><br><span class=\"line\">max()</span><br><span class=\"line\">maxby()</span><br></pre></td></tr></table></figure>\n<h5 id=\"reduce\"><a class=\"markdownIt-Anchor\" href=\"#reduce\">#</a> reduce</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">、keyby之后调用</span><br><span class=\"line\">、输入类型=输出类型,类型不能变</span><br><span class=\"line\">、每个key的第一条数据来的时候,不会执行reduce方法,在起来,直接输出</span><br><span class=\"line\">reduce 方法中的参数，value1之前计算结果  value2 现在来的数据</span><br></pre></td></tr></table></figure>\n<h5 id=\"udf\"><a class=\"markdownIt-Anchor\" href=\"#udf\">#</a> UDF</h5>\n<p>Flink 暴露了所有 UDF 函数的接口，具体实现方式为接口或者抽象类，例如 MapFunction、FilterFunction、ReduceFunction 等。所以用户可以自定义一个函数类，实现对应的接口</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  DataStream&lt;String&gt; filter = stream.filter(<span class=\"keyword\">new</span> <span class=\"title class_\">UserFilter</span>());</span><br><span class=\"line\">      </span><br><span class=\"line\">        filter.print();</span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UserFilter</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">FilterFunction</span>&lt;WaterSensor&gt; &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">filter</span><span class=\"params\">(WaterSensor e)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> e.id.equals(<span class=\"string\">&quot;sensor_1&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"richxxxfunction\"><a class=\"markdownIt-Anchor\" href=\"#richxxxfunction\">#</a> RichXXXFunction</h5>\n<p>多了生命周期管理方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">............多亏你申请</span><br><span class=\"line\">open():每个子任务,在启动时,调用一次</span><br><span class=\"line\">close():每个子任务,在结束时,调用一次</span><br></pre></td></tr></table></figure>\n<p>多了运行时上下文，可以获取运行时环境信息，比如子任务编号，名称</p>\n<p>无界流，如果 flink 程序异常终止，不会调用 close</p>\n<p>正常调用 web ui cancel，会调用 close</p>\n<h5 id=\"分区算子\"><a class=\"markdownIt-Anchor\" href=\"#分区算子\">#</a> 分区算子</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        Configuration configuration = new Configuration();</span></span><br><span class=\"line\"><span class=\"comment\">//        configuration.set(RestOptions.BIND_PORT, &quot;8082&quot;);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 随机分区</span></span><br><span class=\"line\">        <span class=\"comment\">// localhost.shuffle().print();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// rebalanced 轮询</span></span><br><span class=\"line\">        <span class=\"comment\">// 数据源倾斜场景，source读进来后，调用rebalance可以解决</span></span><br><span class=\"line\">        <span class=\"comment\">//localhost.rebalance().print();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// rescale 缩放</span></span><br><span class=\"line\">        <span class=\"comment\">// 局部组队，比rebalance更高效</span></span><br><span class=\"line\">        localhost.rescale().print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//</span></span><br><span class=\"line\">        localhost.broadcast().print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将所有数据发到一个子任务</span></span><br><span class=\"line\">        localhost.global().print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// keyby:按指定key去发送,相同key发往同一个子任务</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"自定义分区器\"><a class=\"markdownIt-Anchor\" href=\"#自定义分区器\">#</a> 自定义分区器</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyPart</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Partitioner</span>&lt;String&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">partition</span><span class=\"params\">(String s, <span class=\"type\">int</span> i)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Integer.parseInt(s) % i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">custompartption</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        localhost.partitionCustom(<span class=\"keyword\">new</span> <span class=\"title class_\">MyPart</span>(),num -&gt; num).print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"分流\"><a class=\"markdownIt-Anchor\" href=\"#分流\">#</a> 分流</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1.</span>使用process算子</span><br><span class=\"line\"><span class=\"number\">2.</span>定义outputtag对象</span><br><span class=\"line\"><span class=\"number\">3.</span>调用ctx.output</span><br><span class=\"line\"><span class=\"number\">4.</span>通过主流获取测流</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SplitStreamByOutputTag</span> &#123;    </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"> </span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class=\"string\">&quot;hadoop102&quot;</span>, <span class=\"number\">7777</span>)</span><br><span class=\"line\">              .map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">        OutputTag&lt;WaterSensor&gt; s1 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;s1&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class=\"line\">        OutputTag&lt;WaterSensor&gt; s2 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;s2&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class=\"line\">       <span class=\"comment\">//返回的都是主流</span></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessFunction</span>&lt;WaterSensor, WaterSensor&gt;()</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">                <span class=\"keyword\">if</span> (<span class=\"string\">&quot;s1&quot;</span>.equals(value.getId())) &#123;</span><br><span class=\"line\">                    ctx.output(s1, value);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (<span class=\"string\">&quot;s2&quot;</span>.equals(value.getId())) &#123;</span><br><span class=\"line\">                    ctx.output(s2, value);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">//主流</span></span><br><span class=\"line\">                    out.collect(value);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"> </span><br><span class=\"line\">        ds1.print(<span class=\"string\">&quot;主流，非s1,s2的传感器&quot;</span>);</span><br><span class=\"line\">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class=\"line\">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class=\"line\"> </span><br><span class=\"line\">        s1DS.printToErr(<span class=\"string\">&quot;s1&quot;</span>);</span><br><span class=\"line\">        s2DS.printToErr(<span class=\"string\">&quot;s2&quot;</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\"> </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"合流\"><a class=\"markdownIt-Anchor\" href=\"#合流\">#</a> 合流</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># union合流的数据类型必须相同</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">unionn</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;Integer&gt; integerDataStreamSource1 = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\">        DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        DataStream&lt;Integer&gt; union = integerDataStreamSource.union(integerDataStreamSource1);</span><br><span class=\"line\"></span><br><span class=\"line\">        union.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">多条流可以用 , 分割</span><br><span class=\"line\">source.union(source1,source2);</span><br><span class=\"line\"></span><br><span class=\"line\">也可以用.union </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 一次可以合并多条流</span><br><span class=\"line\"># connect合流数据类型可以不一样</span><br><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource1 = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>);</span><br><span class=\"line\">DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromElements(<span class=\"string\">&quot;111&quot;</span>, <span class=\"string\">&quot;222&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">ConnectedStreams&lt;Integer, String&gt; connect = integerDataStreamSource.connect(stringDataStreamSource);</span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; map = connect.map(<span class=\"keyword\">new</span> <span class=\"title class_\">CoMapFunction</span>&lt;Integer, String, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">map1</span><span class=\"params\">(Integer value)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> value.toString();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">map2</span><span class=\"params\">(String value)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">map.print();</span><br><span class=\"line\"></span><br><span class=\"line\">connect 一次只能连接两条流</span><br><span class=\"line\">连接后可以调用map等处理，但是各处理各的</span><br></pre></td></tr></table></figure>\n<h5 id=\"sink\"><a class=\"markdownIt-Anchor\" href=\"#sink\">#</a> sink</h5>\n<h6 id=\"输出到外部系统\"><a class=\"markdownIt-Anchor\" href=\"#输出到外部系统\">#</a> 输出到外部系统</h6>\n<p>Flink 作为数据处理框架，最终还是要把计算处理的结果写，入外部存储为外部应用提供支持。</p>\n<p>Sink 多数情况下同样并不需要我们自己实现。之前我们一直在使用的 print 方法其实就是一种 Sink, 它表示将数据流写入标准控制台打印输出。Flink 官方为我们提供了一部分</p>\n<h6 id=\"输出到文件\"><a class=\"markdownIt-Anchor\" href=\"#输出到文件\">#</a> 输出到文件</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 同时写入的文件数由并行度决定</span></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;datagen&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    FileSink&lt;String&gt; build = FileSink.forRowFormat(<span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(<span class=\"string\">&quot;/Users/bytedance/Desktop&quot;</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">SimpleStringEncoder</span>&lt;String&gt;(<span class=\"string\">&quot;UTF-8&quot;</span>)).</span><br><span class=\"line\">            withOutputFileConfig(OutputFileConfig.builder().withPartPrefix(<span class=\"string\">&quot;==&quot;</span>).withPartSuffix(<span class=\"string\">&quot;++&quot;</span>).build()).</span><br><span class=\"line\">            withBucketAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">DateTimeBucketAssigner</span>&lt;&gt;(<span class=\"string\">&quot;yyyy-MM-dd HH&quot;</span>, ZoneId.systemDefault())).</span><br><span class=\"line\">            withRollingPolicy(DefaultRollingPolicy.builder().withRolloverInterval(Duration.ofSeconds(<span class=\"number\">10</span>)).withMaxPartSize(<span class=\"keyword\">new</span> <span class=\"title class_\">MemorySize</span>(<span class=\"number\">1024</span> * <span class=\"number\">1024</span>)).build()).build();</span><br><span class=\"line\"></span><br><span class=\"line\">    stringDataStreamSource.sinkTo(build);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"输出到kafka\"><a class=\"markdownIt-Anchor\" href=\"#输出到kafka\">#</a> 输出到 kafka</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 同时写入的文件数由并行度决定</span></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;datagen&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// TODO注意:如果要使用精准一次 写入Kafka,需要满足以下条件,缺一不可</span></span><br><span class=\"line\">    <span class=\"comment\">//1、开启checkpoint(后续介绍)</span></span><br><span class=\"line\">    <span class=\"comment\">//2、设置事务前缀</span></span><br><span class=\"line\">    <span class=\"comment\">//3、设置事务超时时间:</span></span><br><span class=\"line\">    <span class=\"comment\">//checkpoint间隔&lt;</span></span><br><span class=\"line\">    <span class=\"comment\">//事务超时时间&lt;max的15分钟</span></span><br><span class=\"line\">    KafkaSink&lt;String&gt; build = KafkaSink.&lt;String&gt;builder()</span><br><span class=\"line\">            .setBootstrapServers(<span class=\"string\">&quot;hadoop100:9092,hadoop101:9092,hadoop102:9092&quot;</span>)</span><br><span class=\"line\">            .setRecordSerializer(</span><br><span class=\"line\">                    KafkaRecordSerializationSchema.&lt;String&gt;builder()</span><br><span class=\"line\">                            .setTopic(<span class=\"string\">&quot;aaaaa&quot;</span>)</span><br><span class=\"line\">                            .setValueSerializationSchema(<span class=\"keyword\">new</span> <span class=\"title class_\">SimpleStringSchema</span>())</span><br><span class=\"line\">                            .build()</span><br><span class=\"line\"></span><br><span class=\"line\">            )</span><br><span class=\"line\">            .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class=\"line\">            .setTransactionalIdPrefix(<span class=\"string\">&quot;xxxxxx&quot;</span>)</span><br><span class=\"line\">            .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG,<span class=\"number\">10</span> * <span class=\"number\">60</span> * <span class=\"number\">1000</span> + <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">            .build();</span><br><span class=\"line\">    <span class=\"comment\">// //写到kafka的一致性级别:精准一次、至少一次</span></span><br><span class=\"line\">    <span class=\"comment\">//.setDeliveryGuarantee (DeliveryGuarantee.EXAC)TLY_ONCE</span></span><br><span class=\"line\">    <span class=\"comment\">////如果是精准一次,必须设置事务的前缀</span></span><br><span class=\"line\">    <span class=\"comment\">//.setTransactionalIdPrefix(&quot;-&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\">//如果是精准一次,必须设置事务超时时间:大于checkpoint间隔,小于max15分钟</span></span><br><span class=\"line\"></span><br><span class=\"line\">    stringDataStreamSource.sinkTo(build);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 同时写入的文件数由并行度决定</span></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;datagen&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果要指定写入kafka的key</span></span><br><span class=\"line\">    <span class=\"comment\">//可以自定义反序列器:</span></span><br><span class=\"line\">    <span class=\"comment\">//1、实现一个接口,重写序列化方法</span></span><br><span class=\"line\">    <span class=\"comment\">//2、指定key,转成字节数组</span></span><br><span class=\"line\">    <span class=\"comment\">//3、指定value,转成 字节数组</span></span><br><span class=\"line\">    <span class=\"comment\">//4、返回一个ProducerRecord对象,把key、value放进去</span></span><br><span class=\"line\"></span><br><span class=\"line\">    KafkaSink&lt;String&gt; build = KafkaSink.&lt;String&gt;builder()</span><br><span class=\"line\">            .setBootstrapServers(<span class=\"string\">&quot;hadoop100:9092,hadoop101:9092,hadoop102:9092&quot;</span>)</span><br><span class=\"line\">            .setRecordSerializer(</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> <span class=\"title class_\">KafkaRecordSerializationSchema</span>&lt;String&gt;() &#123;</span><br><span class=\"line\">                        <span class=\"meta\">@Nullable</span></span><br><span class=\"line\">                        <span class=\"meta\">@Override</span></span><br><span class=\"line\">                        <span class=\"keyword\">public</span> ProducerRecord&lt;<span class=\"type\">byte</span>[], <span class=\"type\">byte</span>[]&gt; serialize(String s, KafkaSinkContext kafkaSinkContext, Long aLong) &#123;</span><br><span class=\"line\">                            String[] datas = s.split(<span class=\"string\">&quot;,&quot;</span>);</span><br><span class=\"line\">                            <span class=\"type\">byte</span>[] key = datas[<span class=\"number\">0</span>].getBytes(StandardCharsets.UTF_8);</span><br><span class=\"line\">                            <span class=\"type\">byte</span>[] value = s.getBytes(StandardCharsets.UTF_8);</span><br><span class=\"line\">                            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ProducerRecord</span>&lt;&gt;(<span class=\"string\">&quot;aaa&quot;</span>, key, value);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            )</span><br><span class=\"line\">            .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class=\"line\">            .setTransactionalIdPrefix(<span class=\"string\">&quot;xxxxxx&quot;</span>)</span><br><span class=\"line\">            .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, <span class=\"number\">10</span> * <span class=\"number\">60</span> * <span class=\"number\">1000</span> + <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">            .build();</span><br><span class=\"line\"></span><br><span class=\"line\">    stringDataStreamSource.sinkTo(build);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>输出到 mysql</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">8.0</span><span class=\"number\">.30</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;flink-connector-jdbc&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">1.17</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;repositories&gt;</span><br><span class=\"line\">    &lt;repository&gt;</span><br><span class=\"line\">        &lt;id&gt;apache-snapshots&lt;/id&gt;</span><br><span class=\"line\">        &lt;name&gt;apache snapshots&lt;/name&gt;</span><br><span class=\"line\">        &lt;url&gt;https:<span class=\"comment\">//repository.apache.org/content/repositories/snapshots/&lt;/url&gt;</span></span><br><span class=\"line\">    &lt;/repository&gt;</span><br><span class=\"line\">&lt;/repositories&gt;</span><br><span class=\"line\">SinkFunction&lt;WaterSensor&gt; jdbcSink = JdbcSink.sink(</span><br><span class=\"line\">                <span class=\"string\">&quot;insert into ws values(?,?,?)&quot;</span>,</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcStatementBuilder</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">accept</span><span class=\"params\">(PreparedStatement preparedStatement, WaterSensor waterSensor)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">                        <span class=\"comment\">//每收到一条WaterSensor，如何去填充占位符</span></span><br><span class=\"line\">                        preparedStatement.setString(<span class=\"number\">1</span>, waterSensor.getId());</span><br><span class=\"line\">                        preparedStatement.setLong(<span class=\"number\">2</span>, waterSensor.getTs());</span><br><span class=\"line\">                        preparedStatement.setInt(<span class=\"number\">3</span>, waterSensor.getVc());</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                JdbcExecutionOptions.builder()</span><br><span class=\"line\">                        .withMaxRetries(<span class=\"number\">3</span>) <span class=\"comment\">// 重试次数</span></span><br><span class=\"line\">                        .withBatchSize(<span class=\"number\">100</span>) <span class=\"comment\">// 批次的大小：条数</span></span><br><span class=\"line\">                        .withBatchIntervalMs(<span class=\"number\">3000</span>) <span class=\"comment\">// 批次的时间</span></span><br><span class=\"line\">                        .build(),</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcConnectionOptions</span>.JdbcConnectionOptionsBuilder()</span><br><span class=\"line\">                        .withUrl(<span class=\"string\">&quot;jdbc:mysql://hadoop102:3306/test?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=UTF-8&quot;</span>)</span><br><span class=\"line\">                        .withUsername(<span class=\"string\">&quot;root&quot;</span>)</span><br><span class=\"line\">                        .withPassword(<span class=\"string\">&quot;000000&quot;</span>)</span><br><span class=\"line\">                        .withConnectionCheckTimeoutSeconds(<span class=\"number\">60</span>) <span class=\"comment\">// 重试的超时时间</span></span><br><span class=\"line\">                        .build()</span><br><span class=\"line\">        );</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\">//          * TODO 写入mysql</span></span><br><span class=\"line\"><span class=\"comment\">//         * 1、只能用老的sink写法： addsink</span></span><br><span class=\"line\"><span class=\"comment\">//         * 2、JDBCSink的4个参数:</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第一个参数： 执行的sql，一般就是 insert into</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第二个参数： 预编译sql， 对占位符填充值</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第三个参数： 执行选项 ---》 攒批、重试</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第四个参数： 连接选项 ---》 url、用户名、密码</span></span><br><span class=\"line\">        sensorDS.addSink(jdbcSink);</span><br></pre></td></tr></table></figure>\n<h6 id=\"自定义sink\"><a class=\"markdownIt-Anchor\" href=\"#自定义sink\">#</a> 自定义 sink</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Mysink</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">RichSinkFunction</span>&lt;String&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">open</span><span class=\"params\">(Configuration parameters)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.open(parameters);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">close</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">invoke</span><span class=\"params\">(String value, Context context)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.invoke(value, context);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"时间和窗口\"><a class=\"markdownIt-Anchor\" href=\"#时间和窗口\">#</a> 时间和窗口</h4>\n<p>Flink 是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的 &quot;数据块&quot; 进行处理，这就是所谓的 &quot;窗口&quot;(Window)。</p>\n<p>在 Flink 中，窗口其实并不是一个 &quot;框&quot;, 应该把窗口理解成一个 &quot;桶&quot;。在 Flink 中，窗口可以把流切割成有限大小的多个 &quot;存储桶&quot;。每个数据都会分发到对应的桶中，当到达窗口结束时间时，就对每个桶中收集的数据进行计算处理。</p>\n<p>Flink 中窗口并不是静态准备好的，而是动态创建 —— 当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。</p>\n<h5 id=\"窗口分类\"><a class=\"markdownIt-Anchor\" href=\"#窗口分类\">#</a> 窗口分类</h5>\n<p>窗口本身是截取有界数据的一种方式，所以窗口一个非常重要的信意其实就是 &quot;怎样截取数据&quot;。换句话说，就是以什么标准来开始和结束数据的截取，我们把它叫作窗口的 &quot;驱动类型&quot;。</p>\n<h6 id=\"1按照驱动类型分\"><a class=\"markdownIt-Anchor\" href=\"#1按照驱动类型分\">#</a> 1. 按照驱动类型分</h6>\n<p>(1) 时间窗口 (TimeWindow)</p>\n<p>时间窗口以时间点来定义窗口的开始 (start) 和结束 (end), 所以截取出的就是某一时间段的数据。到达结束时间时，窗口不再收集数据，触发计算输出结果，并将窗口关闭销毁。所以可以说基本思路就是 &quot;定点发车&quot;。</p>\n<p>(2) 计数窗口 (CountWindow)</p>\n<p>计数窗口基于元素的个数来截取数据，到达固定的个数时就触发计算并关闭窗口。每个窗口截取数据的个数，就是窗口的大小。基本思路是 &quot;人齐发车&quot;。</p>\n<h6 id=\"2按照窗口分配数据的规则分类\"><a class=\"markdownIt-Anchor\" href=\"#2按照窗口分配数据的规则分类\">#</a> 2. 按照窗口分配数据的规则分类</h6>\n<p>（1）滚动窗口</p>\n<p>滚动窗口有固定的大小，是一种对数据进行 &quot;均匀切片&quot; 的划分方式。窗口之间没有重叠，也不会有间隔，是 &quot;首尾相接&quot; 的状态。这是最简单的窗口形式，每个数据都都会被分配到一个窗口，而且只会属于一个窗口。</p>\n<p>（2）滑动窗口</p>\n<p>滑动窗口的大小也是固定的。但是窗口之间并不是首尾相接的，而是可以 &quot;错开&quot; 一定的位置。定义滑动窗口的参数有两个：除去窗口大小 (windowsize) 之之外，还有一个 &quot;滑动步长&quot;(windowslide), 它其实就代表了窗口计算的频率。窗口在结束时间触发计算输出结果，那么滑动步长就代表了计算频率。</p>\n<p>（3）会话窗口</p>\n<p>会话窗口，是基于 &quot;会话&quot;(session) 来来对数据进行分分组的。会话窗口只能基于时间来定义。</p>\n<p>会话窗口中，最重要的参数就是会话的超时时间，也就是两个会话窗口之间的最小距离。如果相邻两个数据到来的时间间隔 (Gap) 小于指定的大小 (size), 那说明还在保保持会话，它们就属于同一个窗口；如果 gap 大于 size, 那么新来的数据就应该属于新的会话窗口，而前一个窗口就应该关闭了。</p>\n<p>（4）全局窗口</p>\n<p>“全局窗口”, 这种窗口全局有效，会把相同 key 的所有数据都分配到同一个窗口中，这种窗口没有结束的时候，默认是不会做触发计算的。如果希望它能对数据进行计算处理，还需要自定义 &quot;触发器&quot;(Trigger)。</p>\n<h5 id=\"窗口api\"><a class=\"markdownIt-Anchor\" href=\"#窗口api\">#</a> 窗口 API</h5>\n<h6 id=\"带keyby和不带keyby\"><a class=\"markdownIt-Anchor\" href=\"#带keyby和不带keyby\">#</a> 带 keyby 和不带 keyby</h6>\n<p>1.1 没有 keyby 的窗口：窗口内的所有数据进入同一个子任务，并行度只能为 1     //sensorDS.windowAll ()</p>\n<p>1.2 有 keyby 的窗口：每个 key 上都定义了一组窗口，各自独立地进行统计计算   //sensorDS.window ()</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//1.1没有keyby的窗口:窗口内的所有数据进入同一个子任务,并行度只能为1</span></span><br><span class=\"line\">sensorDS.windowAll()</span><br><span class=\"line\"><span class=\"comment\">//1.2有keyby的窗口:每个key上都定义了一组窗口,各自独立地进行统计计算</span></span><br><span class=\"line\">基于时间的</span><br><span class=\"line\">sensorKS.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))<span class=\"comment\">////滚动窗口长度10s</span></span><br><span class=\"line\">sensorKS.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>),Time.seconds(<span class=\"number\">2</span>)))<span class=\"comment\">// 滑动窗口,窗口长度10s,滑动步长2s</span></span><br><span class=\"line\">sensorKS.window(ProcessingTimeSessionWindows.wiithGap(Time.seconds(<span class=\"number\">5</span>)))<span class=\"comment\">// 会话窗口,超时间隔5s</span></span><br><span class=\"line\"><span class=\"comment\">//基于计数的</span></span><br><span class=\"line\">sensorKS.countWindow(<span class=\"number\">5</span>) <span class=\"comment\">// 滚动窗口,窗口长度=5个元素</span></span><br><span class=\"line\">sensorKS.countWindow(<span class=\"number\">5</span>,<span class=\"number\">2</span>)<span class=\"comment\">//滑动窗口长度=5个元素,滑动步长=2个元素</span></span><br><span class=\"line\">sensorKS.window(Globalwindows.create()),_<span class=\"comment\">//全局窗口,计数窗口的底层就是用的这个,需要自定义触发器</span></span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224618279.png\" alt=\"image-20240804224618279\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// TODO 1.指定窗口分配器，用哪一种窗口</span></span><br><span class=\"line\"><span class=\"comment\">// 没有keyby的窗口</span></span><br><span class=\"line\"><span class=\"comment\">// 窗口内的所有数据进入同一个子任务,并行度只能为1</span></span><br><span class=\"line\"><span class=\"comment\">//waterSensorStringKeyedStream.windowAll()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 有keyby 窗口</span></span><br><span class=\"line\"><span class=\"comment\">// 每个key上都定义了一组窗口,各自独立地进行统计计算</span></span><br><span class=\"line\"><span class=\"comment\">//waterSensorStringKeyedStream.window()</span></span><br><span class=\"line\"><span class=\"comment\">// 基于时间</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));       <span class=\"comment\">// 滚动</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window1 = waterSensorStringKeyedStream.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>), Time.seconds(<span class=\"number\">2</span>)));      <span class=\"comment\">// 滑动</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window2 = waterSensorStringKeyedStream.window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class=\"number\">5</span>)));       <span class=\"comment\">// 会话</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 基于计数的</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream = waterSensorStringKeyedStream.countWindow(<span class=\"number\">5</span>);   <span class=\"comment\">// 滚动，窗口长度为5</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream1 = waterSensorStringKeyedStream.countWindow(<span class=\"number\">5</span>, <span class=\"number\">2</span>);   <span class=\"comment\">// 滑动，长度5 步长 2</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; window3 = waterSensorStringKeyedStream.window(GlobalWindows.create());     <span class=\"comment\">// 全局窗口，计数窗口底层，需要自定义触发器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// TODO 2.指定窗口函数,计算逻辑</span></span><br><span class=\"line\"><span class=\"comment\">// 增量聚合函数：来一条算一条.窗口触发的时候输出计算结果</span></span><br><span class=\"line\">window.reduce();</span><br><span class=\"line\">window.aggregate();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 全窗口函数，数据来了不计算。窗口触发的时候，计算并输出</span></span><br><span class=\"line\">window.process();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">environment.execute();</span><br></pre></td></tr></table></figure>\n<h5 id=\"窗口函数\"><a class=\"markdownIt-Anchor\" href=\"#窗口函数\">#</a> 窗口函数</h5>\n<h6 id=\"增量聚合reduce\"><a class=\"markdownIt-Anchor\" href=\"#增量聚合reduce\">#</a> 增量聚合 reduce</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 窗口的reduce</span></span><br><span class=\"line\"><span class=\"comment\">// 1.相同的key来的第一条数据不会调用reduce</span></span><br><span class=\"line\"><span class=\"comment\">// 2.增量聚合：来一条数据计算一次，不会输出</span></span><br><span class=\"line\"><span class=\"comment\">// 3.窗口触发的时候，才会输出窗口的最终计算结果</span></span><br><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; reduce = window.reduce(<span class=\"keyword\">new</span> <span class=\"title class_\">ReduceFunction</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> WaterSensor <span class=\"title function_\">reduce</span><span class=\"params\">(WaterSensor waterSensor, WaterSensor t1)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;调用reduce，&quot;</span> + waterSensor + <span class=\"string\">&quot;======&quot;</span> + t1);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensor</span>(waterSensor.getId(), t1.getTs(), waterSensor.getVc() + t1.getVc());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">reduce.print();</span><br><span class=\"line\"></span><br><span class=\"line\">environment.execute();</span><br></pre></td></tr></table></figure>\n<h6 id=\"聚合函数aggregate\"><a class=\"markdownIt-Anchor\" href=\"#聚合函数aggregate\">#</a> 聚合函数 aggregate</h6>\n<p>ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。</p>\n<p>Flink WindowAPI 中的 aggregate 就突破了这个限制，可以定义务更加灵活的窗口聚合操作。</p>\n<p>这个方法需要传入一个 AggregateFunction 的实现类作为参数。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 第一条数据来，创建窗口，创建累加器</span></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; aggregate = window.aggregate(<span class=\"keyword\">new</span> <span class=\"title class_\">AggregateFunction</span>&lt;WaterSensor, Integer, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * 输入类型</span></span><br><span class=\"line\"><span class=\"comment\">         * 累加器类型（存储中间结果类型）</span></span><br><span class=\"line\"><span class=\"comment\">         * 输出类型</span></span><br><span class=\"line\"><span class=\"comment\">         * pod</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> Integer <span class=\"title function_\">createAccumulator</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;创建累加器&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> Integer <span class=\"title function_\">add</span><span class=\"params\">(WaterSensor waterSensor, Integer integer)</span> &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;调用add&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> integer + waterSensor.getVc();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">getResult</span><span class=\"params\">(Integer integer)</span> &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;result&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> integer.toString();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> Integer <span class=\"title function_\">merge</span><span class=\"params\">(Integer integer, Integer acc1)</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 会话窗口会用到</span></span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;merge&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    aggregate.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"全窗口函数\"><a class=\"markdownIt-Anchor\" href=\"#全窗口函数\">#</a> 全窗口函数</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; apply = window.apply(<span class=\"keyword\">new</span> <span class=\"title class_\">WindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         *</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> s The key for which this window is evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> window The window that is being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> input The elements in the window being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> out A collector for emitting elements.</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">apply</span><span class=\"params\">(String s, TimeWindow window, Iterable&lt;WaterSensor&gt; input, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; process = window.process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         *</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> s The key for which this window is evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> context The context in which the window is being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> elements The elements in the window being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> out A collector for emitting elements.</span></span><br><span class=\"line\"><span class=\"comment\">         * 全窗口函数计算逻辑，窗口触发是才会调用一次，统一计算</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"type\">long</span> <span class=\"variable\">start</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">            <span class=\"type\">long</span> <span class=\"variable\">end</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">            DateFormatUtils.format(start,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">            DateFormatUtils.format(end,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"type\">long</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\">            out.collect(elements.toString()+l);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    process.print();</span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"结合使用\"><a class=\"markdownIt-Anchor\" href=\"#结合使用\">#</a> 结合使用</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增量聚合Aggregate+全窗口process</span></span><br><span class=\"line\"><span class=\"comment\">     * 1、增量聚合函数处理数据:来一条计算一条</span></span><br><span class=\"line\"><span class=\"comment\">     * 2、窗口触发时,增量聚合的结果(只有一条)</span></span><br><span class=\"line\"><span class=\"comment\">     * 3、经过全窗口函数的处理包装后,输出</span></span><br><span class=\"line\"><span class=\"comment\">     * 传递给全窗口函数</span></span><br><span class=\"line\"><span class=\"comment\">     * 结合两者的优点:</span></span><br><span class=\"line\"><span class=\"comment\">     * 1、增量聚合:来一条计算一条,存储中间的计算结果,占用的空间少</span></span><br><span class=\"line\"><span class=\"comment\">     * 2、全窗口函数:可以通过上下文实现灵活的功能</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; aggregate1 = window.aggregate(<span class=\"keyword\">new</span> <span class=\"title class_\">MyAgg</span>(), <span class=\"keyword\">new</span> <span class=\"title class_\">MyProcWin</span>());</span><br><span class=\"line\">    aggregate1.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyAgg</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">AggregateFunction</span>&lt;WaterSensor, Integer, String&gt; &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Integer <span class=\"title function_\">createAccumulator</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;创建累加器&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Integer <span class=\"title function_\">add</span><span class=\"params\">(WaterSensor waterSensor, Integer integer)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;调用add&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> integer + waterSensor.getVc();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">getResult</span><span class=\"params\">(Integer integer)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;result&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> integer.toString();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Integer <span class=\"title function_\">merge</span><span class=\"params\">(Integer integer, Integer acc1)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 会话窗口会用到</span></span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;merge&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyProcWin</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;String,String,String,TimeWindow&gt;&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;String, String, String, TimeWindow&gt;.Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">long</span> <span class=\"variable\">start</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">        <span class=\"type\">long</span> <span class=\"variable\">end</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">        DateFormatUtils.format(start,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">        DateFormatUtils.format(end,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">long</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\">        out.collect(elements.toString()+l);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"动态会话窗口\"><a class=\"markdownIt-Anchor\" href=\"#动态会话窗口\">#</a> 动态会话窗口</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(ProcessingTimeSessionWindows.withDynamicGap(</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">SessionWindowTimeGapExtractor</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">                <span class=\"meta\">@Override</span></span><br><span class=\"line\">                <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extract</span><span class=\"params\">(WaterSensor element)</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 根据ts的值动态变化会话窗口</span></span><br><span class=\"line\">                    <span class=\"keyword\">return</span> element.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">    ));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"计数窗口\"><a class=\"markdownIt-Anchor\" href=\"#计数窗口\">#</a> 计数窗口</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 窗口分配器</span></span><br><span class=\"line\">        <span class=\"comment\">// 滚动，窗口长度5条数据</span></span><br><span class=\"line\"><span class=\"comment\">//        WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream = waterSensorStringKeyedStream.countWindow(5);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream = waterSensorStringKeyedStream.countWindow(<span class=\"number\">5</span>, <span class=\"number\">2</span>);</span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorStringGlobalWindowWindowedStream.process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, GlobalWindow&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, GlobalWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> context.window().maxTimestamp();</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">l1</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\">                out.collect(l + <span class=\"string\">&quot;===&quot;</span> + l1);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        process.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"触发器-移除器\"><a class=\"markdownIt-Anchor\" href=\"#触发器-移除器\">#</a> 触发器 移除器</h6>\n<p>触发器：触发计算和输出</p>\n<p>窗口触发：</p>\n<p>时间进展 &gt;= 窗口的最大时间戳 (end-1ms)</p>\n<p>移除器：</p>\n<p>定义移除数据的逻辑</p>\n<p>触发器和移除器都有默认实现，一般不需要自定义</p>\n<blockquote>\n<p>以时间类型的滚动窗口为例，分析原理:</p>\n<p>TODO1、窗口什么时候触发输出？</p>\n<p>时间进展 &gt;= 窗口的最大时间戳 (end-1ms)</p>\n<p>TODO2、窗口是怎么划分的？</p>\n<p>start = 向下取整，取窗口长度的整数倍</p>\n<p>end=start + 窗口长度</p>\n<p>窗口左闭右开 ==》</p>\n<p>属于本窗口的最大时间戳 = end-1ms</p>\n<p>TODO 3、窗口的生命周期？</p>\n<p>创建：属于本窗口的第一条数据来的时候，现 new 的，放入一一个 singleton 单例的集合中</p>\n<p>销毁 (关窗): 时间进展 &gt;= 窗口的最大时间戳 (end-1ms)+ 允许迟到的时间 (默认 0)</p>\n</blockquote>\n<h5 id=\"时间\"><a class=\"markdownIt-Anchor\" href=\"#时间\">#</a> 时间</h5>\n<p>事件时间：数据产生的时间</p>\n<p>处理时间：数据被处理的时间</p>\n<h5 id=\"水位线\"><a class=\"markdownIt-Anchor\" href=\"#水位线\">#</a> 水位线</h5>\n<p>具体实现上，水位线可以看作一条特殊的数据记录，它是插入到数据流中的一个标记点，主要内容就是一个时间戳！用来指示当前的事件时间。</p>\n<p>2) 乱序流中的水位线</p>\n<p>在分布式系统中，数据在节点间传输，会因为网络传输延迟的不确定性，导致顺序发生改变，这就是所谓的 &quot;乱序数据&quot;</p>\n<p>乱序 + 数据量小：我们还是靠数据来驱动，每来一个数据就提取它的时间戳、插入一个水位线。不过现在的情况是数据乱序，所以插入新的水位线时，要先判断一了下时间戳是否比之前的大，否则就不再生成新的水位线。也就是说，只有数据的时间戳比当前时钟大，才能推动动时钟前进，这时才插入水位线。</p>\n<p>乱序 + 数据量大：如果考虑到大量数据同时到来的处理效率，我们同样可以周期性地生成水位线。这时只需要保存一下之前所有数据中的最大时间戳，需要插入水位线时，就直接以它作为时间戳生成新的水位线。</p>\n<p>乱序 + 迟到数据：我们无法正确处理 &quot;迟到&quot; 的数据。为了让窗口能够正确收集到迟到的数据，我们也可以等上一段时间，比如 2 秒；也就是用当前已有数据的最大大时间戳减去 2 秒，就是要插入的水位线的时间戳。这样的话，9 秒的数据到来之后，事件时钟不会直接推进到 9 秒，而是进展到了 7 秒；必须等到 11 秒的数据到来之后，事件时钟才会进展到 9 秒，这时迟到数据也都已收集齐，0~9 秒的窗口就可以正确计算结果了。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224636418.png\" alt=\"image-20240804224636418\"></p>\n<p>3) 水位线特性</p>\n<p>水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据</p>\n<p>水位线主要的内容是一个时间戳，用来表示当前事件时间的进展</p>\n<p>水位线是基于数据的时间戳生成的</p>\n<p>水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进</p>\n<p>水位线可以通过设置延迟，来保证正确处理乱序数据</p>\n<p>一个水位线 Watermark (t), 表示在当前流中事件时间已经达到了时间戳 t, 这代表 t 之前的所有数据都到齐了，之后流中不会出现时间戳 t’&lt;t 的数据</p>\n<p>水位线是 Flink 流处理中保证结果正确性的核心机制，它往往会跟窗口一起配合，完成对乱序数据的正确处理。</p>\n<p>正确理解：在 Flink 中，窗口其实并不是一个 &quot;框&quot;, 应该把窗口理解成一个 &quot;桶&quot;。在 Flink 中，窗口可以把流切割成有限大小的多个 &quot;存储桶&quot;(bucket); 每个数据都会分发到对应的桶中，当到达窗口结束时间时，就对每个桶中收集的数据进行计算处理。</p>\n<h6 id=\"生成水位线\"><a class=\"markdownIt-Anchor\" href=\"#生成水位线\">#</a> 生成水位线</h6>\n<p>一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。</p>\n<p>内置水位线，有序流水位线</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">watermarkdemo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">                <span class=\"comment\">// 升序的watermark，没有等待时间</span></span><br><span class=\"line\">                .&lt;WaterSensor&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                <span class=\"comment\">// 指定时间传分配器</span></span><br><span class=\"line\">                .withTimestampAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">SerializableTimestampAssigner</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extractTimestamp</span><span class=\"params\">(WaterSensor waterSensor, <span class=\"type\">long</span> l)</span> &#123;</span><br><span class=\"line\">                System.out.println(waterSensor + <span class=\"string\">&quot;=======&quot;</span> + l);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> waterSensor.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = localhost.assignTimestampsAndWatermarks(waterSensorWatermarkStrategy);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">                <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">                .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                        out.collect(windowStart + windowEnd);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>内置 watermark 都是周期性生成的  environment.getConfig ().setAutoWatermarkInterval ();  默认 200 毫秒</p>\n<p>有序流 watermark = 当前最大的事件时间 - 1ms</p>\n<p>乱序流 watermark = 当前最大的事件时间 - 延迟时间 -1ms</p>\n<p>升序 watermark 就是等待时间为 0 的乱序 watermark</p>\n<p>乱序流设置水位线</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.time.Duration;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">watermarkOOOdemo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">                <span class=\"comment\">// 乱序的watermark，等三秒</span></span><br><span class=\"line\">                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class=\"number\">3</span>))</span><br><span class=\"line\">                <span class=\"comment\">// 指定时间传分配器</span></span><br><span class=\"line\">                .withTimestampAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">SerializableTimestampAssigner</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extractTimestamp</span><span class=\"params\">(WaterSensor waterSensor, <span class=\"type\">long</span> l)</span> &#123;</span><br><span class=\"line\">                System.out.println(waterSensor + <span class=\"string\">&quot;=======&quot;</span> + l);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> waterSensor.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = localhost.assignTimestampsAndWatermarks(waterSensorWatermarkStrategy);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">                <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">                .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                        out.collect(windowStart + windowEnd);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>水位线生成器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyPeriodWaterMark</span>&lt;T&gt; <span class=\"keyword\">implements</span> <span class=\"title class_\">WatermarkGenerator</span>&lt;T&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> maxTs;</span><br><span class=\"line\">    <span class=\"comment\">// 保存当前为止最大时间</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> delayTs;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">MyPeriodWaterMark</span><span class=\"params\">(<span class=\"type\">long</span> delayTs)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.delayTs = delayTs;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.maxTs = Long.MIN_VALUE + <span class=\"built_in\">this</span>.delayTs + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onEvent</span><span class=\"params\">(T t, <span class=\"type\">long</span> l, WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\">        maxTs = Math.max(maxTs, l);</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;最大时间穿&quot;</span> + maxTs);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onPeriodicEmit</span><span class=\"params\">(WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\">        watermarkOutput.emitWatermark(<span class=\"keyword\">new</span> <span class=\"title class_\">Watermark</span>(maxTs-delayTs -<span class=\"number\">1</span>));</span><br><span class=\"line\">        System.out.println(maxTs-delayTs -<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">watermarkCuustomdemo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.getConfig().setAutoWatermarkInterval(<span class=\"number\">200</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">                <span class=\"comment\">// 指定自定义生成器</span></span><br><span class=\"line\">                .&lt;WaterSensor&gt;forGenerator(<span class=\"keyword\">new</span> <span class=\"title class_\">WatermarkGeneratorSupplier</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> WatermarkGenerator&lt;WaterSensor&gt; <span class=\"title function_\">createWatermarkGenerator</span><span class=\"params\">(Context context)</span> &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MyPeriodWaterMark</span>&lt;&gt;(<span class=\"number\">3000L</span>);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;)</span><br><span class=\"line\">                <span class=\"comment\">// 指定时间传分配器</span></span><br><span class=\"line\">                .withTimestampAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">SerializableTimestampAssigner</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extractTimestamp</span><span class=\"params\">(WaterSensor waterSensor, <span class=\"type\">long</span> l)</span> &#123;</span><br><span class=\"line\">                System.out.println(waterSensor + <span class=\"string\">&quot;=======&quot;</span> + l);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> waterSensor.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = localhost.assignTimestampsAndWatermarks(waterSensorWatermarkStrategy);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">                <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">                .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                        out.collect(windowStart + windowEnd);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"断点式水位线生成器\"><a class=\"markdownIt-Anchor\" href=\"#断点式水位线生成器\">#</a> 断点式水位线生成器</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyPuntuatedPeriodWaterMark</span>&lt;T&gt; <span class=\"keyword\">implements</span> <span class=\"title class_\">WatermarkGenerator</span>&lt;T&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> maxTs;</span><br><span class=\"line\">    <span class=\"comment\">// 保存当前为止最大时间</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> delayTs;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">MyPuntuatedPeriodWaterMark</span><span class=\"params\">(<span class=\"type\">long</span> delayTs)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.delayTs = delayTs;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.maxTs = Long.MIN_VALUE + <span class=\"built_in\">this</span>.delayTs + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onEvent</span><span class=\"params\">(T t, <span class=\"type\">long</span> l, WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\">        maxTs = Math.max(maxTs, l);</span><br><span class=\"line\">        watermarkOutput.emitWatermark(<span class=\"keyword\">new</span> <span class=\"title class_\">Watermark</span>(maxTs - delayTs - <span class=\"number\">1</span>));</span><br><span class=\"line\">        System.out.println( maxTs);</span><br><span class=\"line\">        System.out.println( maxTs - delayTs - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onPeriodicEmit</span><span class=\"params\">(WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">        <span class=\"comment\">// 指定自定义生成器</span></span><br><span class=\"line\">        .&lt;WaterSensor&gt;forGenerator(ctx -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">MyPuntuatedPeriodWaterMark</span>&lt;&gt;(<span class=\"number\">3000L</span>))</span><br></pre></td></tr></table></figure>\n<h6 id=\"水位线传递\"><a class=\"markdownIt-Anchor\" href=\"#水位线传递\">#</a> 水位线传递</h6>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224656483.png\" alt=\"image-20240804224656483\"></p>\n<p>收到上游多个，取最小</p>\n<p>往下游多个发送，广播</p>\n<p>空闲等待：超过时间上有还没水位过来，后续不使用</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;Integer&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>)</span><br><span class=\"line\">        .partitionCustom(<span class=\"keyword\">new</span> <span class=\"title class_\">MyPart</span>(), r -&gt; r)</span><br><span class=\"line\">        .map(Integer::parseInt)</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Integer&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((r, ts) -&gt; r * <span class=\"number\">1000L</span>)</span><br><span class=\"line\">                <span class=\"comment\">// 空闲等待五秒</span></span><br><span class=\"line\">                .withIdleness(Duration.ofSeconds(<span class=\"number\">5</span>)));</span><br></pre></td></tr></table></figure>\n<p>允许迟到</p>\n<p>推迟关窗时间，在关窗之前，会计算迟到数据，来一条计算一次。</p>\n<p>关窗口迟到数据不会被计算</p>\n<p>乱序：数据顺序乱了，出现时间小的比时间大的后来</p>\n<p>迟到：当前数据的时间戳 &lt; 当前的 watermark</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">        <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">        .allowedLateness(Time.seconds(<span class=\"number\">2</span>))   <span class=\"comment\">// 允许推迟两秒关窗</span></span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                out.collect(windowStart + windowEnd);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br></pre></td></tr></table></figure>\n<p>侧输出流</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OutputTag&lt;WaterSensor&gt; latedata = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;latedata&quot;</span>, Types.POJO(WaterSensor.class));</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">        <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">        .allowedLateness(Time.seconds(<span class=\"number\">2</span>))   <span class=\"comment\">// 允许推迟两秒关窗</span></span><br><span class=\"line\">        <span class=\"comment\">// 关窗后迟到数据放入侧输出流</span></span><br><span class=\"line\">        .sideOutputLateData(latedata)</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                out.collect(windowStart + windowEnd);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        <span class=\"comment\">// 从主流获取侧输出流打印</span></span><br><span class=\"line\">        process.getSideOutput(latedata).printToErr();</span><br><span class=\"line\">        process.print();</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224716145.png\" alt=\"image-20240804224716145\"></p>\n<h5 id=\"合流-2\"><a class=\"markdownIt-Anchor\" href=\"#合流-2\">#</a> 合流</h5>\n<h6 id=\"基于时间的合流-双流联结\"><a class=\"markdownIt-Anchor\" href=\"#基于时间的合流-双流联结\">#</a> 基于时间的合流 - 双流联结</h6>\n<p>窗口联结</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; operator1 = environment.fromElements(Tuple2.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>),Tuple2.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>),Tuple2.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; operator2 = environment.fromElements(Tuple3.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">11</span>), Tuple3.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>, <span class=\"number\">22</span>), Tuple3.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">33</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple3&lt;String, Integer, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 落在同一个时间范围内才能匹配</span></span><br><span class=\"line\"><span class=\"comment\">// 根据keyby的key，来进行关联</span></span><br><span class=\"line\"><span class=\"comment\">// 只能拿到匹配上的数据，类似inner join</span></span><br><span class=\"line\">DataStream&lt;String&gt; apply = operator1.join(operator2)</span><br><span class=\"line\">        .where(r1 -&gt; r1.f0)</span><br><span class=\"line\">        .equalTo(r2 -&gt; r2.f0)</span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">5</span>)))</span><br><span class=\"line\">        .apply(<span class=\"keyword\">new</span> <span class=\"title class_\">JoinFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">             * 关联上的数据，调用join</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> stringIntegerTuple2</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> stringIntegerIntegerTuple3</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">join</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2, Tuple3&lt;String, Integer, Integer&gt; stringIntegerIntegerTuple3)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> stringIntegerTuple2 + <span class=\"string\">&quot;&quot;</span> + stringIntegerIntegerTuple3;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">apply.print();</span><br></pre></td></tr></table></figure>\n<p>间隔联结</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; operator1 = environment.fromElements(Tuple2.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>),Tuple2.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>),Tuple2.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; operator2 = environment.fromElements(Tuple3.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">11</span>), Tuple3.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>, <span class=\"number\">22</span>), Tuple3.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">33</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple3&lt;String, Integer, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2StringKeyedStream = operator1.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; tuple3StringKeyedStream = operator2.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = tuple2StringKeyedStream.intervalJoin(tuple3StringKeyedStream)</span><br><span class=\"line\">        .between(Time.seconds(-<span class=\"number\">2</span>), Time.seconds(<span class=\"number\">2</span>))</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessJoinFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">             * 两条流数据匹配，调用方法</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> left The left element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> right The right element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> ctx A context that allows querying the timestamps of the left, right and joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             *     In addition, this context allows to emit elements on a side output.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> out The collector to emit resulting elements to.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;.Context ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                out.collect(left + <span class=\"string\">&quot;=====&quot;</span> + right);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.print();</span><br></pre></td></tr></table></figure>\n<p>处理迟到数据</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2StringKeyedStream = operator1.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; tuple3StringKeyedStream = operator2.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\"></span><br><span class=\"line\">OutputTag&lt;Tuple2&lt;String, Integer&gt;&gt; tupleOutputTag = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;ks1-late&quot;</span>, Types.TUPLE(Types.STRING, Types.INT));</span><br><span class=\"line\">OutputTag&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; tupleOutputTag2 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;ks2-late&quot;</span>, Types.TUPLE(Types.STRING, Types.INT, Types.INT));</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = tuple2StringKeyedStream.intervalJoin(tuple3StringKeyedStream)</span><br><span class=\"line\">        .between(Time.seconds(-<span class=\"number\">2</span>), Time.seconds(<span class=\"number\">2</span>))</span><br><span class=\"line\">        .sideOutputLeftLateData(tupleOutputTag)</span><br><span class=\"line\">        .sideOutputRightLateData(tupleOutputTag2)</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessJoinFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">             * 两条流数据匹配，调用方法</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> left The left element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> right The right element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> ctx A context that allows querying the timestamps of the left, right and joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             *     In addition, this context allows to emit elements on a side output.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> out The collector to emit resulting elements to.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;.Context ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                out.collect(left + <span class=\"string\">&quot;=====&quot;</span> + right);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.getSideOutput(tupleOutputTag).printToErr();</span><br><span class=\"line\">process.getSideOutput(tupleOutputTag2).printToErr();</span><br><span class=\"line\">process.print();</span><br></pre></td></tr></table></figure>\n<h4 id=\"处理函数\"><a class=\"markdownIt-Anchor\" href=\"#处理函数\">#</a> 处理函数</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId);</span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = waterSensorStringKeyedStream.process(<span class=\"keyword\">new</span> <span class=\"title class_\">KeyedProcessFunction</span>&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> value The input value.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> ctx A &#123;<span class=\"doctag\">@link</span> Context&#125; that allows querying the timestamp of the element and getting a</span></span><br><span class=\"line\"><span class=\"comment\">     *     &#123;<span class=\"doctag\">@link</span> TimerService&#125; for registering timers and querying the time. The context is only</span></span><br><span class=\"line\"><span class=\"comment\">     *     valid during the invocation of this method, do not store it.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> out The collector for returning result values.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(WaterSensor value, KeyedProcessFunction&lt;String, WaterSensor, String&gt;.Context ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 提取事件事件</span></span><br><span class=\"line\">        <span class=\"type\">Long</span> <span class=\"variable\">timestamp</span> <span class=\"operator\">=</span> ctx.timestamp();</span><br><span class=\"line\">        <span class=\"comment\">// 定时器</span></span><br><span class=\"line\">        <span class=\"type\">TimerService</span> <span class=\"variable\">timerService</span> <span class=\"operator\">=</span> ctx.timerService();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 注册定时器</span></span><br><span class=\"line\">        timerService.registerEventTimeTimer(<span class=\"number\">5000L</span>);</span><br><span class=\"line\">        System.out.println(timestamp + <span class=\"string\">&quot;===&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> timestamp The timestamp of the firing timer.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> ctx An &#123;<span class=\"doctag\">@link</span> OnTimerContext&#125; that allows querying the timestamp, the &#123;<span class=\"doctag\">@link</span></span></span><br><span class=\"line\"><span class=\"comment\">     *     TimeDomain&#125;, and the key of the firing timer and getting a &#123;<span class=\"doctag\">@link</span> TimerService&#125; for</span></span><br><span class=\"line\"><span class=\"comment\">     *     registering timers and querying the time. The context is only valid during the invocation</span></span><br><span class=\"line\"><span class=\"comment\">     *     of this method, do not store it.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> out The collector for returning result values.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onTimer</span><span class=\"params\">(<span class=\"type\">long</span> timestamp, KeyedProcessFunction&lt;String, WaterSensor, String&gt;.OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.onTimer(timestamp, ctx, out);</span><br><span class=\"line\">        System.out.println(timestamp + <span class=\"string\">&quot;=======---&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>定时器：</p>\n<p>keyed 才有</p>\n<p>事件时间定时器，通过 watermark 触发</p>\n<p>watermark &gt;=  注册时间</p>\n<p>watermark = 当前最大时间 - 等待时间 - 1</p>\n<p>在 process 中获取 watermark，显示的上一次的 watermaark，因为 process 还没收到这条新数据的 watermark</p>\n<h5 id=\"侧输出流\"><a class=\"markdownIt-Anchor\" href=\"#侧输出流\">#</a> 侧输出流</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>())</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class=\"number\">3</span>))</span><br><span class=\"line\">                .withTimestampAssigner((ele, ts) -&gt; ele.getTs() * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">OutputTag&lt;String&gt; warn11 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;warn&quot;</span>, Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">KeyedProcessFunction</span>&lt;String, WaterSensor, WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(WaterSensor value, KeyedProcessFunction&lt;String, WaterSensor, WaterSensor&gt;.Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 侧输出流</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (value.getVc() &gt; <span class=\"number\">10</span>) &#123;</span><br><span class=\"line\">                    ctx.output(warn11, <span class=\"string\">&quot;sss&quot;</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">// 主流</span></span><br><span class=\"line\">                out.collect(value);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.getSideOutput(warn11).printToErr();</span><br><span class=\"line\">process.print();</span><br></pre></td></tr></table></figure>\n<h4 id=\"状态管理\"><a class=\"markdownIt-Anchor\" href=\"#状态管理\">#</a> 状态管理</h4>\n<p>在 Flink 中，算子任务可以分为无状态和有状态两种情况。</p>\n<p>无状态的算子任务只需要观察每个独立事件，根据当前输入的数故据直接转换输出结果。我们之前讲到的基本转换算子，如 map、filter、flatMap, 计算时不依赖其他数据，就都属于无状态的算子。</p>\n<p>而有状态的算子任务，则除当前数据之外，还需要一些其他也数据来得到计算结果。这里的 &quot;其他数据&quot;, 就是所谓的状态 (state)。我们之前讲到的算子中，聚合算子、窗口算子都属于有状态的算子。</p>\n<p><strong>托管状态（Managed State）和原始状态（Raw State）</strong></p>\n<p>Flink 的状态有两种：托管状态（Managed State）和原始状态（Raw State）。托管状态就是由 Flink 统一管理的，状态的存储访问、故障恢复和重组等一系列问题都由 Flink 实现，我们只要调接口就可以；而原始状态则是自定义的，相当于就是开辟了一块内存，需要我们自己管理，实现状态的序列化和故障恢复。</p>\n<p>通常我们采用 Flink 托管状态来实现需求。</p>\n<p>托管状态分为算子状态和按键分区状态</p>\n<p>keyby 后的是按键分区状态，其他称为算子状态</p>\n<p><strong>算子状态（Operator State）和按键分区状态（Keyed State）</strong></p>\n<p>接下来我们的重点就是托管状态（Managed State）。</p>\n<p>我们知道在 Flink 中，一个算子任务会按照并行度分为多个并行子任务执行，而不同的子任务会占据不同的任务槽（task slot）。由于不同的 slot 在计算资源上是物理隔离的，所以 Flink 能管理的状态在并行任务间是无法共享的，每个状态只能针对当前子任务的实例有效。</p>\n<p>而很多有状态的操作（比如聚合、窗口）都是要先做 keyBy 进行按键分区的。按键分区之后，任务所进行的所有计算都应该只针对当前 key 有效，所以状态也应该按照 key 彼此隔离。在这种情况下，状态的访问方式又会有所不同。</p>\n<p>基于这样的想法，我们又可以将托管状态分为两类：算子状态和按键分区状态。</p>\n<p>算子状态可以用在所有算子上，使用的时候其实就跟一个本地变量没什么区别–因为本地变量的作用域也是当前任务实例。在使用时，我们还需进一步实现 ChneckpointedFunction 接口。</p>\n<h4 id=\"容错机制\"><a class=\"markdownIt-Anchor\" href=\"#容错机制\">#</a> 容错机制</h4>\n<p>在流处理中，我们可以用存档读档的思路，就是将之前某个日时间点所有的状态保存下来，这份 &quot;存档&quot; 就是所谓的 &quot;检查点&quot;(checkpoint)。</p>\n<p><strong>1）周期性的触发保存</strong></p>\n<p>“随时存档” 确实恢复起来方便，可是需要我们不停地做存档操作。如果每处理一条数据就进行检查点的保存，当大量数据同时到来时，就会耗费很多资源来频繁做检查点，数据处理的速度就会受到影响。所以在 Flink 中，检查点的保存是周期性触发的，间隔时间可以进行设置。</p>\n<p><strong>2）保存的时间点</strong></p>\n<p>我们应该在所有任务（算子）都恰好处理完一个相同的输入数据的时候，将它们的状态保存下来。</p>\n<p>这样做可以实现一个数据被所有任务（算子）完整地处理完，状态得到了保存。</p>\n<p>如果出现故障，我们恢复到之前保存的状态，故障时正在处理的所有数据都需要重新处理；我们只需要让源（source）任务向数据源重新提交偏移量、请求重放数据就可以了。当然这需要源任务可以把偏移量作为算子状态保存下来，而且外部数据源能够重置偏移量</p>\n<p>当发生故障时，就需要找到最近一次成功保存的检查点来恢复状态。</p>\n<p>(1) 重启应用</p>\n<p>遇到故障之后，第一步当然就是重启。我们将应用重新启动动后，所有任务的状态会清空。</p>\n<p>(2) 读取检查点，重置状态</p>\n<p>找到最近一次保存的检查点，从中读出每个算子任务状态的快照，分别填充到对应的状态中。这样，Flink 内部所有任务的状态，就恢复到了保存检查点的那一时刻，也就是刚好处理完第三个数据的时候。</p>\n<p>(3) 重置偏移量</p>\n<p>从检查点恢复状态后还有一个问题：如果直接继续处理数据，那么保存检查点之后、到发生故障这段时间内的数据，也就是第 4、5 个数据就相当于丢掉了；这会造成计算结果的错误。</p>\n<p>为了不丢数据，我们应该从保存检查点后开始重新读取数据，这可以通过 Source 任务向外部数据源重新提交偏移量 (offset) 来实现。</p>\n<p>(4) 继续处理数据</p>\n<p>接下来，我们就可以正常处理数据了。首先是重放第 4、5 个一数据，然后继续读取后面的数据。</p>\n<h4 id=\"flink-sql\"><a class=\"markdownIt-Anchor\" href=\"#flink-sql\">#</a> Flink SQL</h4>\n",
            "tags": [
                "大数据"
            ]
        },
        {
            "id": "http://example.com/2024/04/01/spark/",
            "url": "http://example.com/2024/04/01/spark/",
            "title": "spark",
            "date_published": "2024-04-01T05:38:45.000Z",
            "content_html": "<h2 id=\"spark\"><a class=\"markdownIt-Anchor\" href=\"#spark\">#</a> spark</h2>\n<p>分布式计算引擎框架，基于 mapreduce 开发</p>\n<p>单机：单进程，单节点</p>\n<p>伪分布式：多进程，单节点</p>\n<p>分布式：多进程，多节点</p>\n<p>分布式计算核心：切分数据，减少数据规模</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223237028.png\" alt=\"image-20240804223237028\"></p>\n<p>spark 分布式集群采用集群中心化</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223253379.png\" alt=\"image-20240804223253379\"></p>\n<p>框架：不完整的计算机程序 (核心功能已经开发完毕，但是是和业务相关的代码未开发)(MR，spark)</p>\n<p>系统：完整的计算机程序 (HDFS,Kafka)</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223306758.png\" alt=\"image-20240804223306758\"></p>\n<p>引擎：核心功能</p>\n<p>spark 基于 mr 开发，两者区别</p>\n<p>1. 开发语言：mr：java，不适合进行大量数据处理。spark：scala，适合大量数据处理，封装大量功能</p>\n<p>2. 处理方式：hadoop 出现的早，只考虑单一的计算操作</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223319497.png\" alt=\"image-20240804223319497\"></p>\n<p>spark 优化了计算过程</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223331386.png\" alt=\"image-20240804223331386\"></p>\n<p>回顾：Hadoop 主要解决，海量数据的存储和海量数据的分析计算。</p>\n<p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223347611.png\" alt=\"image-20240804223347611\"></p>\n<p>spark 内置模块</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223358385.png\" alt=\"image-20240804223358385\"></p>\n<h3 id=\"部署spark集群\"><a class=\"markdownIt-Anchor\" href=\"#部署spark集群\">#</a> 部署 spark 集群</h3>\n<p>部署 Spark 其实指的就是 Spark 的程序逻辑在什么资源中执行</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223413816.png\" alt=\"image-20240804223413816\"></p>\n<p>如果资源是当前单节点提供的，那么就称之为单机模式</p>\n<p>如果资源是当前多节点提供的，那么就称之为分布式模式</p>\n<p>如果资源是由 Yarn 提供的，那么就称之为 Yarn 部署环境</p>\n<p>如果资源是由 Spark 提供的，那么就称之为 Spark 部署环境 (Standalone</p>\n<p>生产环境中主要采用：yarn+spark  也称之为（spark on yarna）</p>\n<p>(1) Local 模式：在本地部署单个 Spark 服务</p>\n<p>(2) Standalone 模式：Spark 自带的任务调度模式。(国内不常用)</p>\n<p>(3) YARN 模式：Spark 使用 Hadoop 的 YARN 组件进行资源与任务调度。(国内最常用)</p>\n<p>(4) Mesos 模式：Spark 使用 Mesos 平台进行资源与任务的调度。(国内很少用)</p>\n<h4 id=\"部署local\"><a class=\"markdownIt-Anchor\" href=\"#部署local\">#</a> 部署 local</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https:<span class=\"comment\">//dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz</span></span><br><span class=\"line\">tar xzvf spark-<span class=\"number\">3.4</span><span class=\"number\">.3</span>-bin-hadoop3.tgz</span><br><span class=\"line\"></span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master local[<span class=\"number\">2</span>] ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">--master 指定资源提供者</span><br><span class=\"line\">local 单线程</span><br><span class=\"line\">local[<span class=\"number\">2</span>] 两个线程执行</span><br><span class=\"line\">local[*] 使用全部核</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO DAGScheduler: Job <span class=\"number\">0</span> finished: reduce at SparkPi.scala:<span class=\"number\">38</span>, took <span class=\"number\">1.055199</span> s</span><br><span class=\"line\">Pi is roughly <span class=\"number\">3.1425071142507113</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO SparkContext: SparkContext is stopping with exitCode <span class=\"number\">0.</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO SparkUI: Stopped Spark web UI at http:<span class=\"comment\">//hadoop100:4040</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO MemoryStore: MemoryStore cleared</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO BlockManager: BlockManager stopped</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO SparkContext: Successfully stopped SparkContext</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO ShutdownHookManager: Shutdown hook called</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223435814.png\" alt=\"image-20240804223435814\"></p>\n<p>Spark 在运行时，会启动进程，申请资源，执行计算，但是一旦计算完毕，那么进程会停止，资源会释放掉</p>\n<p>Stopped Spark web UI at <span class=\"exturl\" data-url=\"aHR0cDovL2hhZG9vcDEwMDo0MDQw\">http://hadoop100:4040</span></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223450585.png\" alt=\"image-20240804223450585\"></p>\n<h4 id=\"yarn模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn模式\">#</a> yarn 模式</h4>\n<p>编辑启动关闭脚本</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">echo <span class=\"string\">&quot;========hadoop100==========&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;/root/jdk8u352-b08/bin/jps&quot;</span></span><br><span class=\"line\">echo <span class=\"string\">&quot;========hadoop101==========&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop101</span> <span class=\"string\">&quot;jps&quot;</span></span><br><span class=\"line\">echo <span class=\"string\">&quot;========hadoop102==========&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop102</span> <span class=\"string\">&quot;jps&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/start-dfs.sh&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop101</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/start-yarn.sh&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/bin/mapred --daemon start historyserver&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/bin/mapred --daemon stop historyserver&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop101</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/stop-yarn.sh&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/stop-dfs.sh&quot;</span></span><br><span class=\"line\">vim spark-env.sh</span><br><span class=\"line\">YARN_CONF_DIR=/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/etc/hadoop/</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop-start.sh</span><br><span class=\"line\"></span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master yarn ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# jpsall </span><br><span class=\"line\">========hadoop100==========</span><br><span class=\"line\"><span class=\"number\">42388</span> DataNode</span><br><span class=\"line\"><span class=\"number\">42683</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">43820</span> Jps</span><br><span class=\"line\"><span class=\"number\">42894</span> JobHistoryServer</span><br><span class=\"line\"><span class=\"number\">43583</span> SparkSubmit</span><br><span class=\"line\"><span class=\"number\">42191</span> NameNode</span><br><span class=\"line\">========hadoop101==========</span><br><span class=\"line\"><span class=\"number\">37904</span> Jps</span><br><span class=\"line\"><span class=\"number\">37169</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">36818</span> ResourceManager</span><br><span class=\"line\"><span class=\"number\">36610</span> DataNode</span><br><span class=\"line\"><span class=\"number\">37843</span> YarnCoarseGrainedExecutorBackend</span><br><span class=\"line\"><span class=\"number\">37721</span> ExecutorLauncher</span><br><span class=\"line\">========hadoop102==========</span><br><span class=\"line\"><span class=\"number\">37362</span> DataNode</span><br><span class=\"line\"><span class=\"number\">37509</span> SecondaryNameNode</span><br><span class=\"line\"><span class=\"number\">37626</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">38093</span> Jps</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223505654.png\" alt=\"image-20240804223505654\"></p>\n<p>配置历史服务</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim spark-defaults.conf</span><br><span class=\"line\">spark.eventLog.enabled <span class=\"literal\">true</span></span><br><span class=\"line\">spark.eventLog.dir hdfs:<span class=\"comment\">//hadoop100:8020/directory</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim spark-env.sh</span><br><span class=\"line\">export SPARK_HISTORY_OPTS=<span class=\"string\">&quot;-Dspark.history.ui.port=18080 -Dspark.history.fs.logDirectory=hdfs://hadoop100:8020/directory -Dspark.history.retainedApplications=30&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">vim spark-defaults.conf</span><br><span class=\"line\">spark.yarn.historyServer.address=hadoop100:<span class=\"number\">18080</span></span><br><span class=\"line\">spark.history.ui.port=<span class=\"number\">18080</span></span><br></pre></td></tr></table></figure>\n<p>hdfs 中创建 /directory</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223520765.png\" alt=\"image-20240804223520765\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sbin/start-history-server.sh</span><br><span class=\"line\"></span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master yarn ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">将spark历史记录保存到了hadoop history中</span><br></pre></td></tr></table></figure>\n<p>运行时，会将 yarn 需要用到的 lib 和 conf 上传到 hdfs 中</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">32</span>:<span class=\"number\">57</span> INFO Client: Uploading resource file:/tmp/spark-4dbf287c-<span class=\"number\">9986</span>-44f0-89c1-7ede052800c0/__spark_libs__5882056198812431005.zip -&gt; hdfs:<span class=\"comment\">//hadoop100:8020/user/root/.sparkStaging/application_1718182778487_0005/__spark_libs__5882056198812431005.zip</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">32</span>:<span class=\"number\">58</span> INFO Client: Uploading resource file:/tmp/spark-4dbf287c-<span class=\"number\">9986</span>-44f0-89c1-7ede052800c0/__spark_conf__3646583648306474590.zip -&gt; hdfs:<span class=\"comment\">//hadoop100:8020/user/root/.sparkStaging/application_1718182778487_0005/__spark_conf__.zip</span></span><br></pre></td></tr></table></figure>\n<p>运行结束会删除文件</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">33</span>:<span class=\"number\">07</span> INFO ShutdownHookManager: Deleting directory /tmp/spark-f3d8debb-<span class=\"number\">7304</span>-<span class=\"number\">4251</span>-b80b-11b1ab91f45c</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">33</span>:<span class=\"number\">07</span> INFO ShutdownHookManager: Deleting directory /tmp/spark-4dbf287c-<span class=\"number\">9986</span>-44f0-89c1-7ede052800c0</span><br></pre></td></tr></table></figure>\n<p>yarn 模式中有 client 和 cluster 模式，主要区别在于：Driver 程序的运行节点。</p>\n<p>yarn-client:Driver 程序运行在客户端，适用于交互、调试，希望立即看到 Japp 的输出。</p>\n<p>yarn-cluster:Driver 程序运行在由 ResourceManager 启动的 APPIMaster, 适用于生产环境。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223534005.png\" alt=\"image-20240804223534005\"></p>\n<p>默认使用的客户端模式</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 使用cluster模式</span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">========hadoop100==========</span><br><span class=\"line\"><span class=\"number\">42388</span> DataNode</span><br><span class=\"line\"><span class=\"number\">51081</span> Jps</span><br><span class=\"line\"><span class=\"number\">42683</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">44508</span> HistoryServer</span><br><span class=\"line\"><span class=\"number\">42894</span> JobHistoryServer</span><br><span class=\"line\"><span class=\"number\">50959</span> SparkSubmit</span><br><span class=\"line\"><span class=\"number\">42191</span> NameNode</span><br><span class=\"line\">========hadoop101==========</span><br><span class=\"line\"><span class=\"number\">37169</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">36818</span> ResourceManager</span><br><span class=\"line\"><span class=\"number\">36610</span> DataNode</span><br><span class=\"line\"><span class=\"number\">40596</span> Jps</span><br><span class=\"line\">========hadoop102==========</span><br><span class=\"line\"><span class=\"number\">37362</span> DataNode</span><br><span class=\"line\"><span class=\"number\">37509</span> SecondaryNameNode</span><br><span class=\"line\"><span class=\"number\">42121</span> Jps</span><br><span class=\"line\"><span class=\"number\">37626</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">41995</span> ApplicationMaster</span><br></pre></td></tr></table></figure>\n<h4 id=\"standalone模式\"><a class=\"markdownIt-Anchor\" href=\"#standalone模式\">#</a> standalone 模式</h4>\n<p>Standalone 模式是 Spark 自带的资源调度引擎，构建一个由 Master+VVorker 构成的 Spark 集群，Spark 运行在集群中。</p>\n<p>这个要和 Hadoop 中的 Standalone 区别开来。这里的 Standalone 是指只用 Spark 来搭建一个集群，不需要借助 Hadoop 的 Yarn 和 Mesos 等其他框架。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223542959.png\" alt=\"image-20240804223542959\"></p>\n<h4 id=\"mesos模式\"><a class=\"markdownIt-Anchor\" href=\"#mesos模式\">#</a> mesos 模式</h4>\n<p>Spark 客户端直接连接 Mesos; 不需要额外构建 Spark 集群。国内应用比较少，更多的是运用 Yarn 调度。</p>\n<h4 id=\"模式对比\"><a class=\"markdownIt-Anchor\" href=\"#模式对比\">#</a> 模式对比</h4>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223557692.png\" alt=\"image-20240804223557692\"></p>\n<h4 id=\"端口号\"><a class=\"markdownIt-Anchor\" href=\"#端口号\">#</a> 端口号</h4>\n<p>1) Spark 查看当前 Spark-shell 运行任务情况端口号：4040</p>\n<p>2) Spark 历史服务器端口号：18080 (类比于 Hadoop 历史服务器端口号：19888)</p>\n<h3 id=\"rdd\"><a class=\"markdownIt-Anchor\" href=\"#rdd\">#</a> rdd</h3>\n<p>RDD: 分布式计算模型</p>\n<p>1. 一定是一个对象</p>\n<p>2. 一定封装了大量方法和属性</p>\n<p>3. 一定需要适合进行分布式处理 (减小数据规模，并行计算算)</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223609055.png\" alt=\"image-20240804223609055\"></p>\n<h4 id=\"rdd编程\"><a class=\"markdownIt-Anchor\" href=\"#rdd编程\">#</a> RDD 编程</h4>\n<p>在 Spark 中创建 RDD 的创建方式可以分为三种：从集合中创建] RDD、从外部存储创建 RDD、从其他 RDD 创建。</p>\n<p>RDD 的处理方式和 JavalO 流完全一样，也采用装饰者设计式来实现功能的</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spark-core_2<span class=\"number\">.12</span>&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">3.3</span><span class=\"number\">.1</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.example;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Main</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"comment\">//JavaSparkContext javaSparkContext = new JavaSparkContext(&quot;local&quot;,&quot;spark&quot;);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"对接内存数据构建rdd对象\"><a class=\"markdownIt-Anchor\" href=\"#对接内存数据构建rdd对象\">#</a> 对接内存数据构建 RDD 对象</h4>\n<p>parallelize 方法可以传递参数：集合</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package org.<span class=\"property\">example</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.<span class=\"property\">apache</span>.<span class=\"property\">spark</span>.<span class=\"property\">SparkConf</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.<span class=\"property\">apache</span>.<span class=\"property\">spark</span>.<span class=\"property\">api</span>.<span class=\"property\">java</span>.<span class=\"property\">JavaRDD</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.<span class=\"property\">apache</span>.<span class=\"property\">spark</span>.<span class=\"property\">api</span>.<span class=\"property\">java</span>.<span class=\"property\">JavaSparkContext</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.<span class=\"property\">util</span>.<span class=\"property\">Arrays</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.<span class=\"property\">util</span>.<span class=\"property\">List</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">public <span class=\"keyword\">class</span> <span class=\"title class_\">Main</span> &#123;</span><br><span class=\"line\">    public <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span>(<span class=\"params\"><span class=\"built_in\">String</span>[] args</span>) &#123;</span><br><span class=\"line\">        <span class=\"title class_\">System</span>.<span class=\"property\">out</span>.<span class=\"title function_\">println</span>(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"title class_\">SparkConf</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setMaster</span>(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setAppName</span>(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"title class_\">JavaSparkContext</span> javaSparkContext = <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"comment\">//JavaSparkContext javaSparkContext = new JavaSparkContext(&quot;local&quot;,&quot;spark&quot;);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 对接数据源</span></span><br><span class=\"line\">        <span class=\"title class_\">List</span>&lt;<span class=\"title class_\">String</span>&gt; names = <span class=\"title class_\">Arrays</span>.<span class=\"title function_\">asList</span>(<span class=\"string\">&quot;zhangsan&quot;</span>, <span class=\"string\">&quot;lisi&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">JavaRDD</span>&lt;<span class=\"title class_\">String</span>&gt; rdd = javaSparkContext.<span class=\"title function_\">parallelize</span>(names);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">List</span>&lt;<span class=\"title class_\">String</span>&gt; collect = rdd.<span class=\"title function_\">collect</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        collect.<span class=\"title function_\">forEach</span>(<span class=\"title class_\">System</span>.<span class=\"property\">out</span>::println);</span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.<span class=\"title function_\">close</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"对接磁盘数据\"><a class=\"markdownIt-Anchor\" href=\"#对接磁盘数据\">#</a> 对接磁盘数据</h4>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public <span class=\"keyword\">class</span> <span class=\"title class_\">Main1</span> &#123;</span><br><span class=\"line\">    public <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span>(<span class=\"params\"><span class=\"built_in\">String</span>[] args</span>) &#123;</span><br><span class=\"line\">        <span class=\"title class_\">System</span>.<span class=\"property\">out</span>.<span class=\"title function_\">println</span>(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"title class_\">SparkConf</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setMaster</span>(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setAppName</span>(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"title class_\">JavaSparkContext</span> javaSparkContext = <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">JavaRDD</span>&lt;<span class=\"title class_\">String</span>&gt; stringJavaRDD = javaSparkContext.<span class=\"title function_\">textFile</span>(<span class=\"string\">&quot;C:\\\\Users\\\\Administrator\\\\IdeaProjects\\\\MapReduceDemo\\\\data\\\\text&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">List</span>&lt;<span class=\"title class_\">String</span>&gt; collect = stringJavaRDD.<span class=\"title function_\">collect</span>();</span><br><span class=\"line\">        collect.<span class=\"title function_\">forEach</span>(<span class=\"title class_\">System</span>.<span class=\"property\">out</span>::println);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.<span class=\"title function_\">close</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"磁盘数据分区\"><a class=\"markdownIt-Anchor\" href=\"#磁盘数据分区\">#</a> 磁盘数据分区</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">partition</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// idea开发时相对路径默认以项目根路径为基准</span></span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(&quot;C:\\\\Users\\\\Administrator\\\\IdeaProjects\\\\MapReduceDemo\\\\data\\\\text&quot;);</span></span><br><span class=\"line\">        <span class=\"comment\">// textfile 第二个参数最小分区数，不传递的时候使用默认值</span></span><br><span class=\"line\">        <span class=\"comment\">// 1.textFile可以传递第二个参数:minPartitions(最小分区数)</span></span><br><span class=\"line\">        <span class=\"comment\">//参数可以不需要传递的,那么Spark会采用默认值</span></span><br><span class=\"line\">        <span class=\"comment\">//minPartitions = math.min(defaultParallelism,2)</span></span><br><span class=\"line\">        <span class=\"comment\">//2.使用配置参数:spark.default.parallelism=&gt;4=&gt; 4=&gt; math.min(参数,2)</span></span><br><span class=\"line\">        <span class=\"comment\">//3.采用环境默认总核值=&gt;math.min(总核数,2)</span></span><br><span class=\"line\">        </span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data\\\\text&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        stringJavaRDD.saveAsTextFile(<span class=\"string\">&quot;output1222&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"内存数据源分区数据分配\"><a class=\"markdownIt-Anchor\" href=\"#内存数据源分区数据分配\">#</a> 内存数据源，分区数据分配</h4>\n<p>​      数据分配方式       (i*length)/numSlices,(((i + 1) * 1) * length) /numSlices)</p>\n<h4 id=\"磁盘数据源\"><a class=\"markdownIt-Anchor\" href=\"#磁盘数据源\">#</a> 磁盘数据源</h4>\n<p>Spark 不支持文件操作的。文件操作都是由 Hadoop 完成的</p>\n<p>Hadoop 进行文件切片数量的计算和文件数据存储计算规则不样</p>\n<p>1. 分区数量计算的时候，考虑的是尽可能的平均：按字节来计算</p>\n<p>2. 分区数据的存储是考虑业务数据的完整性：按照行来读取</p>\n<p>读取数据时，还需要考虑数据偏移量，偏移量从 0 开始的。</p>\n<p>读取数据时，相同的偏移量不能重复读取。</p>\n<p>使用 spark 时，数据不能全放一行，会造成数据倾斜</p>\n<h4 id=\"transformation-转换算子\"><a class=\"markdownIt-Anchor\" href=\"#transformation-转换算子\">#</a> transformation 转换算子</h4>\n<h5 id=\"map\"><a class=\"markdownIt-Anchor\" href=\"#map\">#</a> map</h5>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223623531.png\" alt=\"image-20240804223623531\"></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Scala</span>语言中可以将无关的数据封装在一起,形成一个整体,称之为元素的组合,简称为【元组】</span><br><span class=\"line\">如果想要访问元组中的数据,必须采用顺序号</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> kv1 = (<span class=\"string\">&quot;haha&quot;</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"type\">JDK1</span><span class=\"number\">.8</span>以后也存在元组,采用特殊的类:<span class=\"type\">TupleX</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">String</span>&gt; tuple2 = <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;abc&quot;</span>, <span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">System</span>.out.println(tuple2._1);</span><br><span class=\"line\">        <span class=\"type\">System</span>.out.println(tuple2._2());</span><br><span class=\"line\">        tuple中最大容量为<span class=\"number\">22</span></span><br><span class=\"line\">        使用时可以 ._1 也可以 ._1()</span><br></pre></td></tr></table></figure>\n<h6 id=\"函数式编程\"><a class=\"markdownIt-Anchor\" href=\"#函数式编程\">#</a> 函数式编程</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.rdd;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Arrays;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">operator3</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>),<span class=\"number\">2</span>).map(NumberTest::mul2).collect().forEach(System.out::println);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NumberTest</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span>  <span class=\"type\">int</span> <span class=\"title function_\">mul2</span><span class=\"params\">(Integer num2)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num2 *= <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223648424.png\" alt=\"image-20240804223648424\"></p>\n<p>RDD 不会保存数据，不会等每一个 rdd 执行完再执行下一个</p>\n<h5 id=\"filter\"><a class=\"markdownIt-Anchor\" href=\"#filter\">#</a> filter</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">filter</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// TODO filter 过滤 对数据源中数据进行筛选  满足保留，不满足丢弃</span></span><br><span class=\"line\">        <span class=\"comment\">// 返回结果为true满足 ，返回false不满足</span></span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Integer&gt; filter = parallelize111.filter(new Function&lt;Integer, Boolean&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//            @Override</span></span><br><span class=\"line\"><span class=\"comment\">//            public Boolean call(Integer num) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                return true;</span></span><br><span class=\"line\"><span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;);</span></span><br><span class=\"line\"><span class=\"comment\">//        filter.collect().forEach(System.out::println);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; filterrdd = parallelize111.filter(</span><br><span class=\"line\">                num -&gt; <span class=\"literal\">true</span></span><br><span class=\"line\">        );</span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Integer&gt; filterrdd111 = parallelize111.filter(</span></span><br><span class=\"line\"><span class=\"comment\">//                num -&gt; (num % 2 ==1)</span></span><br><span class=\"line\"><span class=\"comment\">//        );</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">/// filter 执行过程中可能会造成数据倾斜</span></span><br><span class=\"line\">        filterrdd.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"flatmap\"><a class=\"markdownIt-Anchor\" href=\"#flatmap\">#</a> flatmap</h5>\n<p>数据扁平化，扁平映射（整体变为个体）</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">flatmap</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>), Arrays.asList(<span class=\"number\">3</span>, <span class=\"number\">4</span>)), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Integer&gt; integerJavaRDD = parallelizerdd.flatMap(new FlatMapFunction&lt;List&lt;Integer&gt;, Integer&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                     @Override</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                     public Iterator&lt;Integer&gt; call(List&lt;Integer&gt; integers) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                         return integers.iterator();</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                     &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                 &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        );</span></span><br><span class=\"line\"></span><br><span class=\"line\">                JavaRDD&lt;Integer&gt; integerJavaRDD = parallelizerdd.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;List&lt;Integer&gt;, Integer&gt;() &#123;</span><br><span class=\"line\">                                                                     <span class=\"meta\">@Override</span></span><br><span class=\"line\">                                                                     <span class=\"keyword\">public</span> Iterator&lt;Integer&gt; <span class=\"title function_\">call</span><span class=\"params\">(List&lt;Integer&gt; integers)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                                                                         List&lt;Integer&gt; objectArrayList = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;&gt;();</span><br><span class=\"line\">                                                                         integers.forEach(num -&gt; objectArrayList.add(num * <span class=\"number\">2</span>));</span><br><span class=\"line\">                                                                         <span class=\"keyword\">return</span> objectArrayList.iterator();</span><br><span class=\"line\">                                                                     &#125;</span><br><span class=\"line\">                                                                 &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">        integerJavaRDD.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">data/text：</span><br><span class=\"line\">hadoop python</span><br><span class=\"line\">java php golang</span><br><span class=\"line\">B</span><br><span class=\"line\">V</span><br><span class=\"line\">D</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">flatmap</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>), Arrays.asList(<span class=\"number\">3</span>, <span class=\"number\">4</span>)), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data/text&quot;</span>);</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"groupby\"><a class=\"markdownIt-Anchor\" href=\"#groupby\">#</a> groupby</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">groupby</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 按照指定的规则分组数据</span></span><br><span class=\"line\"><span class=\"comment\">//        parallelizerdd.groupBy(new Function&lt;Integer, Object&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//            public Object call(Integer integers) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                // 返回值是数据对应组的名称，相同名称的数据防止在同一个组中</span></span><br><span class=\"line\"><span class=\"comment\">//                if (integers % 2 == 0)</span></span><br><span class=\"line\"><span class=\"comment\">//                &#123;return &quot;123&quot;;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">//                else &#123;return &quot;456&quot;;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;).collect().forEach(System.out::println);</span></span><br><span class=\"line\"><span class=\"comment\">//(123,[2, 4, 6])</span></span><br><span class=\"line\"><span class=\"comment\">//(456,[1])</span></span><br><span class=\"line\">        parallelizerdd.groupBy(num -&gt; num % <span class=\"number\">2</span> == <span class=\"number\">0</span>).collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"shuffle\"><a class=\"markdownIt-Anchor\" href=\"#shuffle\">#</a> shuffle</h5>\n<p>默认情况下，数据处理后，所在的分区不会发生变化，但是 groupBy 方法例外</p>\n<p>Spark 在数据处理中，要求同一个组的数据必须在同一个分区中</p>\n<p>所以分组操作会将数据分区打乱重新组合，在 spark 中称为 shuffle</p>\n<p>一个分区可以存放多个组，，所有数据必须分组后才能继续执行操作</p>\n<p>RDD 对象不能保存数据，当前 groupBy 操作会将数据保存到磁盘文件中，保证数据全部分组后执行后续操作</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223703724.png\" alt=\"image-20240804223703724\"></p>\n<p>shuffle 操作一定会落盘</p>\n<p>shuffle 操作有可能会导致资源浪费</p>\n<p>Spark 中含有 shuffle 操作的方法都有改变分区的能力</p>\n<p>RDD 的分区和 task 有关系</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">groupby2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>), <span class=\"number\">3</span>);</span><br><span class=\"line\">        parallelizerdd.groupBy(num -&gt; num % <span class=\"number\">2</span> == <span class=\"number\">0</span>,<span class=\"number\">2</span>).collect().forEach(System.out::println);</span><br><span class=\"line\">        Thread.sleep(<span class=\"number\">100000L</span>);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>shuffle 会将完整的计算流程一分为二，其中一部分任务会写磁盘，另外一部分的任务会读磁盘</p>\n<p>写磁盘的操作不完成，不允许读磁盘</p>\n<h5 id=\"distinct\"><a class=\"markdownIt-Anchor\" href=\"#distinct\">#</a> distinct</h5>\n<p>hashset 是单点去重</p>\n<p>distinct 是分布式去重，采用分组 + shuffle 方式</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">distinct</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>), <span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        parallelizerdd.distinct().collect().forEach(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//parallelizerdd.groupBy(num -&gt; num % 2 == 0,2).collect().forEach(System.out::println);</span></span><br><span class=\"line\">        <span class=\"comment\">//Thread.sleep(100000L);</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"sortby\"><a class=\"markdownIt-Anchor\" href=\"#sortby\">#</a> sortby</h5>\n<p>按照指定规则排序</p>\n<p>第一个参数表示排序规则</p>\n<p>Spark 会为每一个数据增加一个标记，然后按照标记对数据进行排序</p>\n<p>第二个参数表示排序的方式：升序 (true), 降序 (false)</p>\n<p>第三个参数表示分区数量</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sortby</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">12</span>, <span class=\"number\">52</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>), <span class=\"number\">3</span>);</span><br><span class=\"line\">        parallelizerdd.saveAsTextFile(<span class=\"string\">&quot;sort222&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        parallelizerdd.sortBy(<span class=\"keyword\">new</span> <span class=\"title class_\">Function</span>&lt;Integer, Object&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> Object <span class=\"title function_\">call</span><span class=\"params\">(Integer integerssss)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> integerssss;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;,<span class=\"literal\">true</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">               <span class=\"comment\">// .collect()</span></span><br><span class=\"line\">                <span class=\"comment\">//.forEach(System.out::println);</span></span><br><span class=\"line\">                .saveAsTextFile(<span class=\"string\">&quot;sort333&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//parallelizerdd.groupBy(num -&gt; num % 2 == 0,2).collect().forEach(System.out::println);</span></span><br><span class=\"line\">        <span class=\"comment\">//Thread.sleep(100000L);</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>return “”+integerssss;    // 按照字典排序</p>\n<h5 id=\"coalesce\"><a class=\"markdownIt-Anchor\" href=\"#coalesce\">#</a> coalesce</h5>\n<p>缩减分区</p>\n<p>coalesce 方法默认没有 shuffle 功能，所以数据不会被打击乱重新组合，所以如果要扩大分区是无法实现的</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filterrdd.coalesce(<span class=\"number\">3</span>,<span class=\"literal\">true</span>);    <span class=\"comment\">// shuffle 为true的时候可以扩大分区</span></span><br></pre></td></tr></table></figure>\n<h5 id=\"repartition\"><a class=\"markdownIt-Anchor\" href=\"#repartition\">#</a> repartition</h5>\n<p>重分区，就是设定 shuffle=true 的 coalesce 方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filterrdd.repartition(<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"kv\"><a class=\"markdownIt-Anchor\" href=\"#kv\">#</a> kv</h4>\n<p>Spark RDD 会整体数据的处理就称之为单值类型的数据处理</p>\n<p>Spark RDD 会 KV 数据个体的处理就称之为 KV 类型的数据处理：K 和 V 不作为整体使用</p>\n<p>mapValues 方法只对 V 进行处理，K 不做任何操作</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kv</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(1, 2, 4 ,6, 7, 8, 1, 2), 3);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        Tuple2&lt;String, Integer&gt; a = <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>);</span><br><span class=\"line\">        Tuple2&lt;String, Integer&gt; b = <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Arrays.asList(a, b);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaPairRDD&lt;String, Integer&gt; parallelized = javaSparkContext.parallelizePairs(list);</span><br><span class=\"line\">        parallelized.mapValues(num -&gt; num*<span class=\"number\">2</span>).collect().forEach(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kv2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(1, 2, 4 ,6, 7, 8, 1, 2), 3);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        List&lt;Integer&gt; integerList = Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; integerJavaRDD = javaSparkContext.parallelize(integerList);</span><br><span class=\"line\"></span><br><span class=\"line\">        integerJavaRDD.mapToPair(</span><br><span class=\"line\">                num -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(num, num*<span class=\"number\">2</span>)</span><br><span class=\"line\">        ).collect().forEach(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        integerJavaRDD.mapToPair(</span></span><br><span class=\"line\"><span class=\"comment\">//                num -&gt; new Tuple2&lt;&gt;(num, num*2)</span></span><br><span class=\"line\"><span class=\"comment\">//        ).mapValues(num-&gt;num*2).collect().forEach(System.out::println);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"mapvalue\"><a class=\"markdownIt-Anchor\" href=\"#mapvalue\">#</a> mapvalue</h5>\n<h5 id=\"groupbykey\"><a class=\"markdownIt-Anchor\" href=\"#groupbykey\">#</a> groupbykey</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kv3_groupbykey</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; parallelized = javaSparkContext.parallelize(Arrays.asList(new Tuple2&lt;&gt;(&quot;a&quot;, 1), new Tuple2&lt;&gt;(&quot;b&quot;, 2), new Tuple2&lt;&gt;(&quot;a&quot;, 3), new Tuple2&lt;&gt;(&quot;b&quot;, 4)));</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        JavaPairRDD&lt;String, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; stringIterableJavaPairRDD = parallelized.groupBy(t -&gt; t._1);</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        stringIterableJavaPairRDD.collect().forEach(System.out::println);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// (a,[(a,1), (a,3)])</span></span><br><span class=\"line\">        <span class=\"comment\">//(b,[(b,2), (b,4)])</span></span><br><span class=\"line\"></span><br><span class=\"line\">       javaSparkContext.parallelizePairs((Arrays.asList(<span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">3</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">4</span>)))).groupByKey().collect().forEach(System.out::println);</span><br><span class=\"line\">       javaSparkContext.close();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//(a,[1, 3])</span></span><br><span class=\"line\"><span class=\"comment\">//(b,[2, 4])</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>groupby 底层调用 greoupbykey</p>\n<p>groupbykey 有 shuffle</p>\n<h5 id=\"reducebykey\"><a class=\"markdownIt-Anchor\" href=\"#reducebykey\">#</a> reducebykey</h5>\n<p>reduceByKey 方法的作用：将 KV 类型的数据按照 K 对 V 进行 reduce (将多个值聚合成 1 个值) 操作</p>\n<p>基本思想：两两计算</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaPairRDD&lt;String ,Integer&gt; wordcountrdd = parallelized.reduceByKey(Integer::sum);</span><br></pre></td></tr></table></figure>\n<h5 id=\"sortbykey\"><a class=\"markdownIt-Anchor\" href=\"#sortbykey\">#</a> sortbykey</h5>\n<p>groupByKey : 按照 K 对 V 进行分组</p>\n<p>reduceByKey 按照 K 对进行两两聚合</p>\n<p>sortByKey 按照 K 排序</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaPairRDD&lt;String ,Integer&gt; sortrdd = parallelized.sortByKey();</span><br></pre></td></tr></table></figure>\n<h4 id=\"rdd行动算子\"><a class=\"markdownIt-Anchor\" href=\"#rdd行动算子\">#</a> RDD 行动算子</h4>\n<p>RDD 的行动算子会触发作业 (Job) 的执行</p>\n<p>转换算子的目的：将旧的 RDD 转换成新的 RDD, 为了组合多个 RDD 的功能</p>\n<p>返回值是 rdd，是转换算子。具体值是行动算子</p>\n<p>collect 将 executor 执行的结果按照分区的数据拉取回到 driver，将结果组合成集合对象</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaRDD&lt;Integer&gt; integerJavaRDD = parallelize111.map(num -&gt; num * <span class=\"number\">2</span>);</span><br><span class=\"line\"><span class=\"comment\">// collect 是行动算子</span></span><br><span class=\"line\">List&lt;Integer&gt; collect = integerJavaRDD.collect();</span><br><span class=\"line\">collect.forEach(System.out::println);</span><br></pre></td></tr></table></figure>\n<p>Spark 在编写代码时，调用转换算子，并不会真正执行，因为只是在 Driver 端组合功能</p>\n<p>所以当前的代码其实就是在 Driver 端执行</p>\n<p>所以当前 main 方法也称之为 driver 方法，当前运行 main 纟我程，也称之 Driver 线程</p>\n<p>转换算子中的逻辑代码是在 Executor 端执行的。并不会在 tDriver 端调用和执行。</p>\n<p>RDD 封装的逻辑其实就是转换算子中的逻辑</p>\n<p>集合数据</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223718854.png\" alt=\"image-20240804223718854\"></p>\n<p>文件：读取切片</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223732201.png\" alt=\"image-20240804223732201\"></p>\n<p>collect 方法就是将 Executor 端执行的结果按照分区的顺序位取 (采集) 回到 Driver 端，将结果组合成集合对象</p>\n<p>collect 方法可能会导致多个 Executor 的大量数据拉取到 Driiver 端，导致内存溢出，所以生成环境慎用</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>),<span class=\"number\">2</span>);</span><br><span class=\"line\"> JavaRDD&lt;Integer&gt; integerJavaRDD = parallelize111.map(num -&gt; num * <span class=\"number\">2</span>);</span><br><span class=\"line\"> <span class=\"comment\">// collect 是行动算子,用于采集数据</span></span><br><span class=\"line\"> List&lt;Integer&gt; collect = integerJavaRDD.collect();</span><br><span class=\"line\"> collect.forEach(System.out::println);</span><br><span class=\"line\"> <span class=\"comment\">// count获取结果数量</span></span><br><span class=\"line\"> <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> integerJavaRDD.count();</span><br><span class=\"line\"> <span class=\"comment\">// 获取结果的第一个</span></span><br><span class=\"line\"> <span class=\"type\">Integer</span> <span class=\"variable\">first</span> <span class=\"operator\">=</span> integerJavaRDD.first();</span><br><span class=\"line\"> <span class=\"comment\">// 从结果中获取前n个</span></span><br><span class=\"line\"> List&lt;Integer&gt; take = integerJavaRDD.take(<span class=\"number\">3</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\">// countbykey 将结果按照key计算数量</span></span><br><span class=\"line\"> <span class=\"comment\">// &#123;4=1, 2=1, 1=1, 3=1&#125;</span></span><br><span class=\"line\"> JavaPairRDD&lt;Integer, Integer&gt; integerIntegerJavaPairRDD = parallelize111.mapToPair(num -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(num, num));</span><br><span class=\"line\"> Map&lt;Integer, Long&gt; integerLongMap = integerIntegerJavaPairRDD.countByKey();</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 保存</span></span><br><span class=\"line\"> integerIntegerJavaPairRDD.saveAsTextFile(<span class=\"string\">&quot;a&quot;</span>);</span><br><span class=\"line\"> <span class=\"comment\">// 保存对象</span></span><br><span class=\"line\"> integerIntegerJavaPairRDD.saveAsObjectFile(<span class=\"string\">&quot;bb&quot;</span>);</span><br><span class=\"line\"> <span class=\"comment\">// 单线循环</span></span><br><span class=\"line\"> parallelize111.collect().forEach(System.out::println);</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\">// 分布式循环，占内存比较小，执行效率低</span></span><br><span class=\"line\"> parallelize111.foreach(</span><br><span class=\"line\">         System.out::println</span><br><span class=\"line\"> );</span><br><span class=\"line\"> <span class=\"comment\">// 执行效率高，依托于内存大小</span></span><br><span class=\"line\"> parallelize111.foreachPartition(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\"> javaSparkContext.close();</span><br></pre></td></tr></table></figure>\n<p>main 方法也叫 driver 方法，foreach 是在 executor 执行</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">action_serialize</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Student1</span> <span class=\"variable\">s</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Student1</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        parallelize111.foreach(</span><br><span class=\"line\">                num -&gt; &#123;</span><br><span class=\"line\">                    System.out.println(s.age + num);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student1</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"variable\">age</span> <span class=\"operator\">=</span> <span class=\"number\">30</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Student 想 new ，得在 executor 上拉取</p>\n<p>在 Executor 端循环遍历的时候使用到了 Driver 端对象</p>\n<p>运行过程中，就需要将 Driver 端的对象通过网络传递到 Executor 端，否则无法使用</p>\n<p>传输的对象必须要实现可序列化接口，否则无法传递</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        JavaRDD&lt;String&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"string\">&quot;haha&quot;</span>,<span class=\"string\">&quot;Haha&quot;</span>), <span class=\"number\">2</span>);</span><br><span class=\"line\">        <span class=\"type\">Search</span> <span class=\"variable\">search</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Search</span>(<span class=\"string\">&quot;H&quot;</span>);</span><br><span class=\"line\">        search.match(parallelize111);</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Search</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String query;            <span class=\"comment\">// query是成员变量，需要序列化</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Search</span><span class=\"params\">(String query)</span>&#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.query = query;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span>  <span class=\"title function_\">match</span><span class=\"params\">(JavaRDD&lt;String&gt; rdd)</span> &#123;</span><br><span class=\"line\">        rdd.filter(</span><br><span class=\"line\">                s -&gt; s.startsWith(query)</span><br><span class=\"line\">        ).collect().forEach(System.out::println);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">class Search implements Serializable &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    private String query;            // query是成员变量，需要序列化</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    public Search(String query)&#123;</span></span><br><span class=\"line\"><span class=\"comment\">        this.query = query;</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    public void  match(JavaRDD&lt;String&gt; rdd) &#123;</span></span><br><span class=\"line\"><span class=\"comment\">        String q = this.query;         // q是局部变量，在栈中，和search无关，不需要序列化</span></span><br><span class=\"line\"><span class=\"comment\">        rdd.filter(</span></span><br><span class=\"line\"><span class=\"comment\">                s -&gt; s.startsWith(q)</span></span><br><span class=\"line\"><span class=\"comment\">        ).collect().forEach(System.out::println);</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;</span></span><br></pre></td></tr></table></figure>\n<p>rdd 算子 (方法) 的逻辑代码是在 executo 执行的，其他的是在 driver 执行的</p>\n<p>collect 是行动算子，没有逻辑代码</p>\n<p>filter 中的成为逻辑代码</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//        parallelize111.foreach(</span></span><br><span class=\"line\"><span class=\"comment\">//             executor端执行</span></span><br><span class=\"line\"><span class=\"comment\">//                num -&gt; System.out.println(num)</span></span><br><span class=\"line\"><span class=\"comment\">//        );</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// jdk1.8 函数式编程采用对象模拟,使用这种方式会报错，但是系统的类无法改写继承序列化</span></span><br><span class=\"line\">        parallelize111.foreach(</span><br><span class=\"line\">        <span class=\"comment\">// 在driver端创建</span></span><br><span class=\"line\">                System.out::println</span><br><span class=\"line\">                <span class=\"comment\">// PrintStream out = System.out;</span></span><br><span class=\"line\">                <span class=\"comment\">// out::println</span></span><br><span class=\"line\">        );</span><br></pre></td></tr></table></figure>\n<h4 id=\"kryo\"><a class=\"markdownIt-Anchor\" href=\"#kryo\">#</a> kryo</h4>\n<p>Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;kryo&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">5.0</span><span class=\"number\">.3</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.atguigu.bean.User;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.function.Function;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Arrays;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Test02_Kryo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 1.创建配置对象</span></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkCore&quot;</span>)</span><br><span class=\"line\">                <span class=\"comment\">// 替换默认的序列化机制</span></span><br><span class=\"line\">                .set(<span class=\"string\">&quot;spark.serializer&quot;</span>, <span class=\"string\">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class=\"line\">                <span class=\"comment\">// 注册需要使用kryo序列化的自定义类</span></span><br><span class=\"line\">                .registerKryoClasses(<span class=\"keyword\">new</span> <span class=\"title class_\">Class</span>[]&#123;Class.forName(<span class=\"string\">&quot;com.atguigu.bean.User&quot;</span>)&#125;);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 2. 创建sparkContext</span></span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">sc</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(conf);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 3. 编写代码</span></span><br><span class=\"line\">        <span class=\"type\">User</span> <span class=\"variable\">zhangsan</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">User</span>(<span class=\"string\">&quot;zhangsan&quot;</span>, <span class=\"number\">13</span>);</span><br><span class=\"line\">        <span class=\"type\">User</span> <span class=\"variable\">lisi</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">User</span>(<span class=\"string\">&quot;lisi&quot;</span>, <span class=\"number\">13</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">        JavaRDD&lt;User&gt; userJavaRDD = sc.parallelize(Arrays.asList(zhangsan, lisi), <span class=\"number\">2</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">        JavaRDD&lt;User&gt; mapRDD = userJavaRDD.map(<span class=\"keyword\">new</span> <span class=\"title class_\">Function</span>&lt;User, User&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> User <span class=\"title function_\">call</span><span class=\"params\">(User v1)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">User</span>(v1.getName(), v1.getAge() + <span class=\"number\">1</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"> </span><br><span class=\"line\">        mapRDD. collect().forEach(System.out::println);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 4. 关闭sc</span></span><br><span class=\"line\">        sc.stop();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"依赖\"><a class=\"markdownIt-Anchor\" href=\"#依赖\">#</a> 依赖</h4>\n<p>RDD 转换算子 (方法):RDD 可以通过方法将旧的 RDD 转换成新的 RDD</p>\n<p>RDD 依赖：Spark 中相邻的 2 个 RDD 之间存在的依赖关系</p>\n<p>连续的依赖关系称为血缘关系</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223744879.png\" alt=\"image-20240804223744879\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">flatmap</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4)), 2);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//System.out.println(parallelizerdd.toDebugString());</span></span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data/text&quot;</span>);</span><br><span class=\"line\">        System.out.println(stringJavaRDD.toDebugString());</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;============&quot;</span>);</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\">        System.out.println(stringJavaRDD1.toDebugString());</span><br><span class=\"line\">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">1</span>) data/text MapPartitionsRDD[<span class=\"number\">1</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\"> |  data/text HadoopRDD[<span class=\"number\">0</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\">============</span><br><span class=\"line\">(<span class=\"number\">1</span>) MapPartitionsRDD[<span class=\"number\">2</span>] at flatMap at flatmap.java:<span class=\"number\">23</span> []</span><br><span class=\"line\"> |  data/text MapPartitionsRDD[<span class=\"number\">1</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\"> |  data/text HadoopRDD[<span class=\"number\">0</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\"> </span><br><span class=\"line\"> shuffle +-  代表两段流程</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data/text&quot;</span>);</span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(stringJavaRDD.toDebugString());</span></span><br><span class=\"line\">        System.out.println(stringJavaRDD.rdd().dependencies());</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;============&quot;</span>);</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(stringJavaRDD1.toDebugString());</span></span><br><span class=\"line\">        System.out.println(stringJavaRDD1.rdd().dependencies());</span><br><span class=\"line\"></span><br><span class=\"line\">        stringJavaRDD1.groupBy(num -&gt; num);</span><br><span class=\"line\">        System.out.println(stringJavaRDD1.rdd().dependencies());</span><br><span class=\"line\">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">List(org.apache.spark.OneToOneDependency@4b5a078a)</span><br><span class=\"line\">============</span><br><span class=\"line\">List(org.apache.spark.OneToOneDependency@33f2df51)</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">25</span> <span class=\"number\">17</span>:<span class=\"number\">16</span>:<span class=\"number\">52</span> INFO FileInputFormat: Total input files to process : <span class=\"number\">1</span></span><br><span class=\"line\">List(org.apache.spark.OneToOneDependency@33f2df51)</span><br></pre></td></tr></table></figure>\n<p>onetoonedep 窄依赖</p>\n<p>shuffledep  宽依赖</p>\n<p>rdd 的依赖关系是 rdd 对象中分区数据的关系</p>\n<p>窄依赖：如果上游 rdd 一个分区的数据被下游 rdd 的一个分区独享</p>\n<p>宽依赖：如果上游 rdd 一个分区的数据被下游 rdd 的多个分区共享。会将分区数据打乱重新组合，所以此层存在 shuffle 操作</p>\n<p>依赖关系和任务数量，阶段数量</p>\n<p>作业 (Job): 行动算子执行时，会触发作业的执行 (ActiveJob)</p>\n<p>阶段 (Stage): 一个 job 中 RDD 的计算流程，默认就一个完整的阶段，但是如果计算流程中存在 shuffle, 那么流程就会一分为二。分开的每一段就称之为 stage，前一个阶段不执行完，后一个阶段不允许执行</p>\n<p>任务 (Task): 每个 Executor 执行的计算单元</p>\n<p>阶段的数量和 shuffle 依赖的数量有关系：1+shuffle 依赖的数量</p>\n<p>任务（分区）的数量就是每个阶段分区的数量之和，一般推荐分区数量为资源核数的 2-3 倍</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223805444.png\" alt=\"image-20240804223805444\"></p>\n<p>任务 (Task): 每个 Executor 执行的计算单元</p>\n<p>任务的数量其实就是每个阶段最后一个 RDD 分区的数量之和</p>\n<p>移动数据不如移动计算</p>\n<h4 id=\"持久化\"><a class=\"markdownIt-Anchor\" href=\"#持久化\">#</a> 持久化</h4>\n<p>持久化：将对象长时间的保存</p>\n<p>序列化：内存中对象 =&gt;byte 序列 (byte 数组)</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223856875.png\" alt=\"image-20240804223856875\"></p>\n<p>maprdd.cache();</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223912457.png\" alt=\"image-20240804223912457\"></p>\n<p>maprdd.persist(Storage.MEMORY_ONLY());</p>\n<p>cache 方法底层调用 persist.  maprdd.persist (Storage.MEMORY_ONLY ());  ===  cache ()</p>\n<p>// 落盘持久化</p>\n<p>maprdd.persist(Storage.DISK_ONLY());</p>\n<p>MEMORY_ONLY 超出数据直接丢弃</p>\n<p>MEMORY_AND_DISK.  内存满了放磁盘</p>\n<p>MEMORY_ONLY_SER 序列化后再存内存</p>\n<p>MEMORY_ONLY_SER_2. 副本 2 份</p>\n<h5 id=\"checkpoint\"><a class=\"markdownIt-Anchor\" href=\"#checkpoint\">#</a> checkpoint</h5>\n<p>将计算结果保存到 hdfs 或者本地文件路径，实现不同进程共享</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">javaSparkContext.setCheckpointDir(<span class=\"string\">&quot;cp&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">mapRDD.cache();</span><br><span class=\"line\">mapRDD.checkPoint();</span><br></pre></td></tr></table></figure>\n<p>检查点目的是 rdd 结果长时间保存，所以需要保证数据安全，会从头再跑一遍。把第二遍结果放到里面</p>\n<p>性能比较低，可以在检查点之前执行 cache，将数据缓存</p>\n<p>cache 方法会在血缘关系中增加依赖关系</p>\n<p>checkpoint 方法改变血缘关系</p>\n<p>每个 shuffle 都自动带有缓存，为了提高 shuffle 算子的性能</p>\n<p>如果重复调用相同规则的 shuffle 算子，第二个 shfulle 算子不会有相同 shuffle 操作</p>\n<p>使用完缓存，可以使用 unpersist 释放缓存</p>\n<h4 id=\"分区器\"><a class=\"markdownIt-Anchor\" href=\"#分区器\">#</a> 分区器</h4>\n<p>数据分区的规则</p>\n<p>计算后数据所在的分区是通过 Spark 的内部计算 (分区) 完成我。尽可能均衡一些（hash）</p>\n<p>reducebykey 需要传递两个参数，第一个参数是数据分区的规则，第二个参数是数据聚合逻辑</p>\n<p>第一个参数可以不用传递，使用时会使用默认分区规则。默认分区规则中使用 HashPartitioner</p>\n<p>hashpartitioner 中有一个方法叫 getpartition，需要传递一个参数 key，返回一个值，表示分区编号，从 0 开始</p>\n<p>得到一个分区编号，key.hashcode % partnum (hash 取余)</p>\n<p>只有 kv 类型的有分区器</p>\n<p>rdd.partitioner()</p>\n<p>Spark 目前支持 Hash 分区、Range 分区和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区和 Reduce 的个数。</p>\n<p>自定义分区器</p>\n<p>1. 创建自定义类</p>\n<p>2. 继承抽象类 Partitioner</p>\n<p>3. 重写方法 (2)</p>\n<p>4. 构建对象，在算子中使用</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Part</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Partitioner</span> &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 指定分区数量</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">numPartitions</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">3</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 很久数据的key来获取数据存储的分区编号</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getPartition</span><span class=\"params\">(Object key)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;a&quot;</span>.equals(key))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (<span class=\"string\">&quot;b&quot;</span>.equals(key)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">mapRDD.reduceByKey(<span class=\"keyword\">new</span> <span class=\"title class_\">Part</span>(),Integer::sum)</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Part</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Partitioner</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> numPart;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Part</span><span class=\"params\">(<span class=\"type\">int</span> num)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.numPart = num;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 指定分区数量</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">numPartitions</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">3</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 很久数据的key来获取数据存储的分区编号</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getPartition</span><span class=\"params\">(Object key)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;a&quot;</span>.equals(key))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (<span class=\"string\">&quot;b&quot;</span>.equals(key)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">hashCode</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> numPart;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">equals</span><span class=\"params\">(Object obj)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (obj <span class=\"keyword\">instanceof</span> Part) &#123;</span><br><span class=\"line\">            <span class=\"type\">Part</span> <span class=\"variable\">other</span> <span class=\"operator\">=</span> (Part)obj;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"built_in\">this</span>.numPart == other.numPart;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RDD 在 foreach 循环时，逻辑代码和操作全部都是在 Executor 端完成的，那么结果不会拉取回到 Driver 端</p>\n<p>RDD 无法实现数据拉取操作</p>\n<p>如果 Executor 端使用了 Driver 端数据，那么需要从 Driver 端将数据拉取到 Executor 端</p>\n<p>数据拉取的单位是 Task (任务)</p>\n<p>默认数据传输以 Task 为单位进行传输，如果想要以 Executor 为单位传输，那么需要进行包装 (封装)</p>\n<p>Spark 需要采用特殊的数据模型实现数据传输：广播变量</p>\n<p>jsc.broadcast(list);</p>\n<p>rdd.filter(s -&gt; broadcast.value())</p>\n<h4 id=\"sparksql\"><a class=\"markdownIt-Anchor\" href=\"#sparksql\">#</a> sparksql</h4>\n<p>Spark SQL: 结构化数据处理模块</p>\n<p>SQL: 为了数据库数据访问开发的语言</p>\n<p>Spark 封装模块的目的就是在结构化数据的场合，处理起来方便</p>\n<p>结构化数据：特殊结构的数据 =&gt;(table,json)</p>\n<p>半结构化数据:xml,html</p>\n<p>非结构化数据：压缩文件，图形文件，视频，音频文件</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spark-sql_2<span class=\"number\">.12</span>&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">3.3</span><span class=\"number\">.1</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">envv</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//        SparkConf sparkConf = new SparkConf();</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkConf.setMaster(&quot;local&quot;);</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkConf.setAppName(&quot;SparkSQL&quot;);</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        SparkContext sc = new SparkContext(sparkConf);</span></span><br><span class=\"line\"><span class=\"comment\">//        SparkSession sparkSession = new SparkSession(sc);</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkSession.close();</span></span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用sql文的方式操作</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select avg(age) from user&quot;</span>;</span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\">        rowDataset1.show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"环境之间转换\"><a class=\"markdownIt-Anchor\" href=\"#环境之间转换\">#</a> 环境之间转换</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// core:sparkcontext -&gt; sql:sparksession</span></span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"title class_\">sparkSession</span>( <span class=\"keyword\">new</span> <span class=\"title class_\">SparkContect</span>(conf));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// sql -&gt; core:sparkcontext</span></span><br><span class=\"line\">sparksession.sparkContext().parallelize();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//sql -&gt; core:javasparkcontext</span></span><br><span class=\"line\">        <span class=\"type\">SparkContext</span> <span class=\"variable\">sparkContext</span> <span class=\"operator\">=</span> sparkSession.sparkContext();</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkContext);</span><br><span class=\"line\">        javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>));</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//rowDataset.foreach(</span></span><br><span class=\"line\"><span class=\"comment\">//        row -&gt;&#123;</span></span><br><span class=\"line\"> <span class=\"comment\">//           System.out.println(row.getInt(2));</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 数据模型中的数据类型进行转换，将row转换成其他对象处理</span></span><br><span class=\"line\">        Dataset&lt;User&gt; userDataset = rowDataset.as(Encoders.bean(User.class));</span><br><span class=\"line\">        </span><br><span class=\"line\">        userDataset.foreach(</span><br><span class=\"line\">                user -&gt; &#123;</span><br><span class=\"line\">                    System.out.println(user.getname());</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">User</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> id;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> age;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String name;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    User(<span class=\"type\">int</span> id, <span class=\"type\">int</span> age, String name) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.id = id;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.age = age;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.name = name;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">model</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;select * from user&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">model2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.select(<span class=\"string\">&quot;*&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"自定义udf\"><a class=\"markdownIt-Anchor\" href=\"#自定义udf\">#</a> 自定义 UDF</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_3</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用sql文的方式操作</span></span><br><span class=\"line\">        <span class=\"comment\">// SparkSQL提供了一种特殊的方式,可以在SQL中增加自定义方法来实现复杂的逻辑</span></span><br><span class=\"line\">        <span class=\"comment\">//如果想要自定义的方法能够在SQL中使用,那么必须在SPark中进行声明和注册</span></span><br><span class=\"line\">        <span class=\"comment\">// register方法需要传递3个参数</span></span><br><span class=\"line\">        <span class=\"comment\">//第一个参数表示SQL中使用的方法名</span></span><br><span class=\"line\">        <span class=\"comment\">//第二个参数表示逻辑:IN=&gt;OUT</span></span><br><span class=\"line\">        <span class=\"comment\">//第三个参数表示返回的数据类型  ,DataType类型数据，需要使用scala语法操作</span></span><br><span class=\"line\">        sparkSession.udf().register(<span class=\"string\">&quot;prefixName&quot;</span>, <span class=\"keyword\">new</span> <span class=\"title class_\">UDF1</span>&lt;String, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Name:&quot;</span> + s;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;, StringType$.MODULE$);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select prefixName(name) from user&quot;</span>;</span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\">        rowDataset1.show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.apache.spark.sql.types.DataTypes.StringType;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_udf</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用sql文的方式操作</span></span><br><span class=\"line\">        <span class=\"comment\">// SparkSQL提供了一种特殊的方式,可以在SQL中增加自定义方法来实现复杂的逻辑</span></span><br><span class=\"line\">        <span class=\"comment\">//如果想要自定义的方法能够在SQL中使用,那么必须在SPark中进行声明和注册</span></span><br><span class=\"line\">        <span class=\"comment\">// register方法需要传递3个参数</span></span><br><span class=\"line\">        <span class=\"comment\">//第一个参数表示SQL中使用的方法名</span></span><br><span class=\"line\">        <span class=\"comment\">//第二个参数表示逻辑:IN=&gt;OUT</span></span><br><span class=\"line\">        <span class=\"comment\">//第三个参数表示返回的数据类型  ,DataType类型数据，需要使用scala语法操作</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkSession.udf().register(&quot;prefixName&quot;, new UDF1&lt;String, String&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//            @Override</span></span><br><span class=\"line\"><span class=\"comment\">//            public String call(String s) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                return &quot;Name:&quot; + s;</span></span><br><span class=\"line\"><span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;, DataTypes.StringType);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.udf().register(<span class=\"string\">&quot;prefixName&quot;</span>, <span class=\"keyword\">new</span> <span class=\"title class_\">UDF1</span>&lt;String, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Name:&quot;</span> + s;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;, StringType);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select prefixName(name) from user&quot;</span>;</span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\">        rowDataset1.show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>UDF 函数是每一行数据会都用一次函数</p>\n<p>UDAF 函数是所有的数据产生一个结果数据</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223942980.png\" alt=\"image-20240804223942980\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224006080.png\" alt=\"image-20240804224006080\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.sparksql;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">AvgAgeBuffer</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">long</span> total;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">long</span> count;</span><br><span class=\"line\"></span><br><span class=\"line\">    AvgAgeBuffer() &#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    AvgAgeBuffer(<span class=\"type\">long</span> total, <span class=\"type\">long</span> count) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.total = total;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.count = count;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">getCount</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> count;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setCount</span><span class=\"params\">(<span class=\"type\">long</span> count)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.count = count;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">getTotal</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> total;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setTotal</span><span class=\"params\">(<span class=\"type\">long</span> total)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.total = total;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.example.sparksql;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.Encoder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.Encoders;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 自定义UDAF函数,实现年龄的平均值</span></span><br><span class=\"line\"><span class=\"comment\">//1.创建自定义的【公共】类</span></span><br><span class=\"line\"><span class=\"comment\">//2.继承 org.apache.spark.sql.expressions.Aggreegator</span></span><br><span class=\"line\"><span class=\"comment\">//3.设定泛型</span></span><br><span class=\"line\"><span class=\"comment\">//IN:输入数据类型</span></span><br><span class=\"line\"><span class=\"comment\">//BUFF:缓冲区的数据类型</span></span><br><span class=\"line\"><span class=\"comment\">//OUT:输出数据类型</span></span><br><span class=\"line\"><span class=\"comment\">//4.重写方法(4(计算)+2(状态))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyAvgAgeUDAF</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Aggregator</span>&lt;Long, AvgAgeBuffer, Long&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 缓冲区初始化操作</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> AvgAgeBuffer <span class=\"title function_\">zero</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">AvgAgeBuffer</span>(<span class=\"number\">0L</span>, <span class=\"number\">0L</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 把输入年龄和缓冲器数据聚合</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> AvgAgeBuffer <span class=\"title function_\">reduce</span><span class=\"params\">(AvgAgeBuffer buffer, Long age)</span> &#123;</span><br><span class=\"line\">        buffer.setTotal(buffer.getTotal() + age);</span><br><span class=\"line\">        buffer.setCount(buffer.getCount()+<span class=\"number\">1</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> buffer;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 合并缓冲区数据</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> AvgAgeBuffer <span class=\"title function_\">merge</span><span class=\"params\">(AvgAgeBuffer b1, AvgAgeBuffer b2)</span> &#123;</span><br><span class=\"line\">        b1.setTotal(b1.getTotal()+b2.getTotal());</span><br><span class=\"line\">        b1.setCount(b1.getCount()+b2.getCount());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> b1;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 计算最终结果</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Long <span class=\"title function_\">finish</span><span class=\"params\">(AvgAgeBuffer reduction)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> reduction.getTotal() / reduction.getCount();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Encoder&lt;AvgAgeBuffer&gt; <span class=\"title function_\">bufferEncoder</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Encoders.bean(AvgAgeBuffer.class);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Encoder&lt;Long&gt; <span class=\"title function_\">outputEncoder</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Encoders.LONG();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.example.sparksql;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_udaf</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//SparkSQL采用特殊的方式将UDAF转换成UDF使用</span></span><br><span class=\"line\"><span class=\"comment\">//UDAF使用时需要创建自定义聚合对象</span></span><br><span class=\"line\">        <span class=\"comment\">// 两个恶参数，第一个UADF对象，第二个表示UADF对象</span></span><br><span class=\"line\">        sparkSession.udf().register(<span class=\"string\">&quot;avgage&quot;</span>, functions.udaf(<span class=\"keyword\">new</span> <span class=\"title class_\">MyAvgAgeUDAF</span>(),Encoders.LONG()));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select avgage(age) from user&quot;</span>;</span><br><span class=\"line\">        sparkSession.sql(sql).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\"><span class=\"comment\">//        rowDataset1.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"数据加载与保存\"><a class=\"markdownIt-Anchor\" href=\"#数据加载与保存\">#</a> 数据加载与保存</h5>\n<p>SparkSQL 读取和保存的文件一般为三种，JSON 文件、CSV 文文件和列式存储的文件，同时可以通过添加参数，来识别不同的存储和压缩格式。</p>\n<p>csv</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// csv 文件讲数据采用逗号分隔,可以被excel打开</span></span><br><span class=\"line\">        <span class=\"comment\">// csv 带有header，以,分隔.   可以以, _ \\t 分隔      \\t时叫做tsv</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; csv = sparkSession.read().option(<span class=\"string\">&quot;header&quot;</span>,<span class=\"string\">&quot;true&quot;</span>).option(<span class=\"string\">&quot;sep&quot;</span>,<span class=\"string\">&quot;,&quot;</span>).csv(<span class=\"string\">&quot;data/user.csv&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// +----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//| _c0|     _c1|_c2|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//|1001|zhangsan| 30|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1002|    lisi| 31|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1003|  wangwu| 32|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//|  id|    name|age|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//|1001|zhangsan| 30|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1002|    lisi| 31|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1003|  wangwu| 32|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//csv.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 输出目录已经存在，默认会发生异常。不希望出错，可以修改配置 保存模式</span></span><br><span class=\"line\">        csv.write().mode(<span class=\"string\">&quot;append&quot;</span>).option(<span class=\"string\">&quot;header&quot;</span>,<span class=\"string\">&quot;true&quot;</span>).csv(<span class=\"string\">&quot;output&quot;</span>); </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//  DataFrameWriter var2;</span></span><br><span class=\"line\">        <span class=\"comment\">//        if (&quot;overwrite&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            var2 = this.mode(SaveMode.Overwrite);</span></span><br><span class=\"line\">        <span class=\"comment\">//        &#125; else if (&quot;append&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            var2 = this.mode(SaveMode.Append);</span></span><br><span class=\"line\">        <span class=\"comment\">//        &#125; else if (&quot;ignore&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            var2 = this.mode(SaveMode.Ignore);</span></span><br><span class=\"line\">        <span class=\"comment\">//        &#125; else &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            boolean var3;</span></span><br><span class=\"line\">        <span class=\"comment\">//            if (&quot;error&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = true;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125; else if (&quot;errorifexists&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = true;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125; else if (&quot;default&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = true;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125; else &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = false;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>json</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source_json</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// js object natation</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果是对象，用&#123;&#125;  数组用[]  json文件符合json格式</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL其实就是对Spark Core RDD的封装。RDD读取文件采用用的是Hadoop,hadoop是按行读取。</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可,无需整个文件符合JSON格式</span></span><br><span class=\"line\">        <span class=\"comment\">// 底层是dataset，可以读csv，写json</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.write().json(<span class=\"string\">&quot;output&quot;</span>);</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>行式存储</p>\n<p>存在主键，可以快速定位。查询快，统计慢</p>\n<p>列式存储</p>\n<p>查询快，统计快</p>\n<p>mysql 行存储，hive 列存储</p>\n<p>spark 列存储 parquet</p>\n<p>列存储可以被压缩，snappy</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ublic <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source_parquet</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// js object natation</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果是对象，用&#123;&#125;  数组用[]  json文件符合json格式</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL其实就是对Spark Core RDD的封装。RDD读取文件采用用的是Hadoop,hadoop是按行读取。</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可,无需整个文件符合JSON格式</span></span><br><span class=\"line\">        <span class=\"comment\">// 底层是dataset，可以读csv，写json</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().parquet(<span class=\"string\">&quot;data/user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.write().json(<span class=\"string\">&quot;output&quot;</span>);</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"mysql交互\"><a class=\"markdownIt-Anchor\" href=\"#mysql交互\">#</a> mysql 交互</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">8.0</span><span class=\"number\">.18</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source_mysql</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Properties</span> <span class=\"variable\">properties</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Properties</span>();</span><br><span class=\"line\">        properties.setProperty(<span class=\"string\">&quot;user&quot;</span>,<span class=\"string\">&quot;admin&quot;</span>);</span><br><span class=\"line\">        properties.setProperty(<span class=\"string\">&quot;password&quot;</span>,<span class=\"string\">&quot;Chaitin@123&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        Dataset&lt;Row&gt; jdbc = sparkSession.read().jdbc(<span class=\"string\">&quot;jdbc:mysql://hadoop100:3306/metastore?useSSL=false&quot;</span>,<span class=\"string\">&quot;TYPES&quot;</span>,properties);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        jdbc.write().mode(<span class=\"string\">&quot;append&quot;</span>).jdbc(<span class=\"string\">&quot;jdbc:mysql://hadoop100:3306/metastore?useSSL=false&quot;</span>,<span class=\"string\">&quot;TYPES_123&quot;</span>,properties);</span><br><span class=\"line\">        jdbc.show();</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"hive交互\"><a class=\"markdownIt-Anchor\" href=\"#hive交互\">#</a> hive 交互</h5>\n<p>复制 hive-site.yaml 到 resources 中</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spark-hive_2<span class=\"number\">.12</span>&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">3.3</span><span class=\"number\">.1</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_hive</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 编码前设定hadoop访问用户</span></span><br><span class=\"line\">        System.setProperty(<span class=\"string\">&quot;HADOOP_USER_NAME&quot;</span>,<span class=\"string\">&quot;root&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession</span><br><span class=\"line\">                .builder()</span><br><span class=\"line\">                .enableHiveSupport()</span><br><span class=\"line\">                .master(<span class=\"string\">&quot;local[*]&quot;</span>)</span><br><span class=\"line\">                .appName(<span class=\"string\">&quot;sparkSQL&quot;</span>)</span><br><span class=\"line\">                .getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;show tables&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;create table user_info(name string, age int)&quot;</span>);</span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;insert into  table user_info values (&#x27;haha&#x27;,100)&quot;</span>);</span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;select * from user_info&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果 reources 中有 hive-site.xml 但是 target/classes 中没有，需要手工拷贝到改目录</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224020783.png\" alt=\"image-20240804224020783\"></p>\n<h4 id=\"spark-streaming\"><a class=\"markdownIt-Anchor\" href=\"#spark-streaming\">#</a> spark streaming</h4>\n<p>有界数据流</p>\n<p>无界数据流</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224044892.png\" alt=\"image-20240804224044892\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224058079.png\" alt=\"image-20240804224058079\"></p>\n<p>spark streaming 底层还是 spark core，在流式数据处理中进行的封装</p>\n<p>从数据处理方式的角度</p>\n<p>流式数据处理：一个数据一个数据的处理</p>\n<p>微批量数据处理：一小批数据处理</p>\n<p>批量数据处理：一批数据一批数据的处理</p>\n<p>从数据处理延迟的角度</p>\n<p>实时数据处理：数据处理的延迟以毫秒为单位</p>\n<p>准实时处理：以秒和分钟为单位</p>\n<p>离线数据处理：数据处理的延迟以小时，天为单位</p>\n<p>Spark 是批量，离线数据处理框架</p>\n<p>spark streaming 是个 微批量 准实时数据处理框架</p>\n<p>streaming 按照时间来定义一小批</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224112151.png\" alt=\"image-20240804224112151\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">env</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 数据采集器是长期执行的任务，不能停止，也不能释放资源</span></span><br><span class=\"line\">        <span class=\"comment\">// javaStreamingContext.close();</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>socket</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m http.server <span class=\"number\">8001</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">socket</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 通过环境对象对接socket数据源</span></span><br><span class=\"line\">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">9999</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        socketTextStream.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kafka</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.function.Function;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.Duration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.api.java.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.*;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kafka</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建配置参数</span></span><br><span class=\"line\">        HashMap&lt;String, Object&gt; map = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class=\"string\">&quot;hadoop102:9092,hadoop103:9092,hadoop104:9092&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class=\"string\">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class=\"string\">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.GROUP_ID_CONFIG,<span class=\"string\">&quot;atguigu&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class=\"string\">&quot;latest&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 需要消费的主题</span></span><br><span class=\"line\">        ArrayList&lt;String&gt; strings = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;&gt;();</span><br><span class=\"line\">        strings.add(<span class=\"string\">&quot;topic_db&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));</span><br><span class=\"line\"></span><br><span class=\"line\">        directStream.map(<span class=\"keyword\">new</span> <span class=\"title class_\">Function</span>&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">call</span><span class=\"params\">(ConsumerRecord&lt;String, String&gt; v1)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> v1.value();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).print(<span class=\"number\">100</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Dstream</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">function</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 通过环境对象对接socket数据源</span></span><br><span class=\"line\">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">9999</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        socketTextStream.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaDStream&lt;String&gt; stringJavaDStream = socketTextStream.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaPairDStream&lt;String, Integer&gt; stringIntegerJavaPairDStream = stringJavaDStream.mapToPair(</span><br><span class=\"line\">                word -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(word, <span class=\"number\">1</span>)</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaPairDStream&lt;String, Integer&gt; stringIntegerJavaPairDStream1 = stringIntegerJavaPairDStream.reduceByKey(</span><br><span class=\"line\">                Integer::sum</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        stringIntegerJavaPairDStream1.foreachRDD(</span><br><span class=\"line\">                rdd -&gt; &#123;</span><br><span class=\"line\">                    rdd.sortByKey().collect().forEach(System.out::println);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>window</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">窗口是可以移动的，成为移动窗口，但是窗口移动是有复幅度的，默认移动幅度就是采集周期</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">窗口:其实就是数据的范围(时间)</span><br><span class=\"line\">window方法可以改变窗口的数据范围(默认数据范围为采集周期</span><br><span class=\"line\">window方法可以传递<span class=\"number\">2</span>个参数</span><br><span class=\"line\">第一个参数表示窗口的数据范围(时间)</span><br><span class=\"line\">第二个参数表示窗口的移动幅度(时间),可以不用传递,默认使用的就是采集周期</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">数据窗口范围和窗口移动幅度一致(3s),数据不会有重复</span><br><span class=\"line\"></span><br><span class=\"line\">数据窗口范围比窗口移动幅度大,数据可能会有重复</span><br><span class=\"line\"></span><br><span class=\"line\">数据窗口范围比窗口移动幅度小,数据可能会有遗漏</span><br></pre></td></tr></table></figure>\n<p>close</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">close</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 通过环境对象对接socket数据源</span></span><br><span class=\"line\">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">9999</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        socketTextStream.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 不能在main中关闭</span></span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">Runnable</span>() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 关闭SparkStreaming的时候,需要在程序运行的过程中,通过外部操作进行关闭</span></span><br><span class=\"line\">                    Thread.sleep(<span class=\"number\">000</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(e);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">///javaStreamingContext.close();           // 强制关闭</span></span><br><span class=\"line\">                javaStreamingContext.stop(<span class=\"literal\">true</span>,<span class=\"literal\">true</span>);       <span class=\"comment\">//graceful stop</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">Runnable</span>() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 关闭SparkStreaming的时候,需要在程序运行的过程中,通过外部操作进行关闭</span></span><br><span class=\"line\">                    Thread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">                    <span class=\"comment\">// 使用zk，redis，mysql，hdfs实现中转</span></span><br><span class=\"line\">                    <span class=\"type\">boolean</span> <span class=\"variable\">flag</span> <span class=\"operator\">=</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(e);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">///javaStreamingContext.close();           // 强制关闭</span></span><br><span class=\"line\">                javaStreamingContext.stop(<span class=\"literal\">true</span>,<span class=\"literal\">true</span>);       <span class=\"comment\">//graceful stop</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"type\">FileSystem</span> <span class=\"variable\">fs</span> <span class=\"operator\">=</span> FileSystem.get(<span class=\"keyword\">new</span> <span class=\"title class_\">URI</span>(<span class=\"string\">&quot;hdfs://hadoop102:8020&quot;</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>(), <span class=\"string\">&quot;atguigu&quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)&#123;</span><br><span class=\"line\">                    Thread.sleep(<span class=\"number\">5000</span>);</span><br><span class=\"line\">                    <span class=\"type\">boolean</span> <span class=\"variable\">exists</span> <span class=\"operator\">=</span> fs.exists(<span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(<span class=\"string\">&quot;hdfs://hadoop102:8020/stopSpark&quot;</span>));</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (exists)&#123;</span><br><span class=\"line\">                        <span class=\"type\">StreamingContextState</span> <span class=\"variable\">state</span> <span class=\"operator\">=</span> javaStreamingContext.getState();</span><br><span class=\"line\">                        <span class=\"comment\">// 获取当前任务是否正在运行</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (state == StreamingContextState.ACTIVE)&#123;</span><br><span class=\"line\">                            <span class=\"comment\">// 优雅关闭</span></span><br><span class=\"line\">                            javaStreamingContext.stop(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">                            System.exit(<span class=\"number\">0</span>);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">catch</span> (Exception e)&#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "大数据"
            ]
        },
        {
            "id": "http://example.com/2024/03/01/hadoop_new/",
            "url": "http://example.com/2024/03/01/hadoop_new/",
            "title": "Hadoop",
            "date_published": "2024-03-01T05:38:45.000Z",
            "content_html": "<h2 id=\"hadoop\"><a class=\"markdownIt-Anchor\" href=\"#hadoop\">#</a> hadoop</h2>\n<p>Hadoop 是 Apache 软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构。</p>\n<ul>\n<li>Hadoop 是基于 Java 语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中；</li>\n<li>Hadoop 的核心是<strong>分布式文件系统 HDFS（Hadoop Distributed File System）和 MapReduce；</strong></li>\n<li>Hadoop 被公认为行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力；</li>\n</ul>\n<p>解决海量数据存储和分析计算问题</p>\n<p>优势： 维护多副本</p>\n<p>在集群间分配任务数据，方便扩展</p>\n<p>并行工作</p>\n<p>自动将失败任务重新分配</p>\n<h4 id=\"hadoop-版本演进\"><a class=\"markdownIt-Anchor\" href=\"#hadoop-版本演进\">#</a> hadoop 版本演进</h4>\n<p>Apache Hadoop 版本分为两代：第一代 Hadoop 称为 Hadoop 1.0，第二代 Hadoop 称为 Hadoop 2.0。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221529201.png\" alt=\"image-20240804221529201\"></p>\n<p><strong>第一代 Hadoop 包含三个大版本，分别是 0.20.x、0.21.x、0.22.x</strong>。</p>\n<ul>\n<li>0.20.x 最后演化成 1.0.x，变成了稳定版。</li>\n<li>0.21.x 和 0.22.x 则增加了 NameNode HA 等新的重大特性。</li>\n</ul>\n<p><strong>第二代 Hadoop 包含两个大版本，分别是 0.23.x、2.x</strong>。</p>\n<ul>\n<li>它们完全不同于 Hadoop 1.0，是一套全新的架构，均包含 HDFS Federation 和 YARN 两个系统。</li>\n<li>相比于 0.23.x，2.x 增加了 NameNode HA 和 Wire-compatibility 两个重大特性。</li>\n</ul>\n<h4 id=\"组件\"><a class=\"markdownIt-Anchor\" href=\"#组件\">#</a> 组件</h4>\n<p>HDFS</p>\n<p>NameNode：</p>\n<ul>\n<li>NameNode 是 HDFS 的主节点，负责管理文件系统的命名空间和元数据信息。</li>\n<li>它维护了整个文件系统的目录树结构以及文件和数据块的映射关系。</li>\n<li>NameNode 还负责处理客户端的读写请求，包括打开、关闭、重命名和删除文件等操作。</li>\n</ul>\n<p>DataNode：</p>\n<ul>\n<li>DataNode 是 HDFS 的数据节点，负责存储实际的数据块。</li>\n<li>它接收来自客户端或其他 DataNode 的数据写入请求，并将数据块存储在本地磁盘上。</li>\n<li>DataNode 还负责处理客户端的数据读取请求，将数据块传输给客户端。</li>\n</ul>\n<p>Standby Namenode(2NN)：</p>\n<ul>\n<li>辅助 namenode。作为备用的 NameNode。当活动的 NameNode 失效时，Standby NameNode 可以接管其工作，从而提高了系统的可用性。</li>\n</ul>\n<p>1) NameNode (nn): 存储文件的元数据，如文件名，文件目录结构，文件属性 (生成时间、副本数、文件权限), 以及每个文件的块列表和块所在 DataNode 等。</p>\n<p>2) DataNode (dn): 在本地文件系统存储文件块数据，以及块数据的校验和。</p>\n<p>3）Secondary NameNode (2nn): 每隔一段时间对 NameNode 元数据备份</p>\n<p><strong>YARN 组件</strong></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221615769.png\" alt=\"\"></p>\n<p>1) ResourceManager (RM): 整个集群资源 (内存、CPU 等) 的老大</p>\n<p>2) NodeManager (NM): 单个节点服务器资源老大</p>\n<p>3) ApplicationMaster (AM): 单个任务运行的老大</p>\n<p>4) Container: 容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。</p>\n<p>说明 1: 客户端可以有多个</p>\n<p>说明 2: 集群上可以运行多个 ApplicationMaster</p>\n<p>说明 3: 每个 NodeManager 上可以有多个 Container</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221919615.png\" alt=\"image-20240804221919615\"></p>\n<h4 id=\"部署hadoop\"><a class=\"markdownIt-Anchor\" href=\"#部署hadoop\">#</a> 部署 hadoop</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">adduser hadoop</span><br><span class=\"line\"></span><br><span class=\"line\">vim /etc/sudoers</span><br><span class=\"line\"><span class=\"comment\"># Allow members of group sudo to execute any command</span></span><br><span class=\"line\">%sudo   ALL=(ALL:ALL) ALL</span><br><span class=\"line\">hadoop  ALL=(ALL:ALL) NOPASSWD:ALL</span><br><span class=\"line\"></span><br><span class=\"line\">hostnamectl</span><br><span class=\"line\"></span><br><span class=\"line\">vim /etc/hosts</span><br><span class=\"line\"></span><br><span class=\"line\">apt update</span><br><span class=\"line\"></span><br><span class=\"line\">apt install vim net-tools lrzsz bash-com* -y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">apt install openjdk-8-jre-headless</span><br><span class=\"line\"></span><br><span class=\"line\">wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</span><br><span class=\"line\">tar xzvf hadoop-3.3.6.tar.gz</span><br><span class=\"line\"></span><br><span class=\"line\">vim /etc/profile</span><br><span class=\"line\"><span class=\"built_in\">export</span> HADOOP_HOME=/root/hadoop-3.3.6</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$HADOOP_HOME</span>/bin</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$HADOOP_HOME</span>/sbin</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$JAVA_HOME</span>/bin</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">source</span> /etc/profile</span><br></pre></td></tr></table></figure>\n<p>部署模式：</p>\n<p>local: 数据存储在本地</p>\n<p>pseudo-distributed：数据存储在 hdfs</p>\n<p>fully-distributed：数据存储在 hdfs，多台服务器工作</p>\n<p>本地运行 example wordcount</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> input</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;aa bb cc cc dd dd dd ee ee&quot;</span> &gt; input/111.txt</span><br><span class=\"line\"></span><br><span class=\"line\">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount input/ output/</span><br></pre></td></tr></table></figure>\n<h4 id=\"完全分布式部署\"><a class=\"markdownIt-Anchor\" href=\"#完全分布式部署\">#</a> 完全分布式部署：</h4>\n<p>NameNode 和 SecondaryNameNode 不要安装在同一台服务器</p>\n<p>ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台机器上</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221900758.png\" alt=\"image-20240804221900758\"></p>\n<p>Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认。配置值时，才需要修改自定义配置文件，更改相应属性值。</p>\n<p>默认配置文件</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221943124.png\" alt=\"image-20240804221943124\"></p>\n<p>自定义配置文件</p>\n<p>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个配置文件存放在</p>\n<p>SHADOOP_HOME/etc/kadop, 这个路径上，用户可以根据项目需求重新进行修改配置。</p>\n<p>在集群上所有节点配置</p>\n<p>core-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=<span class=\"string\">&quot;text/xsl&quot;</span> href=<span class=\"string\">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>fs.defaultFS<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hdfs://hadoop100:8020<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The name of the default file system.  A URI whose</span></span><br><span class=\"line\"><span class=\"language-xml\">      scheme and authority determine the FileSystem implementation.  The</span></span><br><span class=\"line\"><span class=\"language-xml\">      uri&#x27;s scheme determines the config property (fs.SCHEME.impl) naming</span></span><br><span class=\"line\"><span class=\"language-xml\">      the FileSystem implementation class.  The uri&#x27;s authority is used to</span></span><br><span class=\"line\"><span class=\"language-xml\">      determine the host, port, etc. for a filesystem.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop-$&#123;user.name&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>A base for other temporary directories.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"comment\">&lt;!-- Static Web User Filter properties. --&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.http.staticuser.user<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>root<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        The user name to filter as, on static web filters</span></span><br><span class=\"line\"><span class=\"language-xml\">        while rendering content. An example use is the HDFS</span></span><br><span class=\"line\"><span class=\"language-xml\">        web UI (user to be used for browsing files).</span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>vim hdfs-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=<span class=\"string\">&quot;text/xsl&quot;</span> href=<span class=\"string\">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.http-address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hadoop100:9870<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        The address and the base port where the dfs namenode web ui will listen on.</span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hadoop102:9868<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        The secondary namenode http server address and port.</span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span> </span><br></pre></td></tr></table></figure>\n<p>vim yarn-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>A comma separated list of services where service name should only</span></span><br><span class=\"line\"><span class=\"language-xml\">      contain a-zA-Z0-9_ and can not start with numbers<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\">    &lt;!--<span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span>--&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The hostname of the RM.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hadoop101<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span>   </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Environment variables that containers may override rather than use NodeManager&#x27;s default.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span>  </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRER_HOME<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>vim mapred-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span> </span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The runtime framework for executing MapReduce jobs.</span></span><br><span class=\"line\"><span class=\"language-xml\">  Can be one of local, classic or yarn.</span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure>\n<p>vim worker</p>\n<p>不能有空格</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop100</span><br><span class=\"line\">hadoop101</span><br><span class=\"line\">hadoop102</span><br></pre></td></tr></table></figure>\n<p>初始化 namenode</p>\n<p>只有集群第一次启动需要初始化，此时会创建数据目录</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs namenode -format </span><br></pre></td></tr></table></figure>\n<p>启动集群</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_NAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_DATANODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_SECONDARYNAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_RESOURCEMANAGER_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_NODEMANAGER_USER</span>=root</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 只有在这里的java_home 他启动的时候才会认，他不认/etc/profile 和 ~/.<span class=\"property\">bashrc</span></span><br><span class=\"line\">vim etc/hadoop/hadoop-env.<span class=\"property\">sh</span></span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">JAVA_HOME</span>=<span class=\"regexp\">/root/</span>jdk8u352-b08</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">sbin/start-dfs.<span class=\"property\">sh</span></span><br><span class=\"line\"></span><br><span class=\"line\">jps确认 </span><br><span class=\"line\"></span><br><span class=\"line\">curl <span class=\"attr\">http</span>:<span class=\"comment\">//192.168.13.190:9870/dfshealth.html#tab-overview</span></span><br><span class=\"line\">启动resourcemanager </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_NAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_DATANODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_SECONDARYNAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_RESOURCEMANAGER_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_NODEMANAGER_USER</span>=root</span><br><span class=\"line\"></span><br><span class=\"line\">设置主机互信</span><br><span class=\"line\"></span><br><span class=\"line\">sbin/start-yarn.<span class=\"property\">sh</span></span><br><span class=\"line\"></span><br><span class=\"line\">测试</span><br><span class=\"line\">hadoop fs -mkdir /input</span><br><span class=\"line\">hadoop fs -put /root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/input/<span class=\"number\">1.</span>txt /input </span><br><span class=\"line\">存储位置</span><br><span class=\"line\">/data/hadoop-root/dfs/data/current/<span class=\"variable constant_\">BP</span>-<span class=\"number\">897483346</span>-<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.190</span>-<span class=\"number\">1716822540714</span>/current/finalized/subdir0/subdir0</span><br><span class=\"line\"></span><br><span class=\"line\">测试：</span><br><span class=\"line\">hadoop jar /root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-<span class=\"number\">3.3</span><span class=\"number\">.6</span>.<span class=\"property\">jar</span> wordcount /input /output</span><br><span class=\"line\"></span><br><span class=\"line\">如果有报错，是需要往mapred-site.<span class=\"property\">xml</span>中加东西的，那么添加如下字段，在重新执行</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.application.classpath<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.map.env<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.env<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221955345.png\" alt=\"image-20240804221955345\"></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbWlyYWNsZS1sdW5hL3AvMTc3ODUzMTAuaHRtbA==\">https://www.cnblogs.com/miracle-luna/p/17785310.html</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZmFucWlzb2Z0L3AvMTc4NTkwODYuaHRtbA==\">https://www.cnblogs.com/fanqisoft/p/17859086.html</span></p>\n<h4 id=\"故障处理\"><a class=\"markdownIt-Anchor\" href=\"#故障处理\">#</a> 故障处理：</h4>\n<p>停进程，删除数据目录，格式化</p>\n<h4 id=\"记录历史运行情况\"><a class=\"markdownIt-Anchor\" href=\"#记录历史运行情况\">#</a> 记录历史运行情况</h4>\n<p>启动 history server</p>\n<p>mapred-site.xml</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;hadoop100:10020&lt;/value&gt;</span><br><span class=\"line\">  &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;hadoop100:19888&lt;/value&gt;</span><br><span class=\"line\">  &lt;description&gt;MapReduce JobHistory Server Web UI host:port&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure>\n<p>集群内同步该配置文件</p>\n<p>重启 yarn</p>\n<p>启动历史服务器，在 namenode</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>\n<p>测试历史功能：</p>\n<p>hadoop jar /root/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222012877.png\" alt=\"image-20240804222012877\"></p>\n<h4 id=\"日志汇聚\"><a class=\"markdownIt-Anchor\" href=\"#日志汇聚\">#</a> 日志汇聚</h4>\n<p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222029398.png\" alt=\"image-20240804222029398\"></p>\n<p>namenode:</p>\n<p>yarn-site.xml</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class=\"line\">      each container&#x27;s logs and moves these logs onto a file-system, for e.g.</span><br><span class=\"line\">      HDFS, after the application completes. Users can configure the</span><br><span class=\"line\">      &quot;yarn.nodemanager.remote-app-log-dir&quot; and</span><br><span class=\"line\">      &quot;yarn.nodemanager.remote-app-log-dir-suffix&quot; properties to determine</span><br><span class=\"line\">      where these logs are moved to. Users can access the logs via the </span><br><span class=\"line\">      Application Timeline Server.</span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log-aggregation-enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    URL for log aggregation server</span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log.server.url<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>http://hadoop100:19888/jobhistory/logs<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>How long to keep aggregation logs before deleting them.  -1 disables.</span><br><span class=\"line\">    Be careful set this too small and you will spam the name node.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>604800<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span> </span><br><span class=\"line\">mapred --daemon stop historyserver</span><br><span class=\"line\"></span><br><span class=\"line\">stop-yarn.sh</span><br><span class=\"line\"></span><br><span class=\"line\">start-yarn.sh </span><br><span class=\"line\"></span><br><span class=\"line\">mapred --daemon start historyserver </span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222046081.png\" alt=\"image-20240804222046081\"></p>\n<p>启新任务观察：</p>\n<p>对于单个组件重启</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(1)分别启动/停止HDFS组件e</span><br><span class=\"line\">hdfs --daemon start/stop namenode/datanode/secoondarynamenode</span><br><span class=\"line\">(2)启动/停止YARN</span><br><span class=\"line\">yarn --daemon start/stop resourcemanager/nodemanager</span><br><span class=\"line\">#! /bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">ssh hadoop100 &quot;/root/hadoop-3.3.6/sbin/start-dfs.sh</span><br><span class=\"line\">ssh hadoop101 &quot;/root/hadoop-3.3.6/sbin/start-yarn.sh</span><br><span class=\"line\">ssh hadoop100 &quot;mapred --daemon start historyserver&quot;</span><br></pre></td></tr></table></figure>\n<h4 id=\"常用端口号\"><a class=\"markdownIt-Anchor\" href=\"#常用端口号\">#</a> 常用端口号</h4>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222100104.png\" alt=\"image-20240804222100104\"></p>\n<p>常用端口号</p>\n<p>hadoop3.x</p>\n<p>HDFS NameNode 内部通常端口：8020/9000/9820</p>\n<p>HDFS NameNode 对用户的查询端口：9870</p>\n<p>Yarn 查看任务运行情况的：8088</p>\n<p>历史服务器：19888</p>\n<p>hadoop2.x</p>\n<p>HDFS NameNode 内部通常端口：8020/9000</p>\n<p>HDFS NameNode 对用户的查询端口：50070</p>\n<p>Yarn 查看任务运行情况的：8088</p>\n<p>历史服务器：19888</p>\n<p>常用的配置文件</p>\n<p>3.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml  workers</p>\n<p>2.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml  slaves</p>\n<h2 id=\"hdfs\"><a class=\"markdownIt-Anchor\" href=\"#hdfs\">#</a> hdfs</h2>\n<p>HDFS (Hadoop Distributed File System), 它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起起来实现其功能，集群中的服务器有各自的角色。</p>\n<p>HDFS 的使用场景：适合一次写入，多次读出的场景。一个个文件经过创建、写入和关闭之后就不能改变。</p>\n<p>高容错性</p>\n<p>数据自动保存多个副本。它通过增加副本的形式，提高容错性。</p>\n<p>适合处理大数据</p>\n<p>数据规模：能够处理数据规模达到 GB、TB、甚至 PB 级别的数据</p>\n<p>文件规模：能够处理百万规模以上的文件数量，数量相当之大。</p>\n<p>可构建在廉价机器上，通过多副本机制，提高可靠性。</p>\n<p>不适合低延时数据访问，比如毫秒级的存储数据，是做不到到的</p>\n<p>无法高效的对大量小文件进行存储。</p>\n<p>存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的；</p>\n<p>小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。</p>\n<p>不支持并发写入、文件随机修改。</p>\n<p>一个文件只能有一个写，不允许多个线程同时写；</p>\n<p>仅支持数据 append (追加), 不支持文件的随机修改。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222115457.png\" alt=\"image-20240804222115457\"></p>\n<p>1) NameNode (nn): 就是 Master, 它是一个主管、管理者。</p>\n<p>(1) 管理 HDFS 的名称空间；</p>\n<p>(2) 配置副本策略；</p>\n<p>(3) 管理数据块 (Block) 映射信息；</p>\n<p>(4) 处理客户端读写请求。</p>\n<ol>\n<li>DataNode: 就是 Slave。NameNode 下达命令，DataNode 执行实际的操作。</li>\n</ol>\n<p>(1) 存储实际的数据块；</p>\n<p>(2) 执行数据块的读 / 写操作。</p>\n<p>3) Client: 就是客户端。</p>\n<p>(1) 文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block, 然后进行上传；</p>\n<p>(2) 与 NameNode 交互，获取文件的位置信息；</p>\n<p>(3) 与 DataNode 交互，读取或者写入数据；</p>\n<p>(4) Client 提供一些命令来管理 HDFS, 比如 NameNode 格式化；</p>\n<p>(5) Client 可以通过一些命令来访问 HDFS, 比如对 HDFS 增删查改操作；</p>\n<p>4) Secondary NameNode: 并非 NameNode 的热备。当 NameNode 挂掉的时候，它并不</p>\n<p>能马上替换 NameNode 并提供服务。</p>\n<p>(1) 辅助 NameNode, 分担其工作量，比如定期合并 Fsimage 和 bEdits, 并推送给 NameNode;</p>\n<p>(2) 在紧急情况下，可辅助恢复 NameNode。</p>\n<p>HDFS 中的文件在物理上是分块存储 (Block), 块的大小可以通过配置参数 (dfs.blocksize) 来规定，默认大小在 Hadoop2.x/3.x 版本中是 128M,1.x 版本中是 64M。</p>\n<p>磁盘速率更快的情况下，可以设置为 256M</p>\n<p>(1) HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</p>\n<p>(2) 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>\n<p>HDFS 块的大小设置主要取决于磁盘传输速率。</p>\n<h3 id=\"hdfs-shell\"><a class=\"markdownIt-Anchor\" href=\"#hdfs-shell\">#</a> hdfs shell</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hadoop fs -mkdir /haha</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 上传，剪切本地文件</span><br><span class=\"line\"> hadoop fs -moveFromLocal ./README.txt /haha</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 复制本地文件</span><br><span class=\"line\">  hadoop fs -copyFromLocal ./README.txt /haha</span><br><span class=\"line\">  </span><br><span class=\"line\">  # 复制本地文件</span><br><span class=\"line\"> hadoop fs -put ./README.txt /haha</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 追加文件到另一个文件末尾</span><br><span class=\"line\">hadoop fs -appendToFile ./README.txt /haha/www.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 下载</span><br><span class=\"line\">hadoop fs -copyToLocal /haha/1.txt ./1111.txt</span><br><span class=\"line\"># 下载</span><br><span class=\"line\">hadoop fs -get /haha/1.txt ./1111.txt</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># </span><br><span class=\"line\">hadoop fs -ls /</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -cat /haha/111.txt</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -chown aaa:aaa /haha/1.txt</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -cp</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -mv /haha/1.txt /xixix/2.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 显示一个文件的末尾1kb数据</span><br><span class=\"line\">hadoop fs -tail /haha/1.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 删除文件或文件夹</span><br><span class=\"line\">hadoop fs -rm</span><br><span class=\"line\">hadoop fs -rm -r</span><br><span class=\"line\"></span><br><span class=\"line\"># 查看文件夹下总大小</span><br><span class=\"line\">hadoop fs -du -s -h /haha</span><br><span class=\"line\"># 查看文件下每个文件大小</span><br><span class=\"line\">hadoop fs -du -h /haha</span><br><span class=\"line\"></span><br><span class=\"line\"># 修改副本大小,副本数大于机器数量时，只会创建对应的副本数。后续集群内新增机器，会增加到对应的数量。但是永远保持每个机器上至多一个副本</span><br><span class=\"line\">hadoop fs -setrep 10 /haha/1.txt</span><br></pre></td></tr></table></figure>\n<h3 id=\"hadoop-api\"><a class=\"markdownIt-Anchor\" href=\"#hadoop-api\">#</a> hadoop api</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 在windows下安装hadoop+winutils</span><br><span class=\"line\">https://blog.csdn.net/shulianghan/article/details/132045605</span><br><span class=\"line\"></span><br><span class=\"line\"># 配置maven</span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400168</span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400995</span><br><span class=\"line\"></span><br><span class=\"line\">hdfs-default &lt; hdfs-site &lt; resource &lt; 代码里修改configuraion.set</span><br><span class=\"line\"></span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400168</span><br><span class=\"line\"></span><br><span class=\"line\">https://blog.csdn.net/weixin_50956145/article/details/130511265</span><br><span class=\"line\">https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.30</span><br><span class=\"line\">https://www.bilibili.com/read/cv28415867/?jump_opus=1</span><br><span class=\"line\">https://www.cnblogs.com/linshengqian/p/15657694.html</span><br><span class=\"line\">https://www.runoob.com/maven/maven-build-life-cycle.html</span><br><span class=\"line\">https://www.cnblogs.com/xfeiyun/p/16740262.html</span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400995</span><br></pre></td></tr></table></figure>\n<p>mvn 处理依赖</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeGxkL2FydGljbGUvZGV0YWlscy84MjI4NDI2OQ==\">https://blog.csdn.net/lixld/article/details/82284269</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0ODg2MjEzL2FydGljbGUvZGV0YWlscy8xMjM0NjE1MjI=\">https://blog.csdn.net/qq_44886213/article/details/123461522</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tdm5yZXBvc2l0b3J5LmNvbS9hcnRpZmFjdC9vcmcuc2xmNGovc2xmNGotcmVsb2FkNGovMi4xLjAtYWxwaGEx\">https://mvnrepository.com/artifact/org.slf4j/slf4j-reload4j/2.1.0-alpha1</span></p>\n<h3 id=\"hdfs-写流程\"><a class=\"markdownIt-Anchor\" href=\"#hdfs-写流程\">#</a> hdfs 写流程</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222138786.png\" alt=\"image-20240804222138786\"></p>\n<blockquote>\n<p>（1）客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p>\n<p>（2）NameNode 返回是否可以上传。</p>\n<p>（3）客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p>\n<p>（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。</p>\n<p>（5）客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>\n<p>（6）dn1、dn2、dn3 逐级应答客户端。</p>\n<p>（7）客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet 会放入一个应答队列等待应答。</p>\n<p>（8）当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行 3-7 步）。</p>\n</blockquote>\n<h3 id=\"网络拓扑-节点距离计算\"><a class=\"markdownIt-Anchor\" href=\"#网络拓扑-节点距离计算\">#</a> 网络拓扑 - 节点距离计算</h3>\n<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。那么这个最近距离怎么计算呢？</p>\n<p>节点距离：两个节点到达最近的共同祖先的距离总和</p>\n<p>第一个副本在 Client 所处的节点上。如果客户端在集群外，随机选一个</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222158187.png\" alt=\"image-20240804222158187\"></p>\n<h3 id=\"hdfs-读流程\"><a class=\"markdownIt-Anchor\" href=\"#hdfs-读流程\">#</a> hdfs 读流程</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222217631.png\" alt=\"image-20240804222217631\"></p>\n<p>判断权限，文件是否存在</p>\n<p>选择原则：节点距离，节点负载</p>\n<p>串行读</p>\n<blockquote>\n<p>（1）客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</p>\n<p>（2）挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</p>\n<p>（3）DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。</p>\n<p>（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</p>\n</blockquote>\n<h3 id=\"nn-2nn\"><a class=\"markdownIt-Anchor\" href=\"#nn-2nn\">#</a> nn &amp; 2nn</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222236021.png\" alt=\"image-20240804222236021\"></p>\n<p>(1) Fsimage 文件：HDFS 文件系统元数据的一个永久性的检查点其中包含 HDFS 文件系统的所有目录和文件 inode 的序列化信息。</p>\n<p>Edits 文件：存放 HDFS 文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到 Edits 文件中。</p>\n<p>(3) seen txid 文件保存的是一个数字，就是最后一个 edits 的数字</p>\n<p>(4) 每次 NameNode 启动的时候都会将 Fsimage 文件读入内存，加载 Edits 里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成 NameNode 启动的时候就将 Fsimage 和 Edits 文件进行了合并。</p>\n<p>查看镜像文件</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs oiv -p <span class=\"variable constant_\">XML</span> -i fsimage_0000000000000000517 -o /root/image.<span class=\"property\">xml</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">479</span> <span class=\"variable constant_\">INFO</span> offlineImageViewer.<span class=\"property\">FSImageHandler</span>: <span class=\"title class_\">Loading</span> <span class=\"number\">5</span> strings</span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">GLOBAL</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">29</span> maxEntries=<span class=\"number\">536870911</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">USER</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">24</span> maxEntries=<span class=\"number\">16777215</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">GROUP</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">24</span> maxEntries=<span class=\"number\">16777215</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">XATTR</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">24</span> maxEntries=<span class=\"number\">16777215</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span><br><span class=\"line\">&lt;fsimage&gt;<span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">layoutVersion</span>&gt;</span>-66<span class=\"tag\">&lt;/<span class=\"name\">layoutVersion</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">onDiskVersion</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">onDiskVersion</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">oivRevision</span>&gt;</span>1be78238728da9266a4f88195058f08fd012bf9c<span class=\"tag\">&lt;/<span class=\"name\">oivRevision</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">NameSection</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">namespaceId</span>&gt;</span>345030828<span class=\"tag\">&lt;/<span class=\"name\">namespaceId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">genstampV1</span>&gt;</span>1000<span class=\"tag\">&lt;/<span class=\"name\">genstampV1</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">genstampV2</span>&gt;</span>1040<span class=\"tag\">&lt;/<span class=\"name\">genstampV2</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">genstampV1Limit</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">genstampV1Limit</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">lastAllocatedBlockId</span>&gt;</span>1073741864<span class=\"tag\">&lt;/<span class=\"name\">lastAllocatedBlockId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">txid</span>&gt;</span>517<span class=\"tag\">&lt;/<span class=\"name\">txid</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">NameSection</span>&gt;</span></span></span><br><span class=\"line\">&lt;<span class=\"title class_\">ErasureCodingSection</span>&gt;</span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">policyId</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">policyId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyName</span>&gt;</span>RS-6-3-1024k<span class=\"tag\">&lt;/<span class=\"name\">policyName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">cellSize</span>&gt;</span>1048576<span class=\"tag\">&lt;/<span class=\"name\">cellSize</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyState</span>&gt;</span>DISABLED<span class=\"tag\">&lt;/<span class=\"name\">policyState</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">codecName</span>&gt;</span>rs<span class=\"tag\">&lt;/<span class=\"name\">codecName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">dataUnits</span>&gt;</span>6<span class=\"tag\">&lt;/<span class=\"name\">dataUnits</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">parityUnits</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">parityUnits</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">policyId</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">policyId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyName</span>&gt;</span>RS-3-2-1024k<span class=\"tag\">&lt;/<span class=\"name\">policyName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">cellSize</span>&gt;</span>1048576<span class=\"tag\">&lt;/<span class=\"name\">cellSize</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyState</span>&gt;</span>DISABLED<span class=\"tag\">&lt;/<span class=\"name\">policyState</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">codecName</span>&gt;</span>rs<span class=\"tag\">&lt;/<span class=\"name\">codecName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">dataUnits</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">dataUnits</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">parityUnits</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">parityUnits</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">policyId</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">policyId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyName</span>&gt;</span>RS-LEGACY-6-3-1024k<span class=\"tag\">&lt;/<span class=\"name\">policyName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">cellSize</span>&gt;</span>1048576<span class=\"tag\">&lt;/<span class=\"name\">cellSize</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyState</span>&gt;</span>DISABLED<span class=\"tag\">&lt;/<span class=\"name\">policyState</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">codecName</span>&gt;</span>rs-legacy<span class=\"tag\">&lt;/<span class=\"name\">codecName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">dataUnits</span>&gt;</span>6<span class=\"tag\">&lt;/<span class=\"name\">dataUnits</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">parityUnits</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">parityUnits</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>datanode 主动向 namenode 上报文件快信息</p>\n<h3 id=\"查看edits日志\"><a class=\"markdownIt-Anchor\" href=\"#查看edits日志\">#</a> 查看 edits 日志</h3>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs oev -p <span class=\"variable constant_\">XML</span> -i /data/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000520  -o /root/image.<span class=\"property\">xml</span></span><br><span class=\"line\"></span><br><span class=\"line\">记录操作步骤，仅进行追加</span><br><span class=\"line\"></span><br><span class=\"line\">每进行一小时进行合并</span><br><span class=\"line\"></span><br><span class=\"line\">2nn没有edits_progres</span><br><span class=\"line\"></span><br><span class=\"line\">namenode 会把edit_Progress 后的合并</span><br><span class=\"line\">比如后面合并<span class=\"number\">356</span>以后的</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222255010.png\" alt=\"image-20240804222255010\"></p>\n<h3 id=\"checkpoint\"><a class=\"markdownIt-Anchor\" href=\"#checkpoint\">#</a> checkpoint</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3600<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    The number of seconds between two periodic checkpoints. </span><br><span class=\"line\">    Support multiple time unit suffix(case insensitive), as described</span><br><span class=\"line\">    in dfs.heartbeat.interval.If no time unit is specified then seconds</span><br><span class=\"line\">    is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1000000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The Secondary NameNode or CheckpointNode will create a checkpoint</span><br><span class=\"line\">  of the namespace every &#x27;dfs.namenode.checkpoint.txns&#x27; transactions, regardless</span><br><span class=\"line\">  of whether &#x27;dfs.namenode.checkpoint.period&#x27; has expired.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>60<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The SecondaryNameNode and CheckpointNode will poll the NameNode</span><br><span class=\"line\">  every &#x27;dfs.namenode.checkpoint.check.period&#x27; seconds to query the number</span><br><span class=\"line\">  of uncheckpointed transactions. Support multiple time unit suffix(case insensitive),</span><br><span class=\"line\">  as described in dfs.heartbeat.interval.If no time unit is specified then</span><br><span class=\"line\">  seconds is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">3600s每隔一小时执行一次</span><br><span class=\"line\">或者每100w次执行一次，每隔60s看下有没有到100w</span><br></pre></td></tr></table></figure>\n<h3 id=\"datanode\"><a class=\"markdownIt-Anchor\" href=\"#datanode\">#</a> datanode</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222303971.png\" alt=\"image-20240804222303971\"></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>21600000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.directoryscan.interval<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>21600<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Interval in seconds for Datanode to scan data directories and</span><br><span class=\"line\">  reconcile the difference between blocks in memory and on the disk.</span><br><span class=\"line\">  Support multiple time unit suffix(case insensitive), as described</span><br><span class=\"line\">  in dfs.heartbeat.interval.If no time unit is specified then seconds</span><br><span class=\"line\">  is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<h3 id=\"数据完整性\"><a class=\"markdownIt-Anchor\" href=\"#数据完整性\">#</a> 数据完整性</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如下是DataNode节点保证数据完整性的方法。</span><br><span class=\"line\">(1)当DataNode读取Block的时候,它会计算CheckSum。</span><br><span class=\"line\">(2)如果计算后的CheckSum,与Block创建时值不一样,说明Block已经损坏。</span><br><span class=\"line\">(3)Client读取其他DataNode上的Block。</span><br><span class=\"line\">(4)常见的校验算法crc(32),md5(128),shal(160) </span><br><span class=\"line\">(5)DataNode在其文件创建后周期验证CheckSum。</span><br></pre></td></tr></table></figure>\n<h3 id=\"掉线参数\"><a class=\"markdownIt-Anchor\" href=\"#掉线参数\">#</a> 掉线参数</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222317915.png\" alt=\"image-20240804222317915\"></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.heartbeat.interval<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    Determines datanode heartbeat interval in seconds.</span><br><span class=\"line\">    Can use the following suffix (case insensitive):</span><br><span class=\"line\">    ms(millis), s(sec), m(min), h(hour), d(day)</span><br><span class=\"line\">    to specify the time (such as 2s, 2m, 1h, etc.).</span><br><span class=\"line\">    Or provide complete number in seconds (such as 30 for 30 seconds).</span><br><span class=\"line\">    If no time unit is specified then seconds is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>300000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    This time decides the interval to check for expired datanodes.</span><br><span class=\"line\">    With this value and dfs.heartbeat.interval, the interval of</span><br><span class=\"line\">    deciding the datanode is stale or not is also calculated.</span><br><span class=\"line\">    The unit of this configuration is millisecond.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">hdfs块大小 读写速度越快，块可以配置越大。一般是128M或者256M</span><br><span class=\"line\">shell操作</span><br><span class=\"line\">读些流程</span><br></pre></td></tr></table></figure>\n<h2 id=\"mapreduce\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce\">#</a> mapreduce</h2>\n<p>MapReduce 是一个分布式运算程序的编程框架，是用户开发 &quot;基于 Hadoop 的数据分析应用&quot; 的核心框架。</p>\n<p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>\n<p>MapReduce: 自己处理业务相关代码 + 自身的默认代码</p>\n<p>优点:</p>\n<p>1、易于编程。用户只关心，业务逻辑。实现框架的接口。</p>\n<p>2、良好扩展性：可以动态增加服务器，解决计算资源不够问题</p>\n<p>3、高容错性。任何一台机器挂擦，可以将任务转移到其他节点。</p>\n<p>4、适合海量数据计算 (TB/PB) 几千台服务器共同计算。</p>\n<p>重阳市公安局</p>\n<p>トラスメット吉祥如意</p>\n<p>半夏散文章</p>\n<p>Праведая праведая我姓张却长不出你爱的模样</p>\n<p>天下之忧而忧无虑</p>\n<p>小麦小兜到了解那样的很美</p>\n<p>多少年重阳节快乐成长庚子大吉</p>\n<p>缺点:</p>\n<p>1、不擅长实时计算。Mysql</p>\n<p>2、不擅长流式计算。Sparkstreaming flink</p>\n<p>3、不擅长 DAG 有向无环图计算。spark</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222335906.png\" alt=\"image-20240804222335906\"></p>\n<p>1) MapReduce 运算程序一般需要分成 2 个阶段：Map 阶段和 Reduce 阶段</p>\n<p>2) Map 阶段的并发 MapTask, 完全并行运行，互不相干</p>\n<p>3) Reduce 阶段的并发 ReduceTask, 完全互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出</p>\n<p>4) MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行</p>\n<p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程:</p>\n<p>(1) MrAppMaster: 负责整个程序的过程调度及状态协调。</p>\n<p>(2) MapTask: 负责 Map 阶段的整个数据处理流程。</p>\n<p>(3) ReduceTask: 负责 Reduce 阶段的整个数据处理流程。</p>\n<p>数据序列化类型</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222347314.png\" alt=\"image-20240804222347314\"></p>\n<h3 id=\"mapreduce编程规范\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce编程规范\">#</a> mapreduce 编程规范</h3>\n<p>1.Mapper 阶段</p>\n<p>(1) 用户自定义的 Mapper 要继承自己的父类</p>\n<p>(2) Mapper 的输入数据是 KV 对的形式 (KV 的类型可自定义)</p>\n<p>(3) Mapper 中的业务逻辑写在 map () 方法中</p>\n<p>(4) Mapper 的输出数据是 KV 对的形式 (KV 的类型可自定义)</p>\n<p>(5) map () 方法 (MapTask 进程) 对每一个 &lt; K,V &gt; 调用一次</p>\n<p>2.Reducer 阶段</p>\n<p>(1) 用户自定义的 Reducer 要继承自己的父类</p>\n<p>(2) Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV</p>\n<p>(3) Reducer 的业务逻辑写在 reduce () 方法中</p>\n<p>ReduceTask 进程对每一组相同 k 的 &lt;k,v&gt; 组调用一次 reduce () 方法法</p>\n<p>3.Driver 阶段</p>\n<p>相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群，提交的是</p>\n<p>封装了 MapReduce 程序相关运行参数的 job 对象</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222405781.png\" alt=\"image-20240804222405781\"></p>\n<h3 id=\"wordcount\"><a class=\"markdownIt-Anchor\" href=\"#wordcount\">#</a> wordcount</h3>\n<p>1.mapper</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.wordcount2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountMapper</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Mapper</span>&lt;LongWritable , Text, Text, IntWritable&gt;&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">Text</span> <span class=\"variable\">outk</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Text</span>();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outv</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">map</span><span class=\"params\">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"comment\">// get a line ,text to string</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">line</span> <span class=\"operator\">=</span> value.toString();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 切割</span></span><br><span class=\"line\">        String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// loop write out</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (String word:words)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            outk.set(word);</span><br><span class=\"line\">            context.write(outk,outv);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>reducer</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.wordcount2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountReducer</span> <span class=\"keyword\">extends</span>  <span class=\"title class_\">Reducer</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outV</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">reduce</span><span class=\"params\">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">sum</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (IntWritable value : values) &#123;</span><br><span class=\"line\">            sum += value.get();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        outV.set(sum);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 写出</span></span><br><span class=\"line\">        context.write(key,outV);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>driver</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.wordcount2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountDriver</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 1 获取job</span></span><br><span class=\"line\">        <span class=\"type\">Configuration</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">        <span class=\"type\">Job</span> <span class=\"variable\">job</span> <span class=\"operator\">=</span> Job.getInstance(conf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 2 设置jar包路径</span></span><br><span class=\"line\">        job.setJarByClass(WordCountDriver.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 3 关联mapper和reducer</span></span><br><span class=\"line\">        job.setMapperClass(WordCountMapper.class);</span><br><span class=\"line\">        job.setReducerClass(WordCountReducer.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 4 设置map输出的kv类型</span></span><br><span class=\"line\">        job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 5 设置最终输出的kV类型</span></span><br><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 6 设置输入路径和输出路径</span></span><br><span class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">0</span>]));</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">1</span>]));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 7 提交job</span></span><br><span class=\"line\">        <span class=\"type\">boolean</span> <span class=\"variable\">result</span> <span class=\"operator\">=</span> job.waitForCompletion(<span class=\"literal\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        System.exit(result ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>package，将没有依赖的包放到 hadoop 集群执行</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop jar MapReduceDemo-<span class=\"number\">1.0</span>-SNAPSHOT.jar org.example.wordcount2.WordCountDriver /input /uuu</span><br></pre></td></tr></table></figure>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2NDEzMDY1L2FydGljbGUvZGV0YWlscy8xMTY0MTkzMjY=\">https://blog.csdn.net/m0_46413065/article/details/116419326</span></p>\n<h3 id=\"序列化\"><a class=\"markdownIt-Anchor\" href=\"#序列化\">#</a> 序列化</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222417067.png\" alt=\"image-20240804222417067\"></p>\n<p>java 自带 serializable 很重，带了各种校验头信息。</p>\n<p>hadoop 序列化：</p>\n<p>紧凑：存储空间少</p>\n<p>快速：传输速度快</p>\n<p>互操作性:</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222431605.png\" alt=\"image-20240804222431605\"></p>\n<h3 id=\"mapreduce-框架原理\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce-框架原理\">#</a> mapreduce 框架原理</h3>\n<h4 id=\"inputformat数据输入\"><a class=\"markdownIt-Anchor\" href=\"#inputformat数据输入\">#</a> inputformat 数据输入</h4>\n<p>MapTask 并行度决定机制</p>\n<p>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</p>\n<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222447121.png\" alt=\"image-20240804222447121\"></p>\n<h4 id=\"fileinputformat-切片机制\"><a class=\"markdownIt-Anchor\" href=\"#fileinputformat-切片机制\">#</a> fileinputformat 切片机制</h4>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222459009.png\" alt=\"image-20240804222459009\"></p>\n<p>(1) 源码中计算切片大小的公式</p>\n<p>Math.max(minSize, Math.min(maxSize, blockSize));</p>\n<p>mapreduce.input.fileinputformat.split.minsize=1 默认值为 1</p>\n<p>mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值 Long.MAXValue</p>\n<p>因此，默认情况下，切片大小 = blocksize。</p>\n<p>(2) 切片大小设置</p>\n<p>maxsize (切片最大值): 参数如果调得比 blockSize 小，则会让切片变小，而且就等于配置的这个参数的值。</p>\n<p>minsize (切片最小值): 参数调的比 blockSize 大，则可以让切片变得比 blockSize 还大。</p>\n<p>(3) 获取切片信息 API</p>\n<p>// 获取切片的文件名称</p>\n<p>String name = inputSplit.getPath().getNamie ();</p>\n<p>// 根据文件类型获取切片信息</p>\n<p>FileSplit inputSplit = (FileSplit) context.getInputsplit()</p>\n<p>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValuue TextInputFormat 、NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</p>\n<h4 id=\"textinputformat切片机制\"><a class=\"markdownIt-Anchor\" href=\"#textinputformat切片机制\">#</a> textinputformat 切片机制</h4>\n<p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，LongWritable 类型。值是这行的内容，不包括任何行终止符 (换行符和回车符),Text 类型。</p>\n<h4 id=\"combinetextinputformat\"><a class=\"markdownIt-Anchor\" href=\"#combinetextinputformat\">#</a> combinetextinputformat</h4>\n<p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask, 这样如果有大量小文件，就会产生大量的 MapTask, 处理效率极其低下。</p>\n<p>1) 应用场景:</p>\n<p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个 / 小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。T</p>\n<p>2) 虚拟存储切片最大值设置 (</p>\n<p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</p>\n<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>\n<p>3) 切片机制～</p>\n<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>\n<p>切片过程</p>\n<p>(a) 判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。</p>\n<p>(b) 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 5 设置最终输出的kV类型</span></span><br><span class=\"line\">job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class=\"line\">CombineTextInputFormat.setMaxInputSplitSize(job,<span class=\"number\">4194034</span>);</span><br><span class=\"line\"><span class=\"comment\">// 6 设置输入路径和输出路径</span></span><br><span class=\"line\">FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">0</span>]));</span><br><span class=\"line\">FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">1</span>]));</span><br></pre></td></tr></table></figure>\n<h3 id=\"mapreduce工作流程\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce工作流程\">#</a> mapreduce 工作流程</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222514569.png\" alt=\"image-20240804222514569\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222529350.png\" alt=\"image-20240804222529350\"></p>\n<h3 id=\"shuffle\"><a class=\"markdownIt-Anchor\" href=\"#shuffle\">#</a> shuffle</h3>\n<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</p>\n<p>可以进行排序压缩等操作</p>\n<p>maptask 阶段：</p>\n<p>对 key 的索引按照字典序快排</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222546927.png\" alt=\"image-20240804222546927\"></p>\n<h3 id=\"partition分区\"><a class=\"markdownIt-Anchor\" href=\"#partition分区\">#</a> partition 分区</h3>\n<p>要求将统计结果按照条件输出到不同文件中 (分区)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class=\"line\">        </span><br><span class=\"line\">        job.setNumReduceTasks(<span class=\"number\">2</span>);</span><br><span class=\"line\">        <span class=\"comment\">// 6 设置输入路径和输出路径</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HashPartitioner</span>&lt;K, V&gt; extendss Partitioner&lt;K, V&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getPartition</span><span class=\"params\">(K key, V value, <span class=\"type\">int</span> 1humReduceTasks)</span></span><br><span class=\"line\">         <span class=\"keyword\">return</span> (key.hashCode() &amp; Integer.MAX_VALUE)៖ numReduceTasks;</span><br></pre></td></tr></table></figure>\n<p>如果分区 &gt; 1 才有 hash</p>\n<p>否则直接 partition-1 = 0 只有 0 号分区</p>\n<p>默认分区是根据 key 的 hashCode 对 ReduceTasks 个数取模得到的。用户没法控制哪个 key 存储到哪个分区。</p>\n<p>自定义 partitoner</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222606892.png\" alt=\"image-20240804222606892\"></p>\n<h3 id=\"排序\"><a class=\"markdownIt-Anchor\" href=\"#排序\">#</a> 排序</h3>\n<p>对于 MapTask, 它会将处理的结果暂时放到环形缓冲区中，当环不形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有有文件进行归并排序。</p>\n<p>对于 ReduceTask, 它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内字中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask 统一对内存和磁盘上的所有数据进行一次师日并排序</p>\n<h2 id=\"yarn\"><a class=\"markdownIt-Anchor\" href=\"#yarn\">#</a> yarn</h2>\n<p>Yam 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。</p>\n<p>YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件构成。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222630987.png\" alt=\"image-20240804222630987\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222643801.png\" alt=\"image-20240804222643801\"></p>\n<h3 id=\"yarn调度器\"><a class=\"markdownIt-Anchor\" href=\"#yarn调度器\">#</a> yarn 调度器</h3>\n<p>目前，Hadoop 作业调度器主要有三种：FIFO、容量 (CapacityScheduler) 和公平 (Fair Scheduler)。Apache Hadoop3.1.3 默认的资源调度器是 Capacity SScheduler</p>\n<h4 id=\"fifo调度器\"><a class=\"markdownIt-Anchor\" href=\"#fifo调度器\">#</a> FIFO 调度器</h4>\n<p>(FirstInFirstOut) 单队列，根据提交作业的先后顺序，先来先服务</p>\n<h4 id=\"容量调度器\"><a class=\"markdownIt-Anchor\" href=\"#容量调度器\">#</a> 容量调度器</h4>\n<p>1、多队列：每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略。</p>\n<p>2、容量保证：管理员可为每个队列设置资源最低保证和资源使用上限</p>\n<p>3、灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。</p>\n<p>4、多租户：支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会又对同一用户提交的作业所占资源量进行限定。</p>\n<p>1) 队列资源分配</p>\n<p>从 root 开始，使用深度优先算法，优先选择资源占用率最低的队列分配资源。</p>\n<p>2) 作业资源分配</p>\n<p>默认按照提交作业的优先级和提交时间顺序分配资源。</p>\n<p>3) 容器资源分配</p>\n<p>按照容器的优先级分配资源；如果优先级相同，按照数据本地性原则:</p>\n<p>(1) 任务和数据在同一节点</p>\n<p>(2) 任务和数据在同一机架</p>\n<p>(3) 任务和数据不在同一节点也不在同一机架</p>\n<h4 id=\"公平调度器\"><a class=\"markdownIt-Anchor\" href=\"#公平调度器\">#</a> 公平调度器</h4>\n<p>同队列所有任务共享资源，在时间尺度上获得公平的资源</p>\n<p>1) 与容量调度器相同点</p>\n<p>(1) 多队列：支持多队列多作业</p>\n<p>(2) 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线</p>\n<p>(3) 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。</p>\n<p>(4) 多租户：支持多用户共享集群和多应用程序同时运行；为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。</p>\n<p>2) 与容量调度器不同点</p>\n<p>(1) 核心调度策略不同</p>\n<p>容量调度器：优先选择资源利用率低的队列</p>\n<p>公平调度器：优先选择对资源的缺额比例大的</p>\n<p>(2) 每个队列可以单独设置资源分配方式</p>\n<p>容量调度器：FIFO，DRF</p>\n<p>公平调度器：FIFO，DRF，FAIR</p>\n<p>公平调度器设计目标是：在时间尺度上，所有作业获得公平的的资源。某一时刻一个作业应获资源和实际获取资源的差距叫 &quot;缺额&quot;</p>\n<p>调度器会优先为缺额大的作业分配资源</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222659518.png\" alt=\"image-20240804222659518\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222717084.png\" alt=\"image-20240804222717084\"></p>\n<h4 id=\"drf\"><a class=\"markdownIt-Anchor\" href=\"#drf\">#</a> DRF</h4>\n<p>DRF (DominantResource Fairness), 我们之前说的资源，都那是单一标准，例如只考虑内存 (也是 Yam 默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU, 网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。</p>\n<p>那么在 YARN 中，我们用 DRF 来决定如何调度:</p>\n<p>假设集群一共有 100CPU 和 10T 内存，而应用 A 需要 (2CPU,300GB), 应用 B 需要 (6CPU,100GB)。则两个应用分别需要 A (2% CPU,3% 内存) 和 B (6% CPU,1% 为存) 的资源，这就意味着 A 是内存主导的，B 是 CPU 主导的，针对这种情况，我们可以选择 DRF 策略对不同应用进行不同资源 (CPU 和内存) 的一个不同比例的限制。</p>\n<h3 id=\"yarn-shell\"><a class=\"markdownIt-Anchor\" href=\"#yarn-shell\">#</a> yarn shell</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn application -list </span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">11</span> <span class=\"number\">06</span>:<span class=\"number\">34</span>:<span class=\"number\">10</span>,<span class=\"number\">406</span> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop101/<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.191</span>:<span class=\"number\">8032</span></span><br><span class=\"line\">Total number of <span class=\"title function_\">applications</span> <span class=\"params\">(application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: [])</span>:<span class=\"number\">1</span></span><br><span class=\"line\">                Application-Id      Application-Name        Application-Type          User           Queue                   State           Final-State             Progress                        Tracking-URL</span><br><span class=\"line\">application_1717055365946_0004            word count               MAPREDUCE          root         <span class=\"keyword\">default</span>                 RUNNING             UNDEFINED                   <span class=\"number\">0</span>%              http:<span class=\"comment\">//hadoop100:38735</span></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn application -list </span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">11</span> <span class=\"number\">06</span>:<span class=\"number\">34</span>:<span class=\"number\">10</span>,<span class=\"number\">406</span> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop101/<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.191</span>:<span class=\"number\">8032</span></span><br><span class=\"line\">Total number of <span class=\"title function_\">applications</span> <span class=\"params\">(application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: [])</span>:<span class=\"number\">1</span></span><br><span class=\"line\">                Application-Id      Application-Name        Application-Type          User           Queue                   State           Final-State             Progress                        Tracking-URL</span><br><span class=\"line\">application_1717055365946_0004            word count               MAPREDUCE          root         <span class=\"keyword\">default</span>                 RUNNING             UNDEFINED                   <span class=\"number\">0</span>%              http:<span class=\"comment\">//hadoop100:38735</span></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn application -kill application_1717055365946_0004  </span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn logs -applicationId application_1717055365946_0004  </span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn logs -applicationId application_1717055365946_0004  -containerId </span><br><span class=\"line\"># 查看尝试运行的任务</span><br><span class=\"line\">yarn applicationattempt -list application_1717055365946_0004 </span><br><span class=\"line\"># </span><br><span class=\"line\">yarn applicationattempt -status application_1717055365946_0004 </span><br><span class=\"line\"># yarn 容器状态，只有在任务运行中可以查看</span><br><span class=\"line\">yarn container -list </span><br><span class=\"line\">yarn container -status</span><br><span class=\"line\">yarn node -list -all</span><br><span class=\"line\"># 加载队列配置</span><br><span class=\"line\">yarn rmadmin -refreshQueues</span><br><span class=\"line\"># 查看队列</span><br><span class=\"line\">yarn queue -status <span class=\"keyword\">default</span> </span><br></pre></td></tr></table></figure>\n<h3 id=\"yarn生产环境参数\"><a class=\"markdownIt-Anchor\" href=\"#yarn生产环境参数\">#</a> yarn 生产环境参数</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222733648.png\" alt=\"image-20240804222733648\"></p>\n<h3 id=\"多队列\"><a class=\"markdownIt-Anchor\" href=\"#多队列\">#</a> 多队列</h3>\n<p>1) 在生产环境怎么创建队列？</p>\n<p>(1) 调度器默认就 1 个 default 队列，不能满足生产要求。</p>\n<p>(2) 按照框架:hive/spark/flink 每个框架的任务放入指定的队列 (企业用的不是特别多)</p>\n<p>(3) 按照业务模块：登录注册、购物车、下单、业务部门 1、业务部门 2</p>\n<p>2) 创建多队列的好处</p>\n<p>(1) 因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。</p>\n<p>(2) 实现任务的降级使用，特殊时期保证重要的任务队列资原充足。</p>\n<p>业务部门 1 (重要)=》业务部门 2 (比较重要)=》下单 (一般)=》购物车 (一般)=》登录注册 (次要)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  vim capacity-scheduler.xml </span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"keyword\">default</span>,hive&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The queues at the <span class=\"built_in\">this</span> <span class=\"title function_\">level</span> <span class=\"params\">(root is the root queue)</span>.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.capacity&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">40</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">60</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.user-limit-factor&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">0.7</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      Default queue user limit a percentage from <span class=\"number\">0.0</span> to <span class=\"number\">1.0</span>.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">1</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      Default queue user limit a percentage from <span class=\"number\">0.0</span> to <span class=\"number\">1.0</span>.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.maximum-capacity&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">60</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The maximum capacity of the <span class=\"keyword\">default</span> queue.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.state&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The state of the <span class=\"keyword\">default</span> queue. State can be one of RUNNING or STOPPED.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.acl_submit_applications&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The ACL of who can submit jobs to the <span class=\"keyword\">default</span> queue.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.acl_administer_queue&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The ACL of who can administer jobs on the <span class=\"keyword\">default</span> queue.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetimeTimeout</span><br><span class=\"line\">参考资料：https:<span class=\"comment\">//blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 </span><br><span class=\"line\">--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-application-lifetime&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;-<span class=\"number\">1</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 如果application没指定超时时间，则用<span class=\"keyword\">default</span>-application-lifetime作为默认值 --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.<span class=\"keyword\">default</span>-application-lifetime&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;-<span class=\"number\">1</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"># 提交任务到不同队列</span><br><span class=\"line\">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-example-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar wordcount -D mapreduce.job.queuename=hive /input /outputttt</span><br><span class=\"line\"></span><br><span class=\"line\"># 或者在driver中</span><br><span class=\"line\"><span class=\"type\">Configure</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configure</span>();</span><br><span class=\"line\">conf.set(<span class=\"string\">&quot;mapreduce.job.queuename&quot;</span>,<span class=\"string\">&quot;hive&quot;</span>);</span><br></pre></td></tr></table></figure>\n<h3 id=\"任务优先级\"><a class=\"markdownIt-Anchor\" href=\"#任务优先级\">#</a> 任务优先级</h3>\n<p>容量调度器，支持任务优先级的配置，在资源紧张时，优先吸高的任务将优先获取资源。默认情况，Yarn 将所有任务的优先级限制为 0，若想使用任务的优先级功能，须开放该限制。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">5</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\">hadoop jar hadoop-mapreduce-examples-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar pi -D mapreduce.job.priorit=<span class=\"number\">5</span> <span class=\"number\">5</span> <span class=\"number\">20000</span></span><br><span class=\"line\"></span><br><span class=\"line\"># 动态设置优先级</span><br><span class=\"line\">yarn application -appId xxx -upgradePriority <span class=\"number\">6</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置多队列公平调度器\"><a class=\"markdownIt-Anchor\" href=\"#配置多队列公平调度器\">#</a> 配置多队列公平调度器</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim yarn-site.xml</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;配置使用公平调度器&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;/opt/<span class=\"keyword\">module</span>/hadoop-<span class=\"number\">3.1</span><span class=\"number\">.3</span>/etc/hadoop/fair-scheduler.xml&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;指明公平调度器队列分配配置文件&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"literal\">false</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;禁止队列间资源抢占&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\">vim fair-scheduler.xml</span><br><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span><br><span class=\"line\">&lt;allocations&gt;</span><br><span class=\"line\">  &lt;!-- 单个队列中Application Master占用资源的最大比例,取值<span class=\"number\">0</span>-<span class=\"number\">1</span> ，企业一般配置<span class=\"number\">0.1</span> --&gt;</span><br><span class=\"line\">  &lt;queueMaxAMShareDefault&gt;<span class=\"number\">0.5</span>&lt;/queueMaxAMShareDefault&gt;</span><br><span class=\"line\">  &lt;!-- 单个队列最大资源的默认值 test atguigu <span class=\"keyword\">default</span> --&gt;</span><br><span class=\"line\">  &lt;queueMaxResourcesDefault&gt;4096mb,4vcores&lt;/queueMaxResourcesDefault&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">  &lt;!-- 增加一个队列test --&gt;</span><br><span class=\"line\">  &lt;queue name=<span class=\"string\">&quot;test&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 队列最小资源 --&gt;</span><br><span class=\"line\">    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列最大资源 --&gt;</span><br><span class=\"line\">    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列中最多同时运行的应用数，默认<span class=\"number\">50</span>，根据线程数配置 --&gt;</span><br><span class=\"line\">    &lt;maxRunningApps&gt;<span class=\"number\">4</span>&lt;/maxRunningApps&gt;</span><br><span class=\"line\">    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span><br><span class=\"line\">    &lt;!-- &lt;maxAMShare&gt;<span class=\"number\">0.5</span>&lt;/maxAMShare&gt;--&gt;</span><br><span class=\"line\">    &lt;!-- 该队列资源权重,默认值为<span class=\"number\">1.0</span> --&gt;</span><br><span class=\"line\">    &lt;weight&gt;<span class=\"number\">1.0</span>&lt;/weight&gt;</span><br><span class=\"line\">    &lt;!-- 队列内部的资源分配策略 --&gt;</span><br><span class=\"line\">    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;</span><br><span class=\"line\">  &lt;/queue&gt;</span><br><span class=\"line\">  &lt;!-- 增加一个队列atguigu --&gt;</span><br><span class=\"line\">  &lt;queue name=<span class=\"string\">&quot;atguigu&quot;</span> type=<span class=\"string\">&quot;parent&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 队列最小资源 --&gt;</span><br><span class=\"line\">    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列最大资源 --&gt;</span><br><span class=\"line\">    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列中最多同时运行的应用数，默认<span class=\"number\">50</span>，根据线程数配置 --&gt;</span><br><span class=\"line\">    &lt;maxRunningApps&gt;<span class=\"number\">4</span>&lt;/maxRunningApps&gt;</span><br><span class=\"line\">    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span><br><span class=\"line\">    &lt;!-- &lt;maxAMShare&gt;<span class=\"number\">0.5</span>&lt;/maxAMShare&gt;--&gt;</span><br><span class=\"line\">    &lt;!-- 该队列资源权重,默认值为<span class=\"number\">1.0</span> --&gt;</span><br><span class=\"line\">    &lt;weight&gt;<span class=\"number\">1.0</span>&lt;/weight&gt;</span><br><span class=\"line\">    &lt;!-- 队列内部的资源分配策略 --&gt;</span><br><span class=\"line\">    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;</span><br><span class=\"line\">  &lt;/queue&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">  &lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;</span><br><span class=\"line\">  &lt;queuePlacementPolicy&gt;</span><br><span class=\"line\">    &lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; <span class=\"literal\">false</span>表示：如果指定队列不存在,不允许自动创建--&gt;</span><br><span class=\"line\">    &lt;rule name=<span class=\"string\">&quot;specified&quot;</span> create=<span class=\"string\">&quot;false&quot;</span>/&gt;</span><br><span class=\"line\">    &lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;</span><br><span class=\"line\">    &lt;rule name=<span class=\"string\">&quot;nestedUserQueue&quot;</span> create=<span class=\"string\">&quot;true&quot;</span>&gt;</span><br><span class=\"line\">        &lt;rule name=<span class=\"string\">&quot;primaryGroup&quot;</span> create=<span class=\"string\">&quot;false&quot;</span>/&gt;</span><br><span class=\"line\">    &lt;/rule&gt;</span><br><span class=\"line\">    &lt;!-- 最后一个规则必须为reject或者<span class=\"keyword\">default</span>。Reject表示拒绝创建提交失败，<span class=\"keyword\">default</span>表示把任务提交到<span class=\"keyword\">default</span>队列 --&gt;</span><br><span class=\"line\">    &lt;rule name=<span class=\"string\">&quot;reject&quot;</span> /&gt;</span><br><span class=\"line\">  &lt;/queuePlacementPolicy&gt;</span><br><span class=\"line\">&lt;/allocations&gt;</span><br><span class=\"line\"># 指定用户和不指定用户</span><br><span class=\"line\">hadoop jar hadoop-mapreduce-examples-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar pi -D mapreduce.job.queuename=root.test <span class=\"number\">1</span> <span class=\"number\">1</span> </span><br><span class=\"line\">hadoop jar hadoop-mapreduce-examples-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar pi <span class=\"number\">1</span> <span class=\"number\">1</span> </span><br></pre></td></tr></table></figure>\n<h3 id=\"tool接口\"><a class=\"markdownIt-Anchor\" href=\"#tool接口\">#</a> tool 接口</h3>\n<p>期望可以动态传参，结果报错，误认为是第一个输入参数。</p>\n<p>[@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1</p>\n<p>1）需求：自己写的程序也可以动态修改参数。编写 Yarn 的 Tool 接口。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">创建类WordCount并实现Tool接口：</span><br><span class=\"line\"><span class=\"keyword\">package</span> com.atguigu.yarn;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCount</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Tool</span> &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">private</span> Configuration conf;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"type\">Job</span> <span class=\"variable\">job</span> <span class=\"operator\">=</span> Job.getInstance(conf);</span><br><span class=\"line\"> </span><br><span class=\"line\">        job.setJarByClass(WordCountDriver.class);</span><br><span class=\"line\"> </span><br><span class=\"line\">        job.setMapperClass(WordCountMapper.class);</span><br><span class=\"line\">        job.setReducerClass(WordCountReducer.class);</span><br><span class=\"line\"> </span><br><span class=\"line\">        job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\"> </span><br><span class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">0</span>]));</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">1</span>]));</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">return</span> job.waitForCompletion(<span class=\"literal\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setConf</span><span class=\"params\">(Configuration conf)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.conf = conf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Configuration <span class=\"title function_\">getConf</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> conf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountMapper</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"type\">Text</span> <span class=\"variable\">outK</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Text</span>();</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outV</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">map</span><span class=\"params\">(LongWritable key, Text value, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">line</span> <span class=\"operator\">=</span> value.toString();</span><br><span class=\"line\">            String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                outK.set(word);</span><br><span class=\"line\"> </span><br><span class=\"line\">                context.write(outK, outV);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountReducer</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outV</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>();</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">reduce</span><span class=\"params\">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">sum</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (IntWritable value : values) &#123;</span><br><span class=\"line\">                sum += value.get();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            outV.set(sum);</span><br><span class=\"line\"> </span><br><span class=\"line\">            context.write(key, outV);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222803108.png\" alt=\"image-20240804222803108\"></p>\n",
            "tags": [
                "大数据"
            ]
        }
    ]
}