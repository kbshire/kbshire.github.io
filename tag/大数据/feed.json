{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hexo • All posts by \"大数据\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2024/11/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E6%A6%82%E8%AE%BA/",
            "url": "http://example.com/2024/11/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E6%A6%82%E8%AE%BA/",
            "title": "大数据生态概论",
            "date_published": "2024-11-01T05:38:45.000Z",
            "content_html": "<h1 id=\"大数据生态概论\"><a class=\"markdownIt-Anchor\" href=\"#大数据生态概论\">#</a> 大数据生态概论</h1>\n<p>大数据通常指的是规模巨大、类型多样、增长速度快的数据集合，这些数据集合难以使用传统的数据处理应用软件进行管理。大数据的特点通常被概括为 “4V”：</p>\n<ul>\n<li>\n<p>Volume（体量）：数据量巨大，从 TB 级别跃升至 PB 级别甚至更高。</p>\n</li>\n<li>\n<p>Velocity（速度）：数据产生和处理的速度非常快，需要实时分析而非事后分析。</p>\n</li>\n<li>\n<p>Variety（多样性）：数据类型繁多，包括结构化数据（如数据库）和非结构化数据（如文本、音频、视频）。</p>\n</li>\n<li>\n<p>Veracity（真实性）：数据的质量和准确性，这是确保分析结果可靠的关键。</p>\n</li>\n</ul>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303223913.png\" alt=\"image.png\"></p>\n<p>大数据生态系统是指一系列相互关联的技术、工具和服务，它们共同支持大数据的收集、存储、处理、分析和可视化。这个生态系统包括以下几个关键组成部分：</p>\n<ul>\n<li>\n<p>数据源：包括各种内部和外部数据源，如企业数据库、社交媒体、物联网设备等。</p>\n</li>\n<li>\n<p>数据存储：用于存储大数据的基础设施，如分布式文件系统（HDFS）、NoSQL 数据库（Cassandra）和数据仓库（Amazon Redshift）。</p>\n</li>\n<li>\n<p>数据处理：用于处理和分析大数据的框架和工具，如 Hadoop 的 MapReduce、Spark 的 Spark Core 等。</p>\n</li>\n<li>\n<p>数据分析与挖掘：用于从大数据中提取有价值信息的工具和技术，如数据挖掘、机器学习、统计分析等。</p>\n</li>\n<li>\n<p>数据可视化：将分析结果以图表、仪表板等形式展现，帮助用户更好地理解和利用数据。</p>\n</li>\n<li>\n<p>数据安全与治理：确保数据安全和合规性的技术和策略，如数据加密、访问控制、数据治理框架等。</p>\n</li>\n</ul>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303223930.png\" alt=\"image.png\"></p>\n<h2 id=\"hadoop生态系统\"><a class=\"markdownIt-Anchor\" href=\"#hadoop生态系统\">#</a> hadoop 生态系统</h2>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303223941.png\" alt=\"image.png\"></p>\n<ol>\n<li>Hadoop 简介</li>\n</ol>\n<p>Hadoop 是一个开源的分布式存储和计算框架，由 Apache 软件基金会开发。它旨在处理大规模数据集，并能够在计算机集群上分布式处理这些数据。Hadoop 的设计理念是 “移动计算比移动数据更经济”，这意味着它将计算任务分配到数据所在的节点上，而不是将数据移动到计算节点。</p>\n<p>Hadoop 的核心优势在于其高度的可扩展性、容错性和成本效益。它可以在廉价的硬件上运行，通过冗余存储和自动故障转移来保证数据的可靠性。</p>\n<ol>\n<li>Hadoop 的核心组件</li>\n</ol>\n<ul>\n<li>HDFS（Hadoop Distributed File System）：</li>\n</ul>\n<p>HDFS 是 Hadoop 的分布式文件系统，它提供了对应用程序数据的高吞吐量访问。HDFS 设计用于存储非常大的文件，并采用 “一次写入，多次读取” 的模型。它将文件分割成块，并在集群的多个节点上存储这些块的副本，以提高数据的可靠性和访问性能。</p>\n<ul>\n<li>MapReduce：</li>\n</ul>\n<p>MapReduce 是 Hadoop 的编程模型，用于处理和生成大数据集。它允许用户编写并行处理数据的程序，而不需要关心底层的并行化和分布式细节。MapReduce 作业通常包括两个阶段：Map 阶段和 Reduce 阶段。Map 阶段将输入数据转换成键值对，Reduce 阶段则对这些键值对进行聚合操作。</p>\n<ul>\n<li>YARN（Yet Another Resource Negotiator）：</li>\n</ul>\n<p>YARN 是 Hadoop 2.0 引入的资源管理器，它负责集群资源的管理和作业调度。YARN 将 Hadoop 的资源管理功能从 MapReduce 中分离出来，使得其他数据处理框架（如 Spark）也可以在 Hadoop 集群上运行。YARN 通过将资源管理和作业调度分离，提高了集群的利用率和灵活性。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303223959.png\" alt=\"image.png\"></p>\n<h2 id=\"spark生态系统\"><a class=\"markdownIt-Anchor\" href=\"#spark生态系统\">#</a> spark 生态系统</h2>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303224007.png\" alt=\"image.png\"></p>\n<ol>\n<li>Spark 简介</li>\n</ol>\n<p>Apache Spark 是一个开源的分布式计算系统，它提供了高效的数据处理能力和易用的 API。Spark 旨在加速大数据处理任务，特别适用于需要多次迭代的机器学习和图形处理算法。与 Hadoop 的 MapReduce 相比，Spark 提供了内存计算能力，这使得它在处理迭代算法时速度更快。</p>\n<p>Spark 支持多种编程语言，包括 Scala、Java、Python 和 R，这使得它对不同技术背景的开发者都很友好。Spark 可以在多种环境中运行，包括独立的 Spark 集群、Hadoop YARN、Apache Mesos 以及云环境。</p>\n<ol>\n<li>Spark 的核心组件</li>\n</ol>\n<ul>\n<li>Spark Core：</li>\n</ul>\n<p>Spark Core 是 Spark 的基础引擎，负责任务调度、内存管理、错误恢复、与存储系统交互等核心功能。Spark Core 还提供了 RDD（Resilient Distributed Datasets）的概念，这是一种容错的、可以并行操作的数据集合。</p>\n<ul>\n<li>Spark SQL：</li>\n</ul>\n<p>Spark SQL 是 Spark 用来处理结构化数据的模块。它允许用户通过 SQL 或 DataFrame API 来查询数据，这些查询会被转换成高效的 Spark 作业。Spark SQL 支持多种数据源，包括 Hive、Parquet、JSON 等，并且可以与现有的 Hive 仓库集成。</p>\n<ul>\n<li>Spark Streaming：</li>\n</ul>\n<p>Spark Streaming 是 Spark 的实时数据处理模块，它允许用户构建可扩展、高吞吐量、容错的实时数据处理应用。Spark Streaming 接收实时输入数据流，并将数据分割成批次，然后由 Spark 引擎处理，生成最终的批次结果流。</p>\n<ol>\n<li>Spark 生态系统中的其他工具</li>\n</ol>\n<ul>\n<li>MLlib（Machine Learning Library）：</li>\n</ul>\n<p>MLlib 是 Spark 的可扩展机器学习库，它提供了一套通用的机器学习算法和工具，包括分类、回归、聚类、协同过滤等。MLlib 的设计目标是让机器学习变得简单且可扩展，它与 Spark 的 RDD 和 DataFrame API 紧密集成。</p>\n<ul>\n<li>GraphX：</li>\n</ul>\n<p>GraphX 是 Spark 的图形处理库，它提供了一套图形计算 API，允许用户进行并行图形操作。GraphX 扩展了 RDD 的概念，引入了弹性分布式属性图（Resilient Distributed Property Graph），这是一种有向多重图，其属性附加到每个顶点和边上。</p>\n<p>Spark 生态系统还包括其他工具，如 SparkR（一个用于 R 语言的 Spark 包）、Spark Streaming Kafka（用于与 Kafka 集成）、Spark Streaming Flume（用于与 Flume 集成）等。这些工具和库共同构成了一个强大的数据处理和分析平台，适用于各种大数据应用场景。</p>\n<h2 id=\"flink\"><a class=\"markdownIt-Anchor\" href=\"#flink\">#</a> flink</h2>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303224027.png\" alt=\"image.png\"></p>\n<p><strong>Flink</strong>，由 Apache Software Foundation 开发，是一个支持流式和批处理的大数据框架。它的低延迟、高吞吐量和良好的容错性使其适用于复杂的数据处理应用。</p>\n<p><strong>DataStream API</strong></p>\n<p>DataStream API 是 Flink 用于处理实时数据流的核心组件，支持多种数据源（如 Kafka、Kinesis）的数据流输入，提供丰富的转换操作，包括 map、flatMap、filter、aggregate 等，方便开发人员进行实时数据处理。</p>\n<p><strong>DataSet API</strong></p>\n<p>DataSet API 用于批处理任务，支持对静态数据集合（如文件系统或数据库表）的处理。DataSet API 提供了丰富的数据转换和操作功能，支持大规模的数据处理任务。</p>\n<p><strong>CEP</strong>**（复杂事件处理）**</p>\n<p>CEP 是 Flink 用于复杂事件处理的组件，它通过定义事件模式和匹配规则，支持从实时数据流中提取和处理复杂事件序列，适合用于实时监控、告警和事件分析等应用场景。</p>\n<p><strong>Table API 和 ****SQL</strong></p>\n<p>Table API 和 SQL 是 Flink 提供的高级数据处理接口，支持开发人员使用 SQL 语法进行数据查询和分析。Table API 和 SQL 可以无缝集成 Flink DataStream 和 DataSet API，支持流式和批处理任务。</p>\n<h2 id=\"kafka\"><a class=\"markdownIt-Anchor\" href=\"#kafka\">#</a> kafka</h2>\n<p><strong>Kafka</strong>，由 Apache Software Foundation 开发，是一个分布式的流处理平台，具有高吞吐量、低延迟、高可扩展和高容错等特点。Kafka 广泛用于实时数据流的采集、传输和处理。</p>\n<p><strong>Producer</strong></p>\n<p>Producer 是 Kafka 中负责数据生产的组件，它将数据写入 Kafka 的主题中。Producer 支持大规模并发写入，保证数据的高效传输和存储。</p>\n<p><strong>Consumer</strong></p>\n<p>Consumer 是 Kafka 中负责数据消费的组件，它从 Kafka 的主题中读取数据，并将数据传输到后续的处理系统或存储系统中。Consumer 支持多种消费模式，如点对点和发布订阅等。</p>\n<p><strong>Broker</strong></p>\n<p>Broker 是 Kafka 的核心组件，负责数据的存储、复制和分发。Kafka 集群由多个 Broker 组成，它们协同工作，保证数据的高可用性和可靠性。</p>\n<p><strong>Zookeeper</strong></p>\n<p>Zookeeper 是 Kafka 用来进行集群状态管理和协同操作的工具，负责 Broker 的协调、状态监控和元数据管理。通过 Zookeeper，Kafka 能够实现高效的节点管理和任务调度。</p>\n<h2 id=\"hive\"><a class=\"markdownIt-Anchor\" href=\"#hive\">#</a> hive</h2>\n<p>由 Apache Software Foundation 开发，是一个基于 Hadoop 的<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuZmFucnVhbi5jb20vc29sdXRpb25zL2R3P3V0bV9zb3VyY2U9c2VvJmFtcDt1dG1fbWVkaXVtPXNlbyZhbXA7dXRtX2NhbXBhaWduPXdwJmFtcDt1dG1fdGVybT1td2I=\">数据仓库</span>工具，它提供了类似 SQL 的查询语言 HiveQL，支持大规模结构化数据的存储、查询和分析。</p>\n<p><strong>HiveQL</strong></p>\n<p>HiveQL 是 Hive 提供的查询语言，支持 SQL 语法的查询和操作。通过 HiveQL，用户可以方便地进行数据查询、插入和更新等操作，无需复杂的编程。</p>\n<p><strong>SerDe</strong></p>\n<p>SerDe 是 Hive 的数据序列化和反序列化组件，它负责将数据从 HDFS 等存储系统中读取，并转换为可供查询的数据格式。SerDe 支持多种数据格式，如 JSON、CSV、Avro 等。</p>\n<p><strong>Metastore</strong></p>\n<p>Metastore 是 Hive 的元数据管理组件，它存储了表的定义、列的信息以及数据的位置等元数据。Metastore 通过 API 与其他组件交互，支持数据的高效管理和查询。</p>\n<p><strong>Optimizer</strong></p>\n<p>Optimizer 是 Hive 的查询优化器，它通过分析查询计划，选择最优的执行路径，提升查询的性能和效率。Optimizer 使用多种技术，如索引、分区和并行计算等，优化数据查询和处理任务。</p>\n<h2 id=\"hbase\"><a class=\"markdownIt-Anchor\" href=\"#hbase\">#</a> hbase</h2>\n<p><strong>HBase</strong>，由 Apache Software Foundation 开发，是一个开源的、分布式的、面向列的数据库，用于实时随机读写大规模的结构化数据。HBase 基于 HDFS 构建，具有高并发、高可靠和高可扩展性。</p>\n<p><strong>Data Model</strong></p>\n<p>HBase 的数据模型是基于表、行和列族的。每个表由多个行组成，每行包含一个唯一的行键，行中的数据按列族进行存储和管理。HBase 的数据模型支持大规模数据的灵活存储和查询。</p>\n<p><strong>Region Server</strong></p>\n<p>Region Server 是 HBase 的核心组件，负责管理数据的存储和读取。每个 Region Server 管理多个 Region（表的子集），对数据进行分片和负载均衡，确保集群的高并发性能。</p>\n<p><strong>Master Server</strong></p>\n<p>Master Server 是 HBase 的管理节点，负责表的创建、删除和元数据管理等操作。Master Server 通过协调 Region Server，保证数据的高可用性和一致性。</p>\n<p><strong>Zookeeper</strong></p>\n<p>HBase 使用 Zookeeper 进行集群管理和协调，负责节点的状态管理、配置分发和故障恢复等任务，通过 Zookeeper，HBase 实现了高效的分布式协调和任务调度。</p>\n<h2 id=\"flume\"><a class=\"markdownIt-Anchor\" href=\"#flume\">#</a> flume</h2>\n<p>Flume，Apache 下的一个顶级开源项目，一个分布式、可靠的数据收集组件。能够高效的收集，整合数据，还可以将来自不同源的大量数据汇聚到数据中心存储落地。 目前常用于企业内收集整合日志数据，但由于其数据源的可自定义<strong>特性</strong>，还可用于传输结构化数据 (oracle, mysql 等)，也常被用于流式数据的采集输入工具。</p>\n<h2 id=\"数据处理框架\"><a class=\"markdownIt-Anchor\" href=\"#数据处理框架\">#</a> 数据处理框架</h2>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/20250303224040.png\" alt=\"image.png\"></p>\n<ol>\n<li>批处理框架</li>\n</ol>\n<ul>\n<li>MapReduce：</li>\n</ul>\n<p>MapReduce 是 Hadoop 的核心处理框架，它提供了一种编程模型，用于处理和生成大数据集。MapReduce 作业通常包括两个阶段：Map 阶段和 Reduce 阶段。Map 阶段将输入数据转换成键值对，Reduce 阶段则对这些键值对进行聚合操作。MapReduce 适合批量处理，但它的迭代处理性能较差，不适合需要快速响应的场景。</p>\n<ul>\n<li>Tez：</li>\n</ul>\n<p>Apache Tez 是一个构建在 YARN 之上的应用框架，它旨在提供一个灵活、高性能的批处理和交互式数据处理引擎。Tez 通过优化 DAG（有向无环图）的执行来提高处理效率，它允许开发者定义复杂的处理流程，并且可以与 Hive、Pig 等 Hadoop 生态系统工具集成。</p>\n<ol>\n<li>流处理框架</li>\n</ol>\n<ul>\n<li>Storm：</li>\n</ul>\n<p>Apache Storm 是一个分布式实时计算系统，它设计用于处理无界数据流。Storm 可以实时处理数据，保证每个消息都会被处理，即使是在出现故障的情况下。Storm 的拓扑结构定义了数据流的处理逻辑，可以用于实时分析、在线机器学习、持续计算等场景。</p>\n<ul>\n<li>Flink：</li>\n</ul>\n<p>Apache Flink 是一个开源的流处理框架，它提供了一个高吞吐量、低延迟的数据处理引擎。Flink 支持事件时间处理和状态管理，可以处理无界和有界数据流。Flink 还提供了批处理 API，使得它既可以作为流处理框架，也可以作为批处理框架使用。</p>\n<ol>\n<li>混合处理框架</li>\n</ol>\n<ul>\n<li>Kafka Streams：</li>\n</ul>\n<p>Kafka Streams 是一个轻量级的客户端库，用于构建应用程序和微服务，其中输入和输出数据存储在 Kafka 集群中。Kafka Streams 结合了流处理和批处理的优点，允许开发者构建实时的、可扩展的数据处理应用。它提供了高级的 API 来处理数据流，并且与 Kafka 紧密集成，提供了高度的可靠性和容错性。</p>\n<p>这些大数据处理框架各有特点，适用于不同的应用场景。批处理框架适合处理大规模的静态数据集，流处理框架适合实时数据处理，而混合处理框架则结合了两者的优点，提供了更灵活的处理能力。随着大数据技术的不断发展，新的处理框架也在不断涌现，为用户提供了更多的选择和可能性。</p>\n<h2 id=\"集成管理工具hue\"><a class=\"markdownIt-Anchor\" href=\"#集成管理工具hue\">#</a> 集成管理工具 HUE</h2>\n<p>HUE 是基于 Web 形式发布的集成管理工具，可以与大数据相关组件进行集成。通过 HUE 可以管理 Hadoop 中的相关组件，也可以管理 Spark 中的相关组件。</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0Y2FzdF9jbi9hcnRpY2xlL2RldGFpbHMvMTI4NTUxMzI3\">https://blog.csdn.net/itcast_cn/article/details/128551327</span></p>\n<h2 id=\"iceberg\"><a class=\"markdownIt-Anchor\" href=\"#iceberg\">#</a> iceberg</h2>\n<p>介于上层计算引擎和底层存储格式之间的一个中间层。这个中间层不是数据存储的方式，只是定义了数据的元数据组织方式，并向计算引擎提供统一的类似传统数据库中 “表” 的语义。它的底层仍然是 Parquet、ORC 等存储格式。</p>\n<p>Iceberg 是一种开放的数据湖表格式。可以简单理解为是基于计算层（Flink、Spark）和存储层（ORC，Parquet，Avro）的一个中间层，用 Flink 或者 Spark 将数据写入 Iceberg，然后再通过其他方式来读取这个表，比如 Spark，Flink，Presto 等。</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JlX3JhY2xlL2FydGljbGUvZGV0YWlscy8xMzI2Mzc0Mjg=\">https://blog.csdn.net/be_racle/article/details/132637428</span></p>\n<h2 id=\"starrocks\"><a class=\"markdownIt-Anchor\" href=\"#starrocks\">#</a> starrocks</h2>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzY0MjYxOTgyL2FydGljbGUvZGV0YWlscy8xMzg3MDk5NTg=\">https://blog.csdn.net/m0_64261982/article/details/138709958</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLm1pcnJvcnNoaXAuY24vemgvZG9jcy9xdWlja19zdGFydC9zaGFyZWQtbm90aGluZy8=\">https://docs.mirrorship.cn/zh/docs/quick_start/shared-nothing/</span></p>\n<h2 id=\"olap-oltp\"><a class=\"markdownIt-Anchor\" href=\"#olap-oltp\">#</a> OLAP OLTP</h2>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDczMDY0MzY=\">https://zhuanlan.zhihu.com/p/647306436</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N4c0FmZmFibGUvYXJ0aWNsZS9kZXRhaWxzLzEzOTgzNTAyMg==\">https://blog.csdn.net/sxsAffable/article/details/139835022</span></p>\n<h2 id=\"etl-elt\"><a class=\"markdownIt-Anchor\" href=\"#etl-elt\">#</a> ETL ELT</h2>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzQyMDUxOTc1MC9hbnN3ZXIvMzQ2MTIyMDY5MQ==\">https://www.zhihu.com/question/420519750/answer/3461220691</span></p>\n<h2 id=\"数据库-数据仓库-数据湖-数据集市-湖仓一体\"><a class=\"markdownIt-Anchor\" href=\"#数据库-数据仓库-数据湖-数据集市-湖仓一体\">#</a> 数据库 数据仓库 数据湖 数据集市 湖仓一体</h2>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9iYWlqaWFoYW8uYmFpZHUuY29tL3M/aWQ9MTc4MDA2MzA2ODk4NTkyOTEwOCZhbXA7d2ZyPXNwaWRlciZhbXA7Zm9yPXBj\">https://baijiahao.baidu.com/s?id=1780063068985929108&amp;wfr=spider&amp;for=pc</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tYmQuYmFpZHUuY29tL25ld3NwYWdlL2RhdGEvbGFuZGluZ3N1cGVyP2NvbnRleHQ9JTdCJTIybmlkJTIyJTNBJTIybmV3c185NzI1MDI1MTAxODQxNDA0NjU5JTIyJTdEJmFtcDtuX3R5cGU9MSZhbXA7cF9mcm9tPTQ=\">https://mbd.baidu.com/newspage/data/landingsuper?context={&quot;nid&quot;%3A&quot;news_9725025101841404659&quot;}&amp;n_type=1&amp;p_from=4</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tYmQuYmFpZHUuY29tL25ld3NwYWdlL2RhdGEvbGFuZGluZ3N1cGVyP2NvbnRleHQ9JTdCJTIybmlkJTIyJTNBJTIybmV3c185NDI2NzIwNDQxNzQ5NjY2NzU0JTIyJTdEJmFtcDtuX3R5cGU9MSZhbXA7cF9mcm9tPTQ=\">https://mbd.baidu.com/newspage/data/landingsuper?context={&quot;nid&quot;%3A&quot;news_9426720441749666754&quot;}&amp;n_type=1&amp;p_from=4</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8yNDA5MDU2\">https://cloud.tencent.com/developer/article/2409056</span></p>\n<h2 id=\"大数据平台-数据中台\"><a class=\"markdownIt-Anchor\" href=\"#大数据平台-数据中台\">#</a> 大数据平台 数据中台</h2>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjZGR0b21hdG8vYXJ0aWNsZS9kZXRhaWxzLzE0MDg5MTI4NA==\">https://blog.csdn.net/ccddtomato/article/details/140891284</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzU3MzQ0MzkzL2FydGljbGUvZGV0YWlscy8xMzk2NDY2MTc=\">https://blog.csdn.net/m0_57344393/article/details/139646617</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8yMDg5NTY2\">https://cloud.tencent.com/developer/article/2089566</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83MDY5MDg3ODA=\">https://zhuanlan.zhihu.com/p/706908780</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuZmFucnVhbi5jb20vYmxvZy9hcnRpY2xlLzUyMC8=\">https://www.fanruan.com/blog/article/520/</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9kZXZlbG9wZXIuYmFpZHUuY29tL2FydGljbGUvZGV0YWlsLmh0bWw/aWQ9MjkzNTEw\">https://developer.baidu.com/article/detail.html?id=293510</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0LzI0MDFfODM4MTcwMjQvYXJ0aWNsZS9kZXRhaWxzLzEzNzgwMzY5MQ==\">https://blog.csdn.net/2401_83817024/article/details/137803691</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NTM5NzMyNTE=\">https://zhuanlan.zhihu.com/p/653973251</span></p>\n",
            "tags": [
                "大数据"
            ]
        },
        {
            "id": "http://example.com/2024/10/01/clickhouse/",
            "url": "http://example.com/2024/10/01/clickhouse/",
            "title": "Clickhouse",
            "date_published": "2024-10-01T05:38:45.000Z",
            "content_html": "<h2 id=\"clickhouse\"><a class=\"markdownIt-Anchor\" href=\"#clickhouse\">#</a> clickhouse</h2>\n<p>Yandex 于 2016 年开源的<strong>列式存储数据库</strong>（DBMS），使用 C++ 语言编写，主要用于在线分析处理查询（OLAP），能够使用 SQL 查询实时生成分析数据报告</p>\n<p><strong>列式储存的好处：</strong></p>\n<p>➢ 对于列的聚合，计数，求和等统计操作原因优于行式存储。</p>\n<p>➢由于某一列的数据类型都是相同的，针对于数据存储更容易进行数据压缩，每一列选择更优的数据压缩算法，大大提高了数据的压缩比重。</p>\n<p>➢ 由于数据压缩比更好，一方面节省了磁盘空间，另一方面对于 cache 也有了更大的发挥空间。</p>\n<p><strong>DBMS</strong> <strong>的功能</strong></p>\n<p>几乎覆盖了标准 SQL 的大部分语法，包括 DDL 和 DML，以及配套的各种函数，用户管 理及权限管理，数据的备份与恢复。</p>\n<p><strong>多样化引擎</strong></p>\n<p>ClickHouse 和 MySQL 类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类 20 多种引擎。</p>\n<p><strong>高吞吐写入能力</strong></p>\n<p>ClickHouse 采用类 LSMTree 的结构，数据写入后定期在后台 Compadction。通过类 LSM tree 的结构，ClickHouse 在数据导入时全部是顺序 append 写，写入后数据段不可更改，在后台 compaction 时也是多个段 mergesort 后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力，即便在 HDD 上也有着优异的写入性能。</p>\n<p><strong>数据分区与线程级并行</strong></p>\n<p>ClickHouse 将数据划分为多个 partition, 每个 partition 再进一步划分为多个 index granularity (索引粒度), 然后通过多个 CPU 核心分别处理其中的一部分来实现并行数据处理。在这种设计下，单条 Query 就能利用整机所有 CPU。极致的并行处理能力，极大的降低了查询延时。</p>\n<p>所以，ClickHouse 即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多 cpu, 就不利于同时并发多条直询。所以对于高 qps 的查询业务，ClickHouse 并不是强项。</p>\n<h3 id=\"安装单机clickhouse\"><a class=\"markdownIt-Anchor\" href=\"#安装单机clickhouse\">#</a> 安装单机 clickhouse</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">前置准备</span><br><span class=\"line\">vim /etc/security/limits.conf</span><br><span class=\"line\">* soft nofile <span class=\"number\">65536</span></span><br><span class=\"line\">* hard nofile <span class=\"number\">65536</span></span><br><span class=\"line\">* soft nproc <span class=\"number\">131072</span></span><br><span class=\"line\">* hard nproc <span class=\"number\">131072</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">/etc/security/limits.d/nproc.conf</span><br><span class=\"line\">* soft nofile <span class=\"number\">65536</span></span><br><span class=\"line\">* hard nofile <span class=\"number\">65536</span></span><br><span class=\"line\">* soft nproc <span class=\"number\">131072</span></span><br><span class=\"line\">* hard nproc <span class=\"number\">131072</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">apt install libtool *unixODBC* -y</span><br><span class=\"line\"></span><br><span class=\"line\">jps -ml # 备份进程</span><br><span class=\"line\"><span class=\"number\">123633</span> org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class=\"line\"><span class=\"number\">124131</span> org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class=\"line\"><span class=\"number\">124804</span> org.apache.hadoop.util.RunJar /root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/lib/hive-service-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar org.apache.hive.service.server.HiveServer2</span><br><span class=\"line\"><span class=\"number\">131204</span> org.apache.hadoop.util.RunJar /root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/lib/hive-metastore-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar org.apache.hadoop.hive.metastore.HiveMetaStore</span><br><span class=\"line\"><span class=\"number\">123831</span> org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class=\"line\"><span class=\"number\">152133</span> org.apache.flink.runtime.webmonitor.history.HistoryServer --configDir /root/flink-<span class=\"number\">1.17</span><span class=\"number\">.2</span>/conf</span><br><span class=\"line\"><span class=\"number\">191481</span> sun.tools.jps.Jps -ml</span><br><span class=\"line\"><span class=\"number\">44508</span> org.apache.spark.deploy.history.HistoryServer</span><br><span class=\"line\"><span class=\"number\">124335</span> org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer</span><br><span class=\"line\"></span><br><span class=\"line\">https:<span class=\"comment\">//packages.clickhouse.com/deb/pool/main/c/</span></span><br><span class=\"line\">https:<span class=\"comment\">//clickhouse.com/docs/en/install#available-installation-options</span></span><br><span class=\"line\"></span><br><span class=\"line\"># 执行后返回</span><br><span class=\"line\">ClickHouse has been successfully installed.</span><br><span class=\"line\"></span><br><span class=\"line\">Start clickhouse-server with:</span><br><span class=\"line\"> sudo clickhouse start/status/restart</span><br><span class=\"line\"></span><br><span class=\"line\">Start clickhouse-client with:</span><br><span class=\"line\"> clickhouse-client --password</span><br><span class=\"line\"></span><br><span class=\"line\"># clickhouse备份文件，默认在/etc/clickhouse-server </span><br><span class=\"line\">config.xml  服务端配置</span><br><span class=\"line\">users.xml   参数配置</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 修改配置文件</span><br><span class=\"line\">vim config.xml </span><br><span class=\"line\">    &lt;listen_host&gt;::&lt;/listen_host&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"># 执行查询操作</span><br><span class=\"line\">  clickhouse-client --query <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">  clickhouse-client -m</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br></pre></td></tr></table></figure>\n<h3 id=\"数据类型\"><a class=\"markdownIt-Anchor\" href=\"#数据类型\">#</a> 数据类型</h3>\n<p>整形</p>\n<p>Uint 8/16/32/64</p>\n<p>int 8/16/32/64</p>\n<p>浮点型</p>\n<p>Float32/64</p>\n<p>布尔型</p>\n<p>可以用 uint8，取值限制 0 1</p>\n<p>Decimal</p>\n<p>Decimal32 (s), 相当于 Decimal (9-s,s), 有效位数为 1~9</p>\n<p>Decimal64 (s), 相当于 Decimal (18-s,s), 有效位数为 1~18</p>\n<p>Decimal128 (s), 相当于 Decimal (38-s,s), 有效位数为 1~38</p>\n<p>字符串</p>\n<p>String</p>\n<p>FixedString (N)   固定长度，不够添加空字节，超过返回错误</p>\n<p>枚举类型</p>\n<p>Enum8/16  用 String = intx 描述</p>\n<p>时间类型</p>\n<p>Date 接受年 - 月 - 日的字符串比如 2019-12-16’</p>\n<p>Datetime 接受年 - 月 - 日时：分: 秒的字符串比如 2019-12-16 20:50:10’</p>\n<p>Datetime64 接受年 - 月 - 日 时：分: 秒。亚秒的字符串比如 '20199-12-16 20:50:10.66</p>\n<p>数组</p>\n<p>Array (T)   T 类型数组</p>\n<p>Array (1,2)     [2,1]   一个数组中类型必须相同</p>\n<p>不能在 mergetree 表中使用多维数组</p>\n<p>Nullable</p>\n<p>引擎分为表引擎和库引擎</p>\n<h3 id=\"表引擎\"><a class=\"markdownIt-Anchor\" href=\"#表引擎\">#</a> 表引擎</h3>\n<p>表引擎决定了如何存储数据，大小写敏感</p>\n<p>表引擎的使用方式就是必须显式在创建表时定义该表使用的引擎，以及引擎使用的相关参数。</p>\n<h4 id=\"tinylog\"><a class=\"markdownIt-Anchor\" href=\"#tinylog\">#</a> TinyLog</h4>\n<p>以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表</p>\n<p>生产环境上作用有限。可以用于平时练习测试用。</p>\n<p>create table haha(id String, name String) ENGINE=TinyLog;</p>\n<h4 id=\"memory\"><a class=\"markdownIt-Anchor\" href=\"#memory\">#</a> Memory</h4>\n<p>内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的的性能表现 (超过 10G/s)。</p>\n<p>一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大 (上限大概 1 亿行) 的场景。</p>\n<h4 id=\"mergetree\"><a class=\"markdownIt-Anchor\" href=\"#mergetree\">#</a> MergeTree</h4>\n<p>ClickHouse 中最强大的表引擎当属 MergeTree (合并树) 引擎及该系列 (*MergeTree) 中的其他引擎，支持索引和分区，地位可以相当于 innodb 之于 Mysql。而且基于 MergeTree, 还衍生出了，也是非常有特色的引擎。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 建表显示指定表引擎</span><br><span class=\"line\">create table <span class=\"title function_\">t_sn</span> <span class=\"params\">(id Uint32, id String,amount Decimal(<span class=\"number\">16</span>,<span class=\"number\">2</span>)</span>)engine=MergeTree partition by <span class=\"title function_\">toYYYYMMDD</span><span class=\"params\">(create_time)</span> primary <span class=\"title function_\">key</span><span class=\"params\">(id)</span> order <span class=\"title function_\">by</span><span class=\"params\">(id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"># primary key</span><br><span class=\"line\">主键可以重复。提供数据一级索引，但不是唯一约束</span><br><span class=\"line\"></span><br><span class=\"line\"># order by是必须的字段</span><br><span class=\"line\"># primary key，partition by不是必须的字段</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># partition by </span><br><span class=\"line\">分区目录</span><br><span class=\"line\">降低扫描范围，优化查询速度</span><br><span class=\"line\">MergeTree是以列文件+索引文件+表定义文件组成的,但是如果定了分区那么这些文件就会保存到不同的分区目录中。如果不指定，只会使用一个分区</span><br><span class=\"line\">分区后，面对涉及跨分区的查询统计，ClickHouse 会以分区为单位并行处理。 </span><br><span class=\"line\"></span><br><span class=\"line\">数据写入与分区合并 </span><br><span class=\"line\">任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入 后的某个时刻（大概 <span class=\"number\">10</span>-<span class=\"number\">15</span> 分钟后），ClickHouse 会自动执行合并操作（等不及也可以手动 通过 optimize 执行），把临时分区的数据，合并到已有分区中。 </span><br><span class=\"line\">optimize table xxxx <span class=\"keyword\">final</span>;</span><br></pre></td></tr></table></figure>\n<h5 id=\"clickhouse文件结构\"><a class=\"markdownIt-Anchor\" href=\"#clickhouse文件结构\">#</a> clickhouse 文件结构</h5>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table t_order_mt(  </span><br><span class=\"line\">    id UInt32,  </span><br><span class=\"line\">    sku_id String,  </span><br><span class=\"line\">    total_amount Decimal(16,2),  </span><br><span class=\"line\">    create_time Datetime </span><br><span class=\"line\">) engine =MergeTree  </span><br><span class=\"line\">    partition by toYYYYMMDD(create_time)  </span><br><span class=\"line\">    primary key (id)  </span><br><span class=\"line\">    order by (id,sku_id);</span><br><span class=\"line\">    </span><br><span class=\"line\">insert into t_order_mt values </span><br><span class=\"line\">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) , (102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;), (102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;), (102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;), (102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;), (102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br><span class=\"line\"></span><br><span class=\"line\">root@hadoop100:/var/lib/clickhouse/data/default/t_order_mt# ll</span><br><span class=\"line\">drwxr-x--- 2 clickhouse clickhouse 4096 Aug 20 18:19 20200601_1_1_0/</span><br><span class=\"line\">drwxr-x--- 2 clickhouse clickhouse 4096 Aug 20 18:19 20200601_1_1_1/</span><br><span class=\"line\">drwxr-x--- 2 clickhouse clickhouse 4096 Aug 20 18:19 20200602_2_2_0/</span><br><span class=\"line\">drwxr-x--- 2 clickhouse clickhouse 4096 Aug 20 18:19 20200602_2_2_1/</span><br><span class=\"line\">drwxr-x--- 2 clickhouse clickhouse 4096 Aug 20 18:18 detached/</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse    1 Aug 20 18:18 format_version.txt</span><br><span class=\"line\"></span><br><span class=\"line\">root@hadoop100:/var/lib/clickhouse/data/default/t_order_mt/20200602_2_2_1# ll</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse  333 Aug 20 18:19 checksums.txt</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse  118 Aug 20 18:19 columns.txt</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse    1 Aug 20 18:19 count.txt</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse  128 Aug 20 18:19 data.bin</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse   66 Aug 20 18:19 data.cmrk3</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse   10 Aug 20 18:19 default_compression_codec.txt</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse    1 Aug 20 18:19 metadata_version.txt</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse    8 Aug 20 18:19 minmax_create_time.idx</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse    4 Aug 20 18:19 partition.dat</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse   42 Aug 20 18:19 primary.cidx</span><br><span class=\"line\">-rw-r----- 1 clickhouse clickhouse  292 Aug 20 18:19 serialization.json</span><br></pre></td></tr></table></figure>\n<p>bin 文件：数据文件</p>\n<p>mrk 文件：标记文件</p>\n<p>标记文件在 idx 索引文件和 bin 数据文件之间起到了桥梁作用</p>\n<p>以 mrk2 结尾的文件，表示该表启用了自适应索引间隔。</p>\n<p>primary.idx 文件：主键索引文件，用于加快查询效率</p>\n<p>minmax_create_time.idx; 分区键的最大最小值</p>\n<p>checksums.txt: 校验文件，用于校验各个文件的正确性。右屏放各个文件的 size 以及 hash 值。</p>\n<p>20200602_2_2_1</p>\n<p>PartitionId     MinBlockNum    MaxBlockNum_Level</p>\n<p>分区值最小   分区块编号最大  分区块编号合并层级</p>\n<h5 id=\"primary-key\"><a class=\"markdownIt-Anchor\" href=\"#primary-key\">#</a> primary key</h5>\n<p>ClickHouse 中的主键，和其他数据库不太一样，** 它只提供了数据的一级<strong><strong>索引</strong></strong>，但是却不是唯一约束。** 这就意味着是可以存在相同 primary key 的数据的。</p>\n<p>主键的设定主要依据是查询语句中的 where 条件。 根据条件通过对主键进行某种形式的二分查找，能够定位到对应的 index granularity, 避</p>\n<p>免了全表扫描。</p>\n<p>index granularity： 索引粒度，指在稀疏索引中两个相邻索引对应数据的间隔。ClickHouse 中的 MergeTree 默认是 8192。官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据</p>\n<p>稀疏索引</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20250226013615511.png\" alt=\"image-20250226013615511\"></p>\n<p>稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索 引粒度的第一行，然后再进行进行一点扫描。</p>\n<h5 id=\"order-by\"><a class=\"markdownIt-Anchor\" href=\"#order-by\">#</a> order by</h5>\n<p>order by 设定了分区内的数据按照哪些字段顺序进行有序保存，MergeTree 中唯一必选项。因为当用户不 设置主键的情况，很多处理会依照 order by 的字段进行处理</p>\n<p>要求：主键必须是 order by 字段的前缀字段。</p>\n<p>比如 orderby 字段是 (id,sku_id) 那么主键必须是 id 或者 (id,sku_id)</p>\n<h5 id=\"二级索引\"><a class=\"markdownIt-Anchor\" href=\"#二级索引\">#</a> 二级索引</h5>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table t_order_mt2(  </span><br><span class=\"line\">    id UInt32,  </span><br><span class=\"line\">    sku_id String,  </span><br><span class=\"line\">    total_amount Decimal(16,2),  </span><br><span class=\"line\">    create_time Datetime, </span><br><span class=\"line\">    INDEX a total_amount TYPE minmax GRANULARITY 5 </span><br><span class=\"line\">) engine =MergeTree  </span><br><span class=\"line\">    partition by toYYYYMMDD(create_time)</span><br><span class=\"line\">    primary key (id)  </span><br><span class=\"line\">    order by (id, sku_id);</span><br></pre></td></tr></table></figure>\n<p>index a total_amount TYPE minmax GRAANULARITY 5</p>\n<p>GRANULARITY N 是设定二级索引对于一级索引粒度的粒度。</p>\n<p>a 别名</p>\n<p>total_amount 字段名</p>\n<p>minmax 索引类型</p>\n<p>GRABULARIRT 索引粒度</p>\n<p>二级索引能够为非主键字段的查询发挥作用</p>\n<h5 id=\"ttl\"><a class=\"markdownIt-Anchor\" href=\"#ttl\">#</a> TTL</h5>\n<p>TTL 即 Time To Live,MergeTree 提供了可以管理数据表或者列的生命周期的功能。</p>\n<p>列级别 ttl</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table <span class=\"title function_\">t_order_mt3</span><span class=\"params\">( </span></span><br><span class=\"line\"><span class=\"params\"> id UInt32, </span></span><br><span class=\"line\"><span class=\"params\"> sku_id String, </span></span><br><span class=\"line\"><span class=\"params\"> total_amount Decimal(<span class=\"number\">16</span>,<span class=\"number\">2</span>)</span> TTL create_time+interval <span class=\"number\">10</span> SECOND, </span><br><span class=\"line\"> create_time Datetime  </span><br><span class=\"line\">) engine =MergeTree </span><br><span class=\"line\">partition by <span class=\"title function_\">toYYYYMMDD</span><span class=\"params\">(create_time)</span> </span><br><span class=\"line\"> primary <span class=\"title function_\">key</span> <span class=\"params\">(id)</span> </span><br><span class=\"line\"> order <span class=\"title function_\">by</span> <span class=\"params\">(id, sku_id)</span>;</span><br></pre></td></tr></table></figure>\n<p>类型必须是日期类型，不能是主键</p>\n<p>时间到期之后自动启动合并任务</p>\n<p>表级别 ttl</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alter table t_order_mt3 MODIFY TTL create_time + INTERVAL <span class=\"number\">10</span> SECOND;</span><br></pre></td></tr></table></figure>\n<p>涉及判断的字段必须是 Date 或者 Datetime 类型，推荐使用分区的日期字段。</p>\n<p>能够使用的时间周期：</p>\n<ul>\n<li>SECOND</li>\n<li>MINUTE</li>\n<li>HOUR</li>\n<li>DAY</li>\n<li>WEEK</li>\n<li>MONTH</li>\n<li>QUARTER</li>\n<li>YEAR</li>\n</ul>\n<h5 id=\"ttl执行行为\"><a class=\"markdownIt-Anchor\" href=\"#ttl执行行为\">#</a> TTL 执行行为</h5>\n<p>Type of TTL rule may follow each TTL expression. It affects an action which is to be done once the expression is satisfied (reaches current time):</p>\n<ul>\n<li><strong> <code>DELETE</code> </strong> delete expired rows (default action);</li>\n<li><strong> <code>RECOMPRESS codec_name</code> </strong> recompress data part with the <strong> <code>codec_name</code> </strong>;</li>\n<li><strong> <code>TO DISK 'aaa'</code> </strong> move part to the disk <strong> <code>aaa</code> </strong>;</li>\n<li><strong> <code>TO VOLUME 'bbb'</code> </strong> move part to the disk <strong> <code>bbb</code> </strong>;</li>\n<li><strong> <code>GROUP BY</code> </strong> aggregate expired rows.</li>\n</ul>\n<h4 id=\"replacingmergetree\"><a class=\"markdownIt-Anchor\" href=\"#replacingmergetree\">#</a> ReplacingMergeTree</h4>\n<p>ReplacingMergeTree 是 MergeTree 的一个变种，它存储特性完全继承 MergeTree，只是多了一个去重的功能。 尽管 MergeTree 可以设置主键，但是 primary key 其实没有唯一约束的功能。</p>\n<p><strong>1）去重时机</strong></p>\n<p><strong>数据的去重只会在合并的过程中出现</strong>。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。也可以手动 OPTIMIZE TABLE</p>\n<p><strong>2）去重范围</strong></p>\n<p><strong>如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重。</strong></p>\n<p>所以 ReplacingMergeTree 能力有限， ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它<strong>不保证没有重复的数据出现</strong>。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table <span class=\"title function_\">t_order_rmt</span><span class=\"params\">( </span></span><br><span class=\"line\"><span class=\"params\">     id UInt32, </span></span><br><span class=\"line\"><span class=\"params\">     sku_id String, </span></span><br><span class=\"line\"><span class=\"params\">     total_amount Decimal(<span class=\"number\">16</span>,<span class=\"number\">2</span>)</span> , </span><br><span class=\"line\">     create_time Datetime  </span><br><span class=\"line\">) engine =ReplacingMergeTree(create_time) </span><br><span class=\"line\">     partition by <span class=\"title function_\">toYYYYMMDD</span><span class=\"params\">(create_time)</span> </span><br><span class=\"line\">     primary <span class=\"title function_\">key</span> <span class=\"params\">(id)</span> </span><br><span class=\"line\">     order <span class=\"title function_\">by</span> <span class=\"params\">(id, sku_id)</span>; </span><br></pre></td></tr></table></figure>\n<p>ReplacingMergeTree () 填入的参数为版本字段，重复数据保留版本字段值最大的。</p>\n<p>如果不填版本字段，默认按照插入顺序保留最后一条。</p>\n<p><strong>通过测试得到结论</strong></p>\n<p>➢ 实际上是使用 order by 字段作为唯一键</p>\n<p>➢ 去重不能跨分区</p>\n<p>➢ 只有同一批插入（新版本）或合并分区时才会进行去重</p>\n<p>➢ 认定重复的数据保留，版本字段值最大的</p>\n<p>➢ 如果版本字段相同则按插入顺序保留最后一条</p>\n<h4 id=\"summingmergetree\"><a class=\"markdownIt-Anchor\" href=\"#summingmergetree\">#</a> SummingMergeTree</h4>\n<p>对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的 MergeTree 的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。</p>\n<p>ClickHouse 为了这种场景，提供了一种能够 “预聚合” 的引擎 SummingMergeTree</p>\n<p>（1）创建表</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table <span class=\"title function_\">t_order_smt</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">     id UInt32,</span></span><br><span class=\"line\"><span class=\"params\">     sku_id String,</span></span><br><span class=\"line\"><span class=\"params\">     total_amount Decimal(<span class=\"number\">16</span>,<span class=\"number\">2</span>)</span> ,</span><br><span class=\"line\">     create_time Datetime  </span><br><span class=\"line\">) engine =SummingMergeTree(total_amount)</span><br><span class=\"line\">     partition by <span class=\"title function_\">toYYYYMMDD</span><span class=\"params\">(create_time)</span></span><br><span class=\"line\">     primary <span class=\"title function_\">key</span> <span class=\"params\">(id)</span></span><br><span class=\"line\">     order <span class=\"title function_\">by</span> <span class=\"params\">(id,sku_id )</span>;</span><br></pre></td></tr></table></figure>\n<p>根据 order by 进行预聚合，聚合时保留最早的一条</p>\n<p>➢ 以 SummingMergeTree（）中指定的列作为汇总数据列</p>\n<p>➢ 可以填写多列必须数字列，如果不填，以所有非维度列且为数字列的字段为汇总数据列</p>\n<p>➢ 以 order by 的列为准，作为维度列</p>\n<p>➢ 其他的列按插入顺序保留第一行</p>\n<p>➢ 不在一个分区的数据不会被聚合</p>\n<p>➢ 只有在同一批次插入 (新版本) 或分片合并时才会进行聚合</p>\n<p>如果要是获取汇总值，还是需要使用 sum 进行聚合，这样效率会有一定的提高，但本身 ClickHouse 是列式存储的，效率提升有限，不会特别明显。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select sum(total_amount) from province_name=’’ and create_date=‘xxx’</span><br></pre></td></tr></table></figure>\n<h3 id=\"sql操作\"><a class=\"markdownIt-Anchor\" href=\"#sql操作\">#</a> SQL 操作</h3>\n<h4 id=\"insert\"><a class=\"markdownIt-Anchor\" href=\"#insert\">#</a> insert</h4>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（1）标准 </span><br><span class=\"line\">insert into [table_name] values(…),(….) </span><br><span class=\"line\">（2）从表到表的插入 </span><br><span class=\"line\">insert into [table_name] select a,b,c from [table_name_2]</span><br></pre></td></tr></table></figure>\n<h4 id=\"update-delete\"><a class=\"markdownIt-Anchor\" href=\"#update-delete\">#</a> update &amp; delete</h4>\n<p>ClickHouse 提供了 Delete 和 Update 的能力，这类操作被称为 Mutation 查询，它可以看做 Alter 的一种。 虽然可以实现修改和删除，但是和一般的 OLTP 数据库不一样，<strong>Mutation 语句是一种很 “重” 的操作，而且不支持<strong><strong>事务</strong></strong>。</strong></p>\n<p>“重” 的原因主要是 ** 每次修改或者删除都会导致放弃目标数据的原有分区，重建新分区。** 所以尽量做批量的变更，不要进行频繁小数据的操作。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（1）删除操作 </span><br><span class=\"line\">alter table t_order_smt delete where sku_id =&#x27;sku_001&#x27;; </span><br><span class=\"line\">（2）修改操作 </span><br><span class=\"line\">alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id =102;</span><br></pre></td></tr></table></figure>\n<p>Mutation 语句分两步执行，同步执行的部分其实只是进行 新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删 除旧数据释放磁盘空间</p>\n<blockquote>\n<p>更新:</p>\n<p>插入一条新的数据，_version + 1</p>\n<p>查询的时候加上一个过滤条件  where version 最大</p>\n<p>删除:</p>\n<p>sign,0 表示未删除，1 表示已删除</p>\n<p>查询的时候加上一个过滤条件，where_sign=0 and version = 最大</p>\n</blockquote>\n<h4 id=\"查询\"><a class=\"markdownIt-Anchor\" href=\"#查询\">#</a> 查询</h4>\n<ul>\n<li>支持子查询</li>\n<li>支持 CTE (Common Table Expression 公用表表达式 with 子句)</li>\n<li>支持各种 JOIN, 但是 JOIN 操作无法使用缓存，所以即使是两次相同的 JOIN 语句，ClickHouse 也会视为两条新 SQL</li>\n<li>窗口函数  <span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGlja2hvdXNlLmNvbS9kb2NzL2VuL3NxbC1yZWZlcmVuY2Uvd2luZG93LWZ1bmN0aW9ucw==\">https://clickhouse.com/docs/en/sql-reference/window-functions</span></li>\n<li>不支持自定义函数</li>\n<li>GROUP BY 操作增加了 with rollup\\with cube\\with total 用来计算小计和总计。</li>\n</ul>\n<p>with rollup/with cube / with total</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select a, b from t_order_mt group by x  with rollup;</span><br></pre></td></tr></table></figure>\n<p>纬度是 a,b</p>\n<p>rollup: 上卷</p>\n<p>group by a</p>\n<p>group by a,b</p>\n<p>group by</p>\n<p>cube: 多维分析</p>\n<p>group by a</p>\n<p>group by b</p>\n<p>group by a,b</p>\n<p>group by</p>\n<p>total: 总计</p>\n<p>group by a,b</p>\n<p>group by</p>\n<h4 id=\"alert\"><a class=\"markdownIt-Anchor\" href=\"#alert\">#</a> alert</h4>\n<p><strong>1）新增字段</strong></p>\n<p>alter table <strong>tableName</strong> add column <strong>newcolname</strong> String after col1;</p>\n<p><strong>2）修改字段类型</strong></p>\n<p>alter table tableName modify column newcolname String;</p>\n<p><strong>3）删除字段</strong></p>\n<p>alter table tableName drop column newcolname;</p>\n<h4 id=\"导出\"><a class=\"markdownIt-Anchor\" href=\"#导出\">#</a> 导出</h4>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">clickhouse-client --query &quot;select * from t_order_mt where  create_time=&#x27;2020-06-01 12:00:00&#x27;&quot; --format CSVWithNames&gt;  /opt/module/data/rs1.csv</span><br></pre></td></tr></table></figure>\n<h3 id=\"副本\"><a class=\"markdownIt-Anchor\" href=\"#副本\">#</a> 副本</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20250226013630641.png\" alt=\"image-20250226013630641\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 配置zk</span><br><span class=\"line\">vim /etc/clickhouse-server/config.d/metrika.xml</span><br><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span><br><span class=\"line\">&lt;yandex&gt;</span><br><span class=\"line\">    &lt;zookeeper-servers&gt;</span><br><span class=\"line\">        &lt;node index=<span class=\"string\">&quot;1&quot;</span>&gt;</span><br><span class=\"line\">            &lt;host&gt;hadoop102&lt;/host&gt;</span><br><span class=\"line\">            &lt;port&gt;<span class=\"number\">2181</span>&lt;/port&gt;</span><br><span class=\"line\">        &lt;/node&gt;</span><br><span class=\"line\">        &lt;node index=<span class=\"string\">&quot;2&quot;</span>&gt;</span><br><span class=\"line\">            &lt;host&gt;hadoop103&lt;/host&gt;</span><br><span class=\"line\">            &lt;port&gt;<span class=\"number\">2181</span>&lt;/port&gt;</span><br><span class=\"line\">        &lt;/node&gt;</span><br><span class=\"line\">        &lt;node index=<span class=\"string\">&quot;3&quot;</span>&gt;</span><br><span class=\"line\">            &lt;host&gt;hadoop104&lt;/host&gt;</span><br><span class=\"line\">            &lt;port&gt;<span class=\"number\">2181</span>&lt;/port&gt;</span><br><span class=\"line\">        &lt;/node&gt;</span><br><span class=\"line\">    &lt;/zookeeper-servers&gt;</span><br><span class=\"line\">&lt;/yandex&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">chown clickhouse:clickhouse /etc/clickhouse-server/config.d/metrika.xml</span><br><span class=\"line\"></span><br><span class=\"line\">vim config.xml</span><br><span class=\"line\">&lt;zookeeper incl=<span class=\"string\">&quot;zookeeper-servers&quot;</span> optional=<span class=\"string\">&quot;true&quot;</span> /&gt; </span><br><span class=\"line\">&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">clickhouse restart</span><br></pre></td></tr></table></figure>\n<p>副本只能同步数据，不能同步表结构。需要手工建表</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hadoop100</span><br><span class=\"line\">create table t_order_rep2 (  </span><br><span class=\"line\">    id UInt32,  </span><br><span class=\"line\">    sku_id String,  </span><br><span class=\"line\">    total_amount Decimal(16,2),  </span><br><span class=\"line\">    create_time Datetime </span><br><span class=\"line\">) engine =ReplicatedMergeTree(&#x27;/clickhouse/table/01/t_order_rep&#x27;,&#x27;rep_100&#x27;)  </span><br><span class=\"line\">    partition by toYYYYMMDD(create_time)  </span><br><span class=\"line\">    primary key (id)  </span><br><span class=\"line\">    order by (id,sku_id);</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">Hadoop101：</span><br><span class=\"line\">create table t_order_rep2 (  </span><br><span class=\"line\">    id UInt32,  </span><br><span class=\"line\">    sku_id String,  </span><br><span class=\"line\">    total_amount Decimal(16,2),  </span><br><span class=\"line\">    create_time Datetime </span><br><span class=\"line\">) engine =ReplicatedMergeTree(&#x27;/clickhouse/table/01/t_order_rep&#x27;,&#x27;rep_101&#x27;)  </span><br><span class=\"line\">    partition by toYYYYMMDD(create_time)  </span><br><span class=\"line\">    primary key (id)  </span><br><span class=\"line\">    order by (id,sku_id);</span><br><span class=\"line\">    </span><br><span class=\"line\">   </span><br><span class=\"line\">第一个参数是分片的 zk_path 一般按照：/clickhouse/table/&#123;shard&#125;/&#123;table_name&#125; 的格式,如果只有一个分片写01即可</span><br><span class=\"line\">第二个参数数副本名称，相同的分片副本名称不能相同。</span><br></pre></td></tr></table></figure>\n<h3 id=\"分片集群\"><a class=\"markdownIt-Anchor\" href=\"#分片集群\">#</a> 分片集群</h3>\n<p>副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。</p>\n<p>要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过 Distributed 表引擎把数据拼接起来一同使用。</p>\n<p>Distributed 表引擎本身不存储数据，有点类似于 MyCat 之于 MySql，成为一种中间件， 通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20250226013643694.png\" alt=\"image-20250226013643694\"></p>\n<p>一般开启 internal_replication ，即使用分片同步副本数据。</p>\n<p>ClickHouse 的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20250226013652131.png\" alt=\"image-20250226013652131\"></p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim metrika-shard.<span class=\"property\">xml</span></span><br><span class=\"line\">&lt;yandex&gt;</span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">remote_servers</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;<span class=\"name\">gmall_cluster</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"comment\">&lt;!-- 集群名称--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"comment\">&lt;!--集群的第一个分片--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"comment\">&lt;!--该分片的第一个副本--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>hadoop101<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"comment\">&lt;!--该分片的第二个副本--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>hadoop102<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"comment\">&lt;!--集群的第二个分片--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"comment\">&lt;!--该分片的第一个副本--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>hadoop103<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"comment\">&lt;!--该分片的第二个副本--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>hadoop104<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"comment\">&lt;!--集群的第三个分片--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"comment\">&lt;!--该分片的第一个副本--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>hadoop105<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"comment\">&lt;!--该分片的第二个副本--&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>hadoop106<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                    <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">gmall_cluster</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">remote_servers</span>&gt;</span></span></span><br><span class=\"line\">&lt;/yandex&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">macros</span>&gt;</span> </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span>01<span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span> <span class=\"comment\">&lt;!--不同机器放的分片数不一样--&gt;</span> </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span>rep_1_1<span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span> <span class=\"comment\">&lt;!--不同机器放的副本数不一样--&gt;</span> </span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">macros</span>&gt;</span></span> </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&lt;include_from&gt; <span class=\"regexp\">/xxxx/m</span>etrika-shard.<span class=\"property\">xml</span></span><br><span class=\"line\"># 建表</span><br><span class=\"line\">create table st_order_mt on cluster gmall_cluster ( </span><br><span class=\"line\">     id <span class=\"title class_\">UInt32</span>, </span><br><span class=\"line\">     sku_id <span class=\"title class_\">String</span>, </span><br><span class=\"line\">     total_amount <span class=\"title class_\">Decimal</span>(<span class=\"number\">16</span>,<span class=\"number\">2</span>), </span><br><span class=\"line\">     create_time <span class=\"title class_\">Datetime</span> </span><br><span class=\"line\">) engine=<span class=\"title class_\">ReplicatedMergeTree</span>(<span class=\"string\">&#x27;/clickhouse/tables/&#123;shard&#125;/st_order_mt&#x27;</span>,<span class=\"string\">&#x27;&#123;replica&#125;&#x27;</span>) </span><br><span class=\"line\">     partition by <span class=\"title function_\">toYYYYMMDD</span>(create_time) </span><br><span class=\"line\">     primary key (id) </span><br><span class=\"line\">     order by (id,sku_id);</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 创建分布式表</span><br><span class=\"line\"> create table st_order_mt_all2 on cluster gmall_cluster </span><br><span class=\"line\">( </span><br><span class=\"line\"> sku_id <span class=\"title class_\">String</span>, </span><br><span class=\"line\"> total_amount <span class=\"title class_\">Decimal</span>(<span class=\"number\">16</span>,<span class=\"number\">2</span>), </span><br><span class=\"line\"> create_time <span class=\"title class_\">Datetime</span> </span><br><span class=\"line\">)engine = <span class=\"title class_\">Distributed</span>(gmall_cluster,<span class=\"keyword\">default</span>, st_order_mt,<span class=\"title function_\">hiveHash</span>(sku_id)); </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title class_\">Distributed</span>（集群名称，库名，本地表名，分片键） </span><br><span class=\"line\">分片键必须是整型数字，所以用 hiveHash 函数转换，也可以 <span class=\"title function_\">rand</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"># 往分布式表插入和查询数据</span><br><span class=\"line\">insert into st_order_mt_all2 values (<span class=\"number\">201</span>,<span class=\"string\">&#x27;sku_001&#x27;</span>,<span class=\"number\">1000.00</span>,<span class=\"string\">&#x27;2020-06-01 12:00:00&#x27;</span>) , (<span class=\"number\">202</span>,<span class=\"string\">&#x27;sku_002&#x27;</span>,<span class=\"number\">2000.00</span>,<span class=\"string\">&#x27;2020-06-01 12:00:00&#x27;</span>), (<span class=\"number\">203</span>,<span class=\"string\">&#x27;sku_004&#x27;</span>,<span class=\"number\">2500.00</span>,<span class=\"string\">&#x27;2020-06-01 12:00:00&#x27;</span>), (<span class=\"number\">204</span>,<span class=\"string\">&#x27;sku_002&#x27;</span>,<span class=\"number\">2000.00</span>,<span class=\"string\">&#x27;2020-06-01 12:00:00&#x27;</span>), (<span class=\"number\">205</span>,<span class=\"string\">&#x27;sku_003&#x27;</span>,<span class=\"number\">600.00</span>,<span class=\"string\">&#x27;2020-06-02 12:00:00&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<h3 id=\"explain\"><a class=\"markdownIt-Anchor\" href=\"#explain\">#</a> explain</h3>\n<p>查看 sql 语句执行计划</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN [AST | SYNTAX | PLAN | PIPELINE] [setting = value, ...] SELECT ... [FORMAT ...]</span><br><span class=\"line\">PLAN：用于查看执行计划，默认值。</span><br><span class=\"line\">AST ：用于查看语法树; </span><br><span class=\"line\">SYNTAX：用于优化语法; </span><br><span class=\"line\">PIPELINE：用于查看 PIPELINE 计划</span><br><span class=\"line\">EXPLAIN plan select arrayJoin([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,null,null]);</span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN</span><br><span class=\"line\">SELECT arrayJoin([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, NULL, NULL])</span><br><span class=\"line\"></span><br><span class=\"line\">Query <span class=\"built_in\">id</span>: 4d263f99-155f-<span class=\"number\">4158</span>-b611-115a64acc2eb</span><br><span class=\"line\"></span><br><span class=\"line\">   ┌─explain─────────────────────────────────────────────────────────────────────────────────┐</span><br><span class=\"line\"><span class=\"number\">1.</span> │ Expression ((Project names + (Projection + Change column names to column identifiers))) │</span><br><span class=\"line\"><span class=\"number\">2.</span> │   ReadFromStorage (SystemOne)                                                           │</span><br><span class=\"line\">   └─────────────────────────────────────────────────────────────────────────────────────────┘</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">2</span> rows <span class=\"keyword\">in</span> <span class=\"built_in\">set</span>. Elapsed: <span class=\"number\">0.001</span> sec. </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">explain select database,table,count(<span class=\"number\">1</span>) cnt <span class=\"keyword\">from</span> system.parts where database <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;datasets&#x27;</span>,<span class=\"string\">&#x27;system&#x27;</span>) group by database,table order by</span><br><span class=\"line\">database,cnt desc limit <span class=\"number\">2</span> by database;</span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN</span><br><span class=\"line\">SELECT</span><br><span class=\"line\">    database,</span><br><span class=\"line\">    `table`,</span><br><span class=\"line\">    count(<span class=\"number\">1</span>) AS cnt</span><br><span class=\"line\">FROM system.parts</span><br><span class=\"line\">WHERE database IN (<span class=\"string\">&#x27;datasets&#x27;</span>, <span class=\"string\">&#x27;system&#x27;</span>)</span><br><span class=\"line\">GROUP BY</span><br><span class=\"line\">    database,</span><br><span class=\"line\">    `table`</span><br><span class=\"line\">ORDER BY</span><br><span class=\"line\">    database ASC,</span><br><span class=\"line\">    cnt DESC</span><br><span class=\"line\">LIMIT <span class=\"number\">2</span> BY database</span><br><span class=\"line\"></span><br><span class=\"line\">Query <span class=\"built_in\">id</span>: 4f3d43d3-4f79-<span class=\"number\">4748</span>-a774-4cf6fd44adce</span><br><span class=\"line\"></span><br><span class=\"line\">   ┌─explain────────────────────────────────────────────────────────────────────┐</span><br><span class=\"line\"><span class=\"number\">1.</span> │ Expression (Project names)                                                 │</span><br><span class=\"line\"><span class=\"number\">2.</span> │   LimitBy                                                                  │</span><br><span class=\"line\"><span class=\"number\">3.</span> │     Expression (Before LIMIT BY)                                           │</span><br><span class=\"line\"><span class=\"number\">4.</span> │       Sorting (Sorting <span class=\"keyword\">for</span> ORDER BY)                                       │</span><br><span class=\"line\"><span class=\"number\">5.</span> │         Expression ((Before ORDER BY + Projection))                        │</span><br><span class=\"line\"><span class=\"number\">6.</span> │           Aggregating                                                      │</span><br><span class=\"line\"><span class=\"number\">7.</span> │             Expression (Before GROUP BY)                                   │</span><br><span class=\"line\"><span class=\"number\">8.</span> │               Filter ((WHERE + Change column names to column identifiers)) │</span><br><span class=\"line\"><span class=\"number\">9.</span> │                 ReadFromSystemPartsBase                                    │</span><br><span class=\"line\">   └────────────────────────────────────────────────────────────────────────────┘</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">9</span> rows <span class=\"keyword\">in</span> <span class=\"built_in\">set</span>. Elapsed: <span class=\"number\">0.002</span> sec. </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN AST SELECT number <span class=\"keyword\">from</span> system.numbers limit <span class=\"number\">10</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN AST</span><br><span class=\"line\">SELECT number</span><br><span class=\"line\">FROM system.numbers</span><br><span class=\"line\">LIMIT <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">Query <span class=\"built_in\">id</span>: ef9768e7-41ab-437d-<span class=\"number\">9707</span>-2e85703e46ef</span><br><span class=\"line\"></span><br><span class=\"line\">    ┌─explain─────────────────────────────────────┐</span><br><span class=\"line\"> <span class=\"number\">1.</span> │ SelectWithUnionQuery (children <span class=\"number\">1</span>)           │</span><br><span class=\"line\"> <span class=\"number\">2.</span> │  ExpressionList (children <span class=\"number\">1</span>)                │</span><br><span class=\"line\"> <span class=\"number\">3.</span> │   SelectQuery (children <span class=\"number\">3</span>)                  │</span><br><span class=\"line\"> <span class=\"number\">4.</span> │    ExpressionList (children <span class=\"number\">1</span>)              │</span><br><span class=\"line\"> <span class=\"number\">5.</span> │     Identifier number                       │</span><br><span class=\"line\"> <span class=\"number\">6.</span> │    TablesInSelectQuery (children <span class=\"number\">1</span>)         │</span><br><span class=\"line\"> <span class=\"number\">7.</span> │     TablesInSelectQueryElement (children <span class=\"number\">1</span>) │</span><br><span class=\"line\"> <span class=\"number\">8.</span> │      TableExpression (children <span class=\"number\">1</span>)           │</span><br><span class=\"line\"> <span class=\"number\">9.</span> │       TableIdentifier system.numbers        │</span><br><span class=\"line\"><span class=\"number\">10.</span> │    <span class=\"type\">Literal</span> UInt64_10                        │</span><br><span class=\"line\">    └─────────────────────────────────────────────┘</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">10</span> rows <span class=\"keyword\">in</span> <span class=\"built_in\">set</span>. Elapsed: <span class=\"number\">0.001</span> sec. </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">打开全部参数</span><br><span class=\"line\">EXPLAIN header=<span class=\"number\">1</span>, actions=<span class=\"number\">1</span>,description=<span class=\"number\">1</span> SELECT number <span class=\"keyword\">from</span> system.numbers limit <span class=\"number\">10</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">//开启三元运算符优化 </span><br><span class=\"line\">SET optimize_if_chain_to_multiif = <span class=\"number\">1</span>; </span><br><span class=\"line\">//再次查看语法优化 </span><br><span class=\"line\">EXPLAIN SYNTAX SELECT number = <span class=\"number\">1</span> ? <span class=\"string\">&#x27;hello&#x27;</span> : (number = <span class=\"number\">2</span> ? <span class=\"string\">&#x27;world&#x27;</span> : <span class=\"string\">&#x27;atguigu&#x27;</span>) FROM numbers(<span class=\"number\">10</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN PIPELINE</span><br><span class=\"line\">SELECT</span><br><span class=\"line\">    database,</span><br><span class=\"line\">    `table`,</span><br><span class=\"line\">    count(<span class=\"number\">1</span>) AS cnt</span><br><span class=\"line\">FROM system.parts</span><br><span class=\"line\">WHERE database IN (<span class=\"string\">&#x27;datasets&#x27;</span>, <span class=\"string\">&#x27;system&#x27;</span>)</span><br><span class=\"line\">GROUP BY</span><br><span class=\"line\">    database,</span><br><span class=\"line\">    `table`</span><br><span class=\"line\">ORDER BY</span><br><span class=\"line\">    database ASC,</span><br><span class=\"line\">    cnt DESC</span><br><span class=\"line\">LIMIT <span class=\"number\">2</span> BY database</span><br><span class=\"line\"></span><br><span class=\"line\">Query <span class=\"built_in\">id</span>: 58b9bb5b-0abc-47de-<span class=\"number\">8322</span>-c1af458e3e54</span><br><span class=\"line\"></span><br><span class=\"line\">    ┌─explain───────────────────────────────────────────┐</span><br><span class=\"line\"> <span class=\"number\">1.</span> │ (Expression)                                      │</span><br><span class=\"line\"> <span class=\"number\">2.</span> │ ExpressionTransform                               │</span><br><span class=\"line\"> <span class=\"number\">3.</span> │   (LimitBy)                                       │</span><br><span class=\"line\"> <span class=\"number\">4.</span> │   LimitByTransform                                │</span><br><span class=\"line\"> <span class=\"number\">5.</span> │     (Expression)                                  │</span><br><span class=\"line\"> <span class=\"number\">6.</span> │     ExpressionTransform                           │</span><br><span class=\"line\"> <span class=\"number\">7.</span> │       (Sorting)                                   │</span><br><span class=\"line\"> <span class=\"number\">8.</span> │       MergingSortedTransform <span class=\"number\">4</span> → <span class=\"number\">1</span>                │</span><br><span class=\"line\"> <span class=\"number\">9.</span> │         MergeSortingTransform × <span class=\"number\">4</span>                 │</span><br><span class=\"line\"><span class=\"number\">10.</span> │           LimitsCheckingTransform × <span class=\"number\">4</span>             │</span><br><span class=\"line\"><span class=\"number\">11.</span> │             PartialSortingTransform × <span class=\"number\">4</span>           │</span><br><span class=\"line\"><span class=\"number\">12.</span> │               (Expression)                        │</span><br><span class=\"line\"><span class=\"number\">13.</span> │               ExpressionTransform × <span class=\"number\">4</span>             │</span><br><span class=\"line\"><span class=\"number\">14.</span> │                 (Aggregating)                     │</span><br><span class=\"line\"><span class=\"number\">15.</span> │                 Resize <span class=\"number\">1</span> → <span class=\"number\">4</span>                      │</span><br><span class=\"line\"><span class=\"number\">16.</span> │                   AggregatingTransform            │</span><br><span class=\"line\"><span class=\"number\">17.</span> │                     (Expression)                  │</span><br><span class=\"line\"><span class=\"number\">18.</span> │                     ExpressionTransform           │</span><br><span class=\"line\"><span class=\"number\">19.</span> │                       (Filter)                    │</span><br><span class=\"line\"><span class=\"number\">20.</span> │                       FilterTransform             │</span><br><span class=\"line\"><span class=\"number\">21.</span> │                         (ReadFromSystemPartsBase) │</span><br><span class=\"line\">    └───────────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure>\n<h3 id=\"建表优化\"><a class=\"markdownIt-Anchor\" href=\"#建表优化\">#</a> 建表优化</h3>\n<h4 id=\"数据类型-2\"><a class=\"markdownIt-Anchor\" href=\"#数据类型-2\">#</a> 数据类型</h4>\n<p>建表时能用数值型或日期时间型表示的字段就不要用字符串，全 String 类型在以 Hive 为中心的数仓建设中常见，但 ClickHouse 环境不应受此影响。 虽然 ClickHouse 底层将 DateTime 存储为时间戳 Long 类型，但不建议存储 Long 类型， 因为 DateTime 不需要经过函数转换处理，执行效率高、可读性好。</p>\n<p>官方已经指出 Nullable 类型几乎总是会拖累性能，因为存储 Nullable 列时需要创建一个 额外的文件来存储 NULL 的标记，并且 Nullable 列无法被索引。因此除非极特殊情况，应直 接使用字段默认值表示空，或者自行指定一个在业务中无意义的值（例如用 - 1 表示没有商品 ID）。</p>\n<p>存在 null 的时候会多存一列 null.bin</p>\n<p>null 无法被索引，效率</p>\n<h4 id=\"分区和索引\"><a class=\"markdownIt-Anchor\" href=\"#分区和索引\">#</a> 分区和索引</h4>\n<p>分区粒度根据业务特点决定，不宜过粗或过细。一般选择按天分区，也可以指定为 Tuple ()， 以单表一亿数据为例，分区大小控制在 10-30 个为最佳。</p>\n<p>必须指定索引列，ClickHouse 中的索引列即排序列，通过 order by 指定，一般在查询条件中经常被用来充当筛选条件的属性被纳入进来；可以是单一维度，也可以是组合维度的索 引；通常需要满足高级列在前、查询频率大的在前原则；还有基数特别大的不适合做索引列， 如用户表的 userid 字段；通常筛选后的数据满足在百万以内为最佳。</p>\n<h4 id=\"表参数\"><a class=\"markdownIt-Anchor\" href=\"#表参数\">#</a> 表参数</h4>\n<p>Index_granularity 是用来控制索引粒度的，默认是 8192，如非必须不建议调整。</p>\n<p>如果表中不是必须保留全量历史数据，建议指定 TTL（生存时间值），可以免去手动过期历史数据的麻烦，TTL 也可以通过 alter table 语句随时修改。</p>\n<h4 id=\"写入和删除优化\"><a class=\"markdownIt-Anchor\" href=\"#写入和删除优化\">#</a> 写入和删除优化</h4>\n<p>（1）尽量不要执行单条或小批量删除和插入操作，这样会产生小分区文件，给后台 Merge 任务带来巨大压力</p>\n<p>（2）不要一次写入太多分区，或数据写入太快，数据写入太快会导致 Merge 速度跟不上而报错，一般建议每秒钟发起 2-3 次写入操作，每次操作写入 2w~5w 条数据（依服务器 性能而定）</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">“ Too many parts 处理 ” ：使用 WAL 预写日志，提高写入性能。 </span><br><span class=\"line\">in_memory_parts_enable_wal 默认为 true</span><br><span class=\"line\"></span><br><span class=\"line\">在服务器内存充裕的情况下增加内存配额，一般通过 max_memory_usage 来实现 </span><br><span class=\"line\">在服务器内存不充裕的情况下，建议将超出部分内容分配到系统硬盘上，但会降低执行 速度，一般通过max_bytes_before_external_group_by、max_bytes_before_external_sort 参数来实现。</span><br></pre></td></tr></table></figure>\n<h4 id=\"配置优化\"><a class=\"markdownIt-Anchor\" href=\"#配置优化\">#</a> 配置优化</h4>\n<p>配置项主要在 config.xml 或 users.xml 中， 基本上都在 users.xml 里</p>\n<p>config.xml 的配置项</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGlja2hvdXNlLnRlY2gvZG9jcy9lbi9vcGVyYXRpb25zL3NlcnZlci1jb25maWd1cmF0aW9uLXBhcmFtZXRlcnMvc2V0dGluZ3Mv\">https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/</span></p>\n<p>users.xml 的配置项</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGlja2hvdXNlLnRlY2gvZG9jcy9lbi9vcGVyYXRpb25zL3NldHRpbmdzL3NldHRpbmdzLw==\">https://clickhouse.tech/docs/en/operations/settings/settings/</span></p>\n<h3 id=\"语法优化\"><a class=\"markdownIt-Anchor\" href=\"#语法优化\">#</a> 语法优化</h3>\n<p>ClickHouse 的 SQL 优化规则是基于 RBO (Rule Based Optimization)</p>\n<h4 id=\"count-优化\"><a class=\"markdownIt-Anchor\" href=\"#count-优化\">#</a> count 优化</h4>\n<p>在调用 count 函数时，如果使用的是 count () 或者 count (*)，且没有 where 条件，则 会直接使用 system.tables 的 total_rows</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN SELECT count()FROM datasets.hits_v1; </span><br><span class=\"line\"></span><br><span class=\"line\">Union  Expression (Projection)  Expression (Before ORDER BY and SELECT)  MergingAggregated  ReadNothing (Optimized trivial count)</span><br></pre></td></tr></table></figure>\n<p>Optimized trivial count ，这是对 count 的优化。 如果 count 具体的列字段，则不会使用此项优化：</p>\n<h4 id=\"消除子查询重复字段\"><a class=\"markdownIt-Anchor\" href=\"#消除子查询重复字段\">#</a> 消除子查询重复字段</h4>\n<p>子查询中有两个重复的 id 字段，会被去重</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN SYNTAX SELECT  a.UserID,  b.VisitID,  a.URL,  b.UserID  FROM  hits_v1 AS a  LEFT JOIN (  SELECT  UserID,  UserID as HaHa,  VisitID  FROM visits_v1) AS b  USING (UserID)  limit 3; </span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化语句： SELECT  UserID,  VisitID,  URL,  b.UserID FROM hits_v1 AS a ALL LEFT JOIN (  SELECT  UserID,  VisitID  FROM visits_v1 ) AS b USING (UserID) LIMIT 3</span><br></pre></td></tr></table></figure>\n<h4 id=\"谓词下推\"><a class=\"markdownIt-Anchor\" href=\"#谓词下推\">#</a> 谓词下推</h4>\n<p>当 group by 有 having 子句，但是没有 with cube、with rollup 或者 with totals 修饰的时 候，having 过滤会下推到 where 提前过滤。例如下面的查询，HAVING name 变成了 WHERE name，在 group by 之前过滤：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN SYNTAX SELECT UserID FROM hits_v1 GROUP BY UserID HAVING UserID = <span class=\"string\">&#x27;8585742290196126178&#x27;</span>; </span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化语句 SELECT UserID FROM hits_v1 WHERE UserID = \\<span class=\"string\">&#x27;8585742290196126178\\&#x27; GROUP BY UserID</span></span><br><span class=\"line\"><span class=\"string\">EXPLAIN SYNTAX SELECT * FROM (  SELECT UserID  FROM visits_v1 ) WHERE UserID = &#x27;</span><span class=\"number\">8585742290196126178</span><span class=\"string\">&#x27; </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">//返回优化后的语句 </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">SELECT UserID FROM (  SELECT UserID  FROM visits_v1  WHERE UserID = \\&#x27;8585742290196126178\\&#x27; ) WHERE UserID = \\&#x27;8585742290196126178\\&#x27;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"聚合计算外推\"><a class=\"markdownIt-Anchor\" href=\"#聚合计算外推\">#</a> 聚合计算外推</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">聚合函数内的计算，会外推，例如： </span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN SYNTAX SELECT <span class=\"built_in\">sum</span>(UserID * <span class=\"number\">2</span>) FROM visits_v1 </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化后的语句 </span><br><span class=\"line\">SELECT <span class=\"built_in\">sum</span>(UserID) * <span class=\"number\">2</span> FROM visits_v1</span><br></pre></td></tr></table></figure>\n<h4 id=\"聚合函数消除\"><a class=\"markdownIt-Anchor\" href=\"#聚合函数消除\">#</a> 聚合函数消除</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果对聚合键，也就是 group by key 使用 <span class=\"built_in\">min</span>、<span class=\"built_in\">max</span>、<span class=\"built_in\">any</span> 聚合函数，则将函数消除， 例如： </span><br><span class=\"line\">EXPLAIN SYNTAX SELECT  <span class=\"built_in\">sum</span>(UserID * <span class=\"number\">2</span>),  <span class=\"built_in\">max</span>(VisitID),  <span class=\"built_in\">max</span>(UserID) FROM visits_v1 GROUP BY UserID</span><br><span class=\"line\"></span><br><span class=\"line\">SELECT  <span class=\"built_in\">sum</span>(UserID) * <span class=\"number\">2</span>,  <span class=\"built_in\">max</span>(VisitID),  UserID FROM visits_v1 GROUP BY UserID</span><br></pre></td></tr></table></figure>\n<h4 id=\"删除重复order-by-key\"><a class=\"markdownIt-Anchor\" href=\"#删除重复order-by-key\">#</a> 删除重复 order by key</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN SYNTAX SELECT * FROM visits_v1 ORDER BY  UserID ASC,  UserID ASC,  VisitID ASC, VisitID ASC </span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化后的语句： </span><br><span class=\"line\"></span><br><span class=\"line\">select …… FROM visits_v1 ORDER BY  UserID ASC, VisitID ASC</span><br></pre></td></tr></table></figure>\n<h4 id=\"删除重复limit-by-key\"><a class=\"markdownIt-Anchor\" href=\"#删除重复limit-by-key\">#</a> 删除重复 limit by key</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN SYNTAX SELECT * FROM visits_v1 LIMIT <span class=\"number\">3</span> BY  VisitID,  VisitID LIMIT <span class=\"number\">10</span> </span><br><span class=\"line\">//返回优化后的语句： </span><br><span class=\"line\">select …… FROM visits_v1 LIMIT <span class=\"number\">3</span> BY VisitID LIMIT <span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"删除重复using-key\"><a class=\"markdownIt-Anchor\" href=\"#删除重复using-key\">#</a> 删除重复 using key</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN SYNTAX SELECT  a.UserID,  a.UserID,  b.VisitID,  a.URL,  b.UserID  FROM hits_v1 AS a LEFT JOIN visits_v1 AS b USING (UserID, UserID) </span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化后的语句：</span><br><span class=\"line\"></span><br><span class=\"line\"> SELECT  UserID,  UserID,  VisitID,  URL,  b.UserID FROM hits_v1 AS a ALL LEFT JOIN visits_v1 AS b USING (UserID)</span><br></pre></td></tr></table></figure>\n<h4 id=\"标量替换\"><a class=\"markdownIt-Anchor\" href=\"#标量替换\">#</a> 标量替换</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果子查询只返回一行数据，在被引用的时候用标量替换，例如下面语句中的 total_disk_usage 字段： </span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN SYNTAX WITH  (  SELECT <span class=\"built_in\">sum</span>(<span class=\"built_in\">bytes</span>)  FROM system.parts  WHERE active  ) AS total_disk_usage SELECT  (<span class=\"built_in\">sum</span>(<span class=\"built_in\">bytes</span>) / total_disk_usage) * <span class=\"number\">100</span> AS table_disk_usage,  table FROM system.parts GROUP BY table ORDER BY table_disk_usage DESC LIMIT <span class=\"number\">10</span>; </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化后的语句： </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">WITH CAST(<span class=\"number\">0</span>, \\<span class=\"string\">&#x27;UInt64\\&#x27;) AS total_disk_usage SELECT  (sum(bytes) / total_disk_usage) * 100 AS table_disk_usage,  table FROM system.parts GROUP BY table ORDER BY table_disk_usage DESC LIMIT 10</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"三元运算符优化\"><a class=\"markdownIt-Anchor\" href=\"#三元运算符优化\">#</a> 三元运算符优化</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果开启了 optimize_if_chain_to_multiif 参数，三元运算符会被替换成 multiIf 函数， 例如： </span><br><span class=\"line\"></span><br><span class=\"line\">EXPLAIN SYNTAX SELECT number = <span class=\"number\">1</span> ? <span class=\"string\">&#x27;hello&#x27;</span> : (number = <span class=\"number\">2</span> ? <span class=\"string\">&#x27;world&#x27;</span> : <span class=\"string\">&#x27;atguigu&#x27;</span>) FROM numbers(<span class=\"number\">10</span>) settings optimize_if_chain_to_multiif = <span class=\"number\">1</span>; </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">//返回优化后的语句： </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SELECT multiIf(number = <span class=\"number\">1</span>, \\<span class=\"string\">&#x27;hello\\&#x27;, number = 2, \\&#x27;world\\&#x27;, \\&#x27;atguigu\\&#x27;) FROM numbers(10) SETTINGS optimize_if_chain_to_multiif = 1</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"单表查询优化\"><a class=\"markdownIt-Anchor\" href=\"#单表查询优化\">#</a> 单表查询优化</h3>\n<h4 id=\"prewhere替代where\"><a class=\"markdownIt-Anchor\" href=\"#prewhere替代where\">#</a> prewhere 替代 where</h4>\n<p>where 过滤行，选出列</p>\n<p>Prewhere 和 where 语句的作用相同，用来过滤数据。不同之处在于 prewhere 只支持 *MergeTree 族系列引擎的表，首先会读取指定的列数据，来判断数据过滤，等待数据过滤 之后再读取 select 声明的列字段来补全其余属性。</p>\n<p>当查询列明显多于筛选列时使用 Prewhere 可十倍提升查询性能，Prewhere 会自动优化 执行过滤阶段的数据读取方式，降低 io 操作。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 关闭where自动优化为prewhere</span></span><br><span class=\"line\"><span class=\"built_in\">set</span> optimize_move_to_prewhere=<span class=\"number\">0</span>; </span><br></pre></td></tr></table></figure>\n<p>默认情况，我们肯定不会关闭 where 自动优化成 prewhere，但是某些场景即使开启优 化，也不会自动转换成 prewhere，需要手动指定 prewhere：</p>\n<ul>\n<li>使用常量表达式</li>\n<li>使用默认值为 alias 类型的字段</li>\n<li>包含了 arrayJOIN，globalIn，globalNotIn 或者 indexHint 的查询</li>\n<li>select 查询的列字段和 where 的谓词相同</li>\n<li>使用了主键字段</li>\n</ul>\n<h4 id=\"数据采样\"><a class=\"markdownIt-Anchor\" href=\"#数据采样\">#</a> 数据采样</h4>\n<p>通过采样运算可极大提升数据分析的性能</p>\n<p>SELECT Title,count (*) AS PageViews FROM hits_v1 SAMPLE 0.1  WHERE CounterID =57 GROUP BY Title ORDER BY PageViews DESC LIMIT 1000   #代表采样 10% 的数据，也可以是具体的条数</p>\n<p>采样修饰符只有在 MergeTree engine 表中才有效，且在创建表时需要指定采样策略。</p>\n<h4 id=\"列裁剪-分区裁剪\"><a class=\"markdownIt-Anchor\" href=\"#列裁剪-分区裁剪\">#</a> 列裁剪、分区裁剪</h4>\n<p>数据量太大时应避免使用 select * 操作，查询的性能会与查询的字段大小和数量成线性 表换，字段越少，消耗的 io 资源越少，性能就会越高。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">分区裁剪就是只读取需要的分区，在过滤条件中指定。 </span><br><span class=\"line\"></span><br><span class=\"line\">select WatchID,  JavaEnable,  Title,  GoodEvent,  EventTime,  EventDate,  CounterID,  ClientIP,  ClientIP6,  RegionID,  UserID <span class=\"keyword\">from</span> datasets.hits_v1 where EventDate=<span class=\"string\">&#x27;2014-03-23&#x27;</span>;</span><br></pre></td></tr></table></figure>\n<h4 id=\"order-by结合limitwhere\"><a class=\"markdownIt-Anchor\" href=\"#order-by结合limitwhere\">#</a> order by 结合 limit，where</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">千万以上数据集进行 order by 查询时需要搭配 where 条件和 limit 语句一起使用。 </span><br><span class=\"line\"><span class=\"comment\">#正例： </span></span><br><span class=\"line\">SELECT UserID,Age FROM hits_v1 WHERE CounterID=<span class=\"number\">57</span> ORDER BY Age DESC LIMIT <span class=\"number\">1000</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"避免构建虚拟列\"><a class=\"markdownIt-Anchor\" href=\"#避免构建虚拟列\">#</a> 避免构建虚拟列</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如非必须，不要在结果集上构建虚拟列，虚拟列非常消耗资源浪费性能，可以考虑在前 端进行处理，或者在表中构造实际字段进行额外存储。</span><br><span class=\"line\"></span><br><span class=\"line\">SELECT Income,Age,Income/Age <span class=\"keyword\">as</span> IncRate FROM datasets.hits_v1; </span><br><span class=\"line\"></span><br><span class=\"line\">正例：拿到 Income 和 Age 后，考虑在前端进行处理，或者在表中构造实际字段进行额外存储</span><br><span class=\"line\"></span><br><span class=\"line\">SELECT Income,Age FROM datasets.hits_v1;</span><br></pre></td></tr></table></figure>\n<h4 id=\"uniqcombined-替代distinct\"><a class=\"markdownIt-Anchor\" href=\"#uniqcombined-替代distinct\">#</a> uniqcombined 替代 distinct</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">性能可提升 <span class=\"number\">10</span> 倍以上，uniqCombined 底层采用类似 HyperLogLog 算法实现，能接收 <span class=\"number\">2</span>% 左右的数据误差，可直接使用这种去重方式提升查询性能。Count(distinct )会使用 uniqExact 精确去重</span><br><span class=\"line\"></span><br><span class=\"line\">反例： select count(distinct rand()) <span class=\"keyword\">from</span> hits_v1; </span><br><span class=\"line\">正例： SELECT uniqCombined(rand()) <span class=\"keyword\">from</span> datasets.hits_v1</span><br></pre></td></tr></table></figure>\n<p>使用物化视图</p>\n<h4 id=\"其他注意事项\"><a class=\"markdownIt-Anchor\" href=\"#其他注意事项\">#</a> 其他注意事项</h4>\n<p>（1）查询熔断 为了避免因个别慢查询引起的服务雪崩的问题，除了可以为单个查询设置超时以外，还 可以配置周期熔断，在一个查询周期内，如果用户频繁进行慢查询操作超出规定阈值后将无 法继续进行查询操作。</p>\n<p>（2）关闭虚拟内存 物理内存和虚拟内存的数据交换，会导致查询变慢，资源允许的情况下关闭虚拟内存。</p>\n<p>（3）配置 join_use_nulls 为每一个账户添加 join_use_nulls 配置，左表中的一条记录在右表中不存在，右表的相应字段会返回该字段相应数据类型的默认值，而不是标准 SQL 中的 Null 值。</p>\n<p>（4）批量写入时先排序 批量写入数据时，必须控制每个批次的数据中涉及到的分区的数量，在写入之前最好对 需要导入的数据进行排序。无序的数据或者涉及的分区太多，会导致 ClickHouse 无法及时对 新导入的数据进行合并，从而影响查询性能。</p>\n<p>（5）关注 CPU cpu 一般在 50% 左右会出现查询波动，达到 70% 会出现大范围的查询超时，cpu 是最关 键的指标，要非常关注。</p>\n<h3 id=\"多表查询优化\"><a class=\"markdownIt-Anchor\" href=\"#多表查询优化\">#</a> 多表查询优化</h3>\n<p>A join  B   B 加载到内存，A 的每条数据去内存查</p>\n<h4 id=\"用in代替join\"><a class=\"markdownIt-Anchor\" href=\"#用in代替join\">#</a> 用 in 代替 join</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">当多表联查时，查询的数据仅从其中一张表出时，可考虑用 IN 操作而不是 JOIN </span><br><span class=\"line\"></span><br><span class=\"line\">insert into hits_v2 select a.* <span class=\"keyword\">from</span> hits_v1 a where a. CounterID <span class=\"keyword\">in</span> (select CounterID <span class=\"keyword\">from</span> visits_v1);</span><br></pre></td></tr></table></figure>\n<h4 id=\"大小表join\"><a class=\"markdownIt-Anchor\" href=\"#大小表join\">#</a> 大小表 join</h4>\n<p>小表在右，右表加载入内存</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">多表 join 时要满足小表在右的原则，右表关联时被加载到内存中与左表进行比较， ClickHouse 中无论是 Left join 、Right join 还是 Inner join 永远都是拿着右表中的每一条记录 到左表中查找该记录是否存在，所以右表必须是小表。 </span><br><span class=\"line\"></span><br><span class=\"line\">（<span class=\"number\">1</span>）小表在右 </span><br><span class=\"line\">insert into table hits_v2 select a.* <span class=\"keyword\">from</span> hits_v1 a left join visits_v2 b on a. CounterID=b. CounterID;</span><br></pre></td></tr></table></figure>\n<h4 id=\"谓词下推-2\"><a class=\"markdownIt-Anchor\" href=\"#谓词下推-2\">#</a> 谓词下推</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ClickHouse 在 join 查询时不会主动发起谓词下推的操作，需要每个子查询提前完成过滤操作，需要注意的是，是否执行谓词下推，对性能影响差别很大（新版本中已经不存在此问 题，但是需要注意谓词的位置的不同依然有性能的差异）</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">insert into hits_v2 select a.* <span class=\"keyword\">from</span> hits_v1 a left join visits_v2 b on a. CounterID=b. CounterID where a.EventDate = <span class=\"string\">&#x27;2014-03-17&#x27;</span>; </span><br><span class=\"line\"></span><br><span class=\"line\">insert into hits_v2 select a.* <span class=\"keyword\">from</span> (  select * <span class=\"keyword\">from</span>  hits_v1  where EventDate = <span class=\"string\">&#x27;2014-03-17&#x27;</span> ) a left join visits_v2 b on a. CounterID=b. CounterID;</span><br></pre></td></tr></table></figure>\n<h4 id=\"分布式表使用global\"><a class=\"markdownIt-Anchor\" href=\"#分布式表使用global\">#</a> 分布式表使用 global</h4>\n<p>两张分布式表上的 IN 和 JOIN 之前必须加上 GLOBAL 关键字，右表只会在接收查询请求的那个节点查询一次，并将其分发到其他节点上。如果不加 GLOBAL 关键字的话，每个节点 都会单独发起一次对右表的查询，而右表又是分布式表，就导致右表一共会被查询 N² 次（N 是该分布式表的分片数量），这就是查询放大，会带来很大开销。</p>\n<h4 id=\"使用字典表\"><a class=\"markdownIt-Anchor\" href=\"#使用字典表\">#</a> 使用字典表</h4>\n<p>将一些需要关联分析的业务创建成字典表进行 join 操作，前提是字典表不宜太大，因为字典表会常驻内存</p>\n<h4 id=\"提前过滤\"><a class=\"markdownIt-Anchor\" href=\"#提前过滤\">#</a> 提前过滤</h4>\n<p>通过增加逻辑过滤可以减少数据扫描，达到提高执行速度及降低内存消耗的目的</p>\n<p>ck 的 join:</p>\n<p>1、原理:</p>\n<p>右表加载到内存，再去匹配</p>\n<p>2、为什么 join 不行：因为 1</p>\n<p>3、非要使用，怎么用比较好:</p>\n<p>能过滤先过滤，特别是右表</p>\n<p>右边放小表</p>\n<p>特殊场景可以考虑使用字典表</p>\n<p>可以替换的话，尽量不要用 join, 比如用 in 实现</p>\n<h3 id=\"数据一致性\"><a class=\"markdownIt-Anchor\" href=\"#数据一致性\">#</a> 数据一致性</h3>\n<p>我们在使用 ReplacingMergeTree、SummingMergeTree 这类表引擎的时候，会出现短暂 数据不一致的情况。</p>\n<h4 id=\"手动optimize\"><a class=\"markdownIt-Anchor\" href=\"#手动optimize\">#</a> 手动 OPTIMIZE</h4>\n<p>在写入数据后，立刻执行 OPTIMIZE 强制触发新写入分区的合并动作。</p>\n<p>OPTIMIZE TABLE test_a FINAL;</p>\n<h4 id=\"通过group-by去重\"><a class=\"markdownIt-Anchor\" href=\"#通过group-by去重\">#</a> 通过 Group by 去重</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">执行去重的查询 </span><br><span class=\"line\"></span><br><span class=\"line\">SELECT  user_id ,  argMax(score, create_time) AS score,  argMax(deleted, create_time) AS deleted,  <span class=\"built_in\">max</span>(create_time) AS ctime FROM test_a GROUP BY user_id HAVING deleted = <span class=\"number\">0</span>; </span><br><span class=\"line\"></span><br><span class=\"line\">函数说明：  argMax(field1，field2):按照 field2 的最大值取 field1 的值。 当我们更新数据时，会写入一行新的数据，例如上面语句中，通过查询最大的 create_time 得到修改后的 score 字段值。</span><br><span class=\"line\"></span><br><span class=\"line\">加标记字段实现</span><br></pre></td></tr></table></figure>\n<h4 id=\"final查询去重\"><a class=\"markdownIt-Anchor\" href=\"#final查询去重\">#</a> final 查询去重</h4>\n<p>在查询语句后增加 FINAL 修饰符，这样在查询的过程中将会执行 Merge 的特殊逻辑（例 如数据去重，预聚合等）。</p>\n<p>20.5 之后版本，final 是多线程，但是读取分区是串行的。</p>\n<h3 id=\"物化视图\"><a class=\"markdownIt-Anchor\" href=\"#物化视图\">#</a> 物化视图</h3>\n<p>普通视图不保存数据，保存的仅仅是查询语句，查询的时候还是从原表读取数据，可以将普通视图理解为是个子查询。物化视图则是把查询的结果根据相应的引擎存入到了磁盘或内存中，对数据重新进行了组织，你可以理解物化视图是完全的一张新表。ClickHouse 的物化视图是一种查询结果的持久化</p>\n<p>优点：查询速度快，要是把物化视图这些规则全部写好，它比原数据查询快了很多，总的行数少了，因为都预计算好了。</p>\n<p>缺点：它的本质是一个流式数据的使用场景，是累加式的技术，所以要用历史数据做去重、去核这样的分析，在物化视图里面是不太好用的。在某些场景的使用也是有限的。而且 如果一张表加了好多物化视图，在写这张表的时候，就会消耗很多机器的资源，比如数据带 宽占满、存储一下子增加了很多。</p>\n<p>也是 create 语法，会创建一个隐藏的目标表来保存视图数据。也可以 TO 表名，保存到 一张显式的表。没有加 TO 表名，表名默认就是 .inner. 物化视图名</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...</span><br><span class=\"line\">物化视图限制</span><br><span class=\"line\"><span class=\"number\">1.</span>必须指定物化视图的 engine 用于数据存储 </span><br><span class=\"line\"><span class=\"number\">2.</span>TO [db].[table]语法的时候，不得使用 POPULATE。 </span><br><span class=\"line\"><span class=\"number\">3.</span>查询语句(select）可以包含下面的子句： DISTINCT, GROUP BY, ORDER BY, LIMIT… </span><br><span class=\"line\"><span class=\"number\">4.</span>物化视图的 alter 操作有些限制，操作起来不大方便。 </span><br><span class=\"line\"><span class=\"number\">5.</span>若物化视图的定义使用了 TO [db.]name 子语句，则可以将目标表的视图 卸载 DETACH 再装载 ATTACH</span><br><span class=\"line\"></span><br><span class=\"line\">clickhouse 官方并不推荐使用 POPULATE，因为在创建物化视图的过程中同时写入 的数据不能被插入物化视图。</span><br><span class=\"line\">CREATE MATERIALIZED VIEW hits_mv ENGINE=SummingMergeTree PARTITION BY toYYYYMM(EventDate) ORDER BY (EventDate, intHash32(UserID)) AS SELECT UserID, EventDate, count(URL) <span class=\"keyword\">as</span> ClickCount, <span class=\"built_in\">sum</span>(Income) AS IncomeSum FROM hits_test WHERE EventDate &gt;= <span class=\"string\">&#x27;2014-03-20&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>如果需要历史数据，手动 insert 到 view 里</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#导入增量数据 </span></span><br><span class=\"line\">INSERT INTO hits_test SELECT  EventDate,  CounterID,  UserID,  URL,  Income FROM hits_v1 WHERE EventDate &gt;= <span class=\"string\">&#x27;2014-03-23&#x27;</span> limit <span class=\"number\">10</span>; </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#查询物化视图 </span></span><br><span class=\"line\"></span><br><span class=\"line\">SELECT * FROM hits_mv;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 查询物化视图</span></span><br><span class=\"line\">`.innner.hits_mv`</span><br></pre></td></tr></table></figure>\n<h3 id=\"materializemysql\"><a class=\"markdownIt-Anchor\" href=\"#materializemysql\">#</a> MaterializeMysql</h3>\n<p>ClickHouse 20.8.2.3 版本新增加了 MaterializeMySQL 的 database 引擎，该 database 能 映 射 到 MySQL 中 的 某 个 database ， 并 自 动 在 ClickHouse 中 创 建 对 应 的 ReplacingMergeTree。ClickHouse 服务做为 MySQL 副本，读取 Binlog 并执行 DDL 和 DML 请 求，实现了基于 MySQL Binlog 机制的业务数据库实时同步功能。</p>\n<p>（1）MaterializeMySQL 同时支持全量和增量同步，在 database 创建之初会全量同步 MySQL 中的表和数据，之后则会通过 binlog 进行增量同步。</p>\n<p>（2）MaterializeMySQL database 为其所创建的每张 ReplacingMergeTree 自动增加了 _sign 和 _version 字段。 其中，_version 用作 ReplacingMergeTree 的 ver 版本参数，每当监听到 insert、update 和 delete 事件时，在 databse 内全局自增。而 _sign 则用于标记是否被删除，取值 1 或 者 -1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（<span class=\"number\">1</span>）确保 MySQL 开启了 binlog 功能，且格式为 ROW 打开/etc/my.cnf,在[mysqld]下添加：</span><br><span class=\"line\">server-<span class=\"built_in\">id</span>=<span class=\"number\">1</span> </span><br><span class=\"line\">log-<span class=\"built_in\">bin</span>=mysql-<span class=\"built_in\">bin</span> </span><br><span class=\"line\">binlog_format=ROW </span><br><span class=\"line\"></span><br><span class=\"line\">（<span class=\"number\">2</span>）开启 GTID 模式 如果如果 clickhouse 使用的是 <span class=\"number\">20.8</span> prestable 之后发布的版本，那么 MySQL 还需要配置 开启 GTID 模式, 这种方式在 mysql 主从模式下可以确保数据同步的一致性(主从切换时)。 </span><br><span class=\"line\">gtid-mode=on </span><br><span class=\"line\">enforce-gtid-consistency=<span class=\"number\">1</span> <span class=\"comment\"># 设置为主从强一致性 </span></span><br><span class=\"line\">log-slave-updates=<span class=\"number\">1</span> <span class=\"comment\"># 记录日志</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"监控clickhouse\"><a class=\"markdownIt-Anchor\" href=\"#监控clickhouse\">#</a> 监控 clickhouse</h3>\n<p>ClickHouse 运行时会将一些个自身的运行状态记录到众多系统表中 (system.*)。所以我们对于 CH 自身的一些运行指标的监控数据，也主要来自这些系统表</p>\n<p>ClickHouse 从 v20.1.2.4 开始，内置了对接 Prometheus 的功能，配置的方式也很简单，可以将其作为 Prometheus 的 Endpoint 服务，从而自动的将 metrics 、 events 和 asynchronous_metrics 三张系统的表的数据发送给 Prometheus。</p>\n<p>安装 Prometheus，修改配置文件</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">scrape_configs:</span></span><br><span class=\"line\">  <span class=\"comment\"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">job_name:</span> <span class=\"string\">&quot;prometheus&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># metrics_path defaults to &#x27;/metrics&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># scheme defaults to &#x27;http&#x27;.</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"attr\">static_configs:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">targets:</span> [<span class=\"string\">&quot;localhost:9090&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">job_name:</span> <span class=\"string\">&quot;clickhouse&quot;</span></span><br><span class=\"line\">    <span class=\"attr\">static_configs:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">targets:</span> [<span class=\"string\">&quot;hadoop100:9363&quot;</span>]</span><br></pre></td></tr></table></figure>\n<p>启动 Prometheus 和 grafana</p>\n<p>修改 clickhouse 配置文件 config.xml，重启服务</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">prometheus</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">endpoint</span>&gt;</span>/metrics<span class=\"tag\">&lt;/<span class=\"name\">endpoint</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9363<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">metrics</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">metrics</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">events</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">events</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">asynchronous_metrics</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">asynchronous_metrics</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">prometheus</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>访问 web：<span class=\"exturl\" data-url=\"aHR0cDovL3gueC54Lng6OTM2My9tZXRyaWNz\">http://x.x.x.x:9363/metrics</span>  确认开启</p>\n<p>配置 grafana</p>\n<p>配置 datasource</p>\n<p>添加 dashboard</p>\n<h3 id=\"数据备份\"><a class=\"markdownIt-Anchor\" href=\"#数据备份\">#</a> 数据备份</h3>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGlja2hvdXNlLnRlY2gvZG9jcy9lbi9vcGVyYXRpb25zL2JhY2t1cC8=\">https://clickhouse.tech/docs/en/operations/backup/</span></p>\n<h4 id=\"手动备份\"><a class=\"markdownIt-Anchor\" href=\"#手动备份\">#</a> 手动备份</h4>\n<p>ClickHouse 允许使用 ALTER TABLE … FREEZE PARTITION … 查询创建表分区的本地副本。 这是利用硬链接 (hardlink) 到 /var/lib/clickhouse/shadow/ 文件夹中实现的，所以它通常不会因为旧数据而占用额外的磁盘空间。 创建的文件副本不由 ClickHouse 服务器处理，所以不需要任何额外的外部系统就有一个简单的备份。防止硬件问题，最好将它们远程复制到另一个位置，然后删除本地副本。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">5.1.1 创建备份路径 </span><br><span class=\"line\">（1）创建用于存放备份数据的目录 shadow </span><br><span class=\"line\">sudo mkdir -p /var/lib/clickhouse/shadow/ </span><br><span class=\"line\">如果目录存在，先清空目录下的数据 </span><br><span class=\"line\">5.1.2 执行备份命令 </span><br><span class=\"line\">echo -n &#x27;alter table t_order_mt freeze&#x27; | clickhouse-client </span><br><span class=\"line\">5.1.3 将备份数据保存到其他路径 </span><br><span class=\"line\">#创建备份存储路径 </span><br><span class=\"line\">sudo mkdir -p /var/lib/clickhouse/backup/ </span><br><span class=\"line\">#拷贝数据到备份路径 </span><br><span class=\"line\">sudo cp -r /var/lib/clickhouse/shadow/  </span><br><span class=\"line\">/var/lib/clickhouse/backup/my-backup-name </span><br><span class=\"line\">#为下次备份准备，删除 shadow 下的数据 </span><br><span class=\"line\">sudo rm -rf /var/lib/clickhouse/shadow/* </span><br><span class=\"line\">5.1.4 恢复数据 </span><br><span class=\"line\">（1）模拟删除备份过的表 </span><br><span class=\"line\">echo &#x27; drop table t_order_mt &#x27; | clickhouse-client </span><br><span class=\"line\">（2）重新创建表 </span><br><span class=\"line\">cat events.sql | clickhouse-client </span><br><span class=\"line\">（3）将备份复制到 detached 目录 </span><br><span class=\"line\">sudo cp -rl </span><br><span class=\"line\">backup/my-backup-name/1/store/cb1/cb176503-cd88-4ea8-8b17-6503cd888ea8/* data/default/t_order_mt/detached/ </span><br><span class=\"line\">ClickHouse 使用文件系统硬链接来实现即时备份，而不会导致 ClickHouse 服务停机（或锁定）。这些硬链接可以进一步用于有效的备份存储。在支持硬链接的文件系统（例如本地文件系统或 NFS）上，将 cp 与-l 标志一起使用（或将 rsync 与–hard-links 和–numeric-ids 标志一起使用）以避免复制数据。</span><br><span class=\"line\"></span><br><span class=\"line\">注意：仅拷贝分区目录，注意目录所属的用户要是 clickhouse </span><br><span class=\"line\">（4）执行 attach </span><br><span class=\"line\">echo &#x27;alter table t_order_mt attach partition 20200601&#x27; | clickhouse-client </span><br><span class=\"line\">（5）查看数据 </span><br><span class=\"line\">echo &#x27;select count() from t_order_mt&#x27; | clickhouse-client </span><br></pre></td></tr></table></figure>\n<h4 id=\"clickhouse-backup\"><a class=\"markdownIt-Anchor\" href=\"#clickhouse-backup\">#</a> clickhouse-backup</h4>\n<p>我们可以使用 Clickhouse 的备份工具 clickhouse-backup 帮我们自动化实现</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL0FsZXhBa3Vsb3YvY2xpY2tob3VzZS1iYWNrdXAv\">https://github.com/AlexAkulov/clickhouse-backup/</span></p>\n",
            "tags": [
                "大数据"
            ]
        },
        {
            "id": "http://example.com/2024/05/01/hive_flink/",
            "url": "http://example.com/2024/05/01/hive_flink/",
            "title": "hive_flink",
            "date_published": "2024-05-01T05:38:45.000Z",
            "content_html": "<h2 id=\"hive\"><a class=\"markdownIt-Anchor\" href=\"#hive\">#</a> hive</h2>\n<p>Hive 是由 Facebook 开源，基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。</p>\n<p>Hive 是一个 Hadoop 客户端，用于将 HQL (HiveSQL) 转化成 MapfReduce 程序。</p>\n<p>(1) Hive 中每张表的数据存储在 HDFS</p>\n<p>(2) Hive 分析数据底层的实现是 MapReduce (也可配置为 Sparl 或者 Tez)</p>\n<p>(3) 执行程序运行在 Yarn 上</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224128675.png\" alt=\"image-20240804224128675\"></p>\n<h3 id=\"最小化安装\"><a class=\"markdownIt-Anchor\" href=\"#最小化安装\">#</a> 最小化安装</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http:<span class=\"comment\">//apache.mirror.iweb.ca/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz</span></span><br><span class=\"line\">tar xzvf apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin.tar.gz</span><br><span class=\"line\">cd apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/</span><br><span class=\"line\"></span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">export HIVE_HOME=/root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin</span><br><span class=\"line\">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class=\"line\"></span><br><span class=\"line\">./schematool -dbType derby -initSchema</span><br><span class=\"line\"></span><br><span class=\"line\">hive </span><br><span class=\"line\">&gt; show databases;</span><br></pre></td></tr></table></figure>\n<p>路径同一时刻，只能允许一个 derby 客户端使用</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL2FwYWNoZS5taXJyb3IuaXdlYi5jYS9oaXZlL2hpdmUtMy4xLjMv\">http://apache.mirror.iweb.ca/hive/hive-3.1.3/</span></p>\n<h3 id=\"mysql-hive\"><a class=\"markdownIt-Anchor\" href=\"#mysql-hive\">#</a> mysql-hive</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 卸载mariadb</span><br><span class=\"line\"></span><br><span class=\"line\"># 安装mysql</span><br><span class=\"line\">apt install mysql-server</span><br><span class=\"line\">systemctl enable mysql --now</span><br><span class=\"line\">sudo mysql_secure_installation</span><br><span class=\"line\">ALTER USER <span class=\"string\">&#x27;root&#x27;</span>@<span class=\"string\">&#x27;localhost&#x27;</span> IDENTIFIED BY <span class=\"string\">&#x27;123456&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># hive存储元数据到mysql</span><br><span class=\"line\">create database metastore;</span><br><span class=\"line\"></span><br><span class=\"line\"># 修改配置文件 hive-site.xml</span><br><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;jdbc:mysql:<span class=\"comment\">//hadoop100:3306/metastore?useSSL=false&lt;/value&gt;</span></span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      JDBC connect string <span class=\"keyword\">for</span> a JDBC metastore.</span><br><span class=\"line\">      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class=\"line\">      For example, jdbc:postgresql:<span class=\"comment\">//myhost/db?ssl=true for postgres database.</span></span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt; </span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;root&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">123456</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Driver <span class=\"keyword\">class</span> <span class=\"title class_\">name</span> <span class=\"keyword\">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;location of <span class=\"keyword\">default</span> database <span class=\"keyword\">for</span> the warehouse&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/lib/log4j-slf4j-impl-<span class=\"number\">2.17</span><span class=\"number\">.1</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/share/hadoop/common/lib/slf4j-reload4j-<span class=\"number\">1.7</span><span class=\"number\">.36</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: See http:<span class=\"comment\">//www.slf4j.org/codes.html#multiple_bindings for an explanation.</span></span><br><span class=\"line\">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class=\"line\">Metastore connection URL:        jdbc:mysql:<span class=\"comment\">//hadoop100:3306/metastore?useSSL=false</span></span><br><span class=\"line\">Metastore Connection Driver :    com.mysql.cj.jdbc.Driver</span><br><span class=\"line\">Metastore connection User:       root</span><br><span class=\"line\">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.</span><br><span class=\"line\">Underlying cause: com.mysql.cj.jdbc.exceptions.CommunicationsException : Communications link failure</span><br><span class=\"line\"></span><br><span class=\"line\">The last packet sent successfully to the server was <span class=\"number\">0</span> milliseconds ago. The driver has not received any packets from the server.</span><br><span class=\"line\">SQL Error code: <span class=\"number\">0</span></span><br><span class=\"line\">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.</span><br><span class=\"line\">        at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:<span class=\"number\">94</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:<span class=\"number\">169</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:<span class=\"number\">475</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:<span class=\"number\">581</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:<span class=\"number\">567</span>)</span><br><span class=\"line\">        at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:<span class=\"number\">1517</span>)</span><br><span class=\"line\">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class=\"line\">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class=\"number\">43</span>)</span><br><span class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:<span class=\"number\">498</span>)</span><br><span class=\"line\">        at org.apache.hadoop.util.RunJar.run(RunJar.java:<span class=\"number\">328</span>)</span><br><span class=\"line\">        at org.apache.hadoop.util.RunJar.main(RunJar.java:<span class=\"number\">241</span>)</span><br><span class=\"line\">Caused by: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure</span><br><span class=\"line\"></span><br><span class=\"line\">The last packet sent successfully to the server was <span class=\"number\">0</span> milliseconds ago. The driver has not received any packets from the server.</span><br><span class=\"line\">        at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:<span class=\"number\">175</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:<span class=\"number\">64</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:<span class=\"number\">825</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:<span class=\"number\">446</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:<span class=\"number\">239</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:<span class=\"number\">188</span>)</span><br><span class=\"line\">        at java.sql.DriverManager.getConnection(DriverManager.java:<span class=\"number\">664</span>)</span><br><span class=\"line\">        at java.sql.DriverManager.getConnection(DriverManager.java:<span class=\"number\">247</span>)</span><br><span class=\"line\">        at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:<span class=\"number\">88</span>)</span><br><span class=\"line\">        ... <span class=\"number\">11</span> more</span><br><span class=\"line\">Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure</span><br><span class=\"line\"></span><br><span class=\"line\">The last packet sent successfully to the server was <span class=\"number\">0</span> milliseconds ago. The driver has not received any packets from the server.</span><br><span class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class=\"number\">45</span>)</span><br><span class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class=\"number\">423</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:<span class=\"number\">105</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:<span class=\"number\">150</span>)</span><br><span class=\"line\">        at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:<span class=\"number\">166</span>)</span><br><span class=\"line\">        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:<span class=\"number\">89</span>)</span><br><span class=\"line\">        at com.mysql.cj.NativeSession.connect(NativeSession.java:<span class=\"number\">121</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:<span class=\"number\">945</span>)</span><br><span class=\"line\">        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:<span class=\"number\">815</span>)</span><br><span class=\"line\">        ... <span class=\"number\">17</span> more</span><br><span class=\"line\">Caused by: java.net.ConnectException: Connection <span class=\"title function_\">refused</span> <span class=\"params\">(Connection refused)</span></span><br><span class=\"line\">        at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class=\"line\">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:<span class=\"number\">350</span>)</span><br><span class=\"line\">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:<span class=\"number\">206</span>)</span><br><span class=\"line\">        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:<span class=\"number\">188</span>)</span><br><span class=\"line\">        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:<span class=\"number\">392</span>)</span><br><span class=\"line\">        at java.net.Socket.connect(Socket.java:<span class=\"number\">607</span>)</span><br><span class=\"line\">        at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:<span class=\"number\">153</span>)</span><br><span class=\"line\">        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:<span class=\"number\">63</span>)</span><br><span class=\"line\">        ... <span class=\"number\">20</span> more</span><br><span class=\"line\">*** schemaTool failed ***</span><br><span class=\"line\">如果报错，注意，当前用户不是localhost</span><br><span class=\"line\">mysql&gt; use mysql;</span><br><span class=\"line\">Reading table information <span class=\"keyword\">for</span> completion of table and column names</span><br><span class=\"line\">You can turn off <span class=\"built_in\">this</span> feature to get a quicker startup with -A</span><br><span class=\"line\"></span><br><span class=\"line\">Database changed</span><br><span class=\"line\">mysql&gt; select host,user from user where user=<span class=\"string\">&#x27;root&#x27;</span>;</span><br><span class=\"line\">+-----------+------+</span><br><span class=\"line\">| host      | user |</span><br><span class=\"line\">+-----------+------+</span><br><span class=\"line\">| localhost | root |</span><br><span class=\"line\">+-----------+------+</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">配置文件中不是<span class=\"number\">127.0</span><span class=\"number\">.0</span><span class=\"number\">.1</span></span><br><span class=\"line\">通常情况下，这个配置文件位于/etc/mysql/mysql.conf.d/mysqld.cnf，找到bind-address这一项并将其更改为<span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span>。然后重启mysql服务：sudo systemctl restart mysql。</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">配置文件中账号密码正确</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">已经下载mysql连接java驱动，并且复制到hive/lib下面</span><br><span class=\"line\">https:<span class=\"comment\">//downloads.mysql.com/archives/c-j/</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Relative path in absolute URI: $&#123;system:java.io.tmpdir%<span class=\"number\">7D</span>/$%7Bsystem:user.name%<span class=\"number\">7D</span></span><br><span class=\"line\">https:<span class=\"comment\">//www.cnblogs.com/qxyy/articles/5247933.html</span></span><br><span class=\"line\">把上面文章中所有 $HIVE_HOME/iotmp 改为绝对路径，比如 /root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/iotmp</span><br><span class=\"line\">验证：</span><br><span class=\"line\">hive</span><br><span class=\"line\">show databases；</span><br></pre></td></tr></table></figure>\n<h3 id=\"hiveserver2\"><a class=\"markdownIt-Anchor\" href=\"#hiveserver2\">#</a> hiveserver2</h3>\n<p>Hive 的 hiveserver2 服务的作用是提供 jdbc/odbc 接口，为用户提供远程访问 Hive 数据的功能，例如用户期望在个人电脑中访问远程服务中的 Hive 数据，就需要用到 Hiveserver2</p>\n<h4 id=\"用户模拟功能\"><a class=\"markdownIt-Anchor\" href=\"#用户模拟功能\">#</a> 用户模拟功能</h4>\n<p>在远程访问 Hive 数据时，客户端并未直接访问 Hadoop 集群，而是由 Hivesever2 代理访问。由于 Hadoop 集群中的数据具备访问权限控制，所以此时需考虑一个问题：那就是访问 Hadoop 集群的用户身份是谁？是 Hiveserver2 的启动用户？还是客户端的登录用户？</p>\n<p>答案是都有可能，具体是谁，由 Hiveserver2 的 hive.server2.enable.doAs 参数决定，该参数的含义是是否启用 Hiveserver2 用户模拟的功能。若启用，则 Hiveserver2 会模拟成客户端的登录用户去访问 Hadoop 集群的数据，不启用，则 Hivesever2 会直接使用启动用户访问 Hadoop 集群数据。模拟用户的功能，默认是开启的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim core-site.yaml</span><br><span class=\"line\">&lt;!--配置所有节点的atguigu用户都可作为代理用户--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;!--配置atguigu用户能够代理的用户组为任意组--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;!--配置atguigu用户能够代理的用户为任意用户--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hadoop.proxyuser.root.users&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">scp core-site.xml root@<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.191</span>:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/etc/hadoop</span><br><span class=\"line\">scp core-site.xml root@<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.192</span>:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/etc/hadoop</span><br><span class=\"line\">hadoop-start.sh</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim hive-site.yaml</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 指定hiveserver2连接的host --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class=\"line\">   &lt;value&gt;hadoop100&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class=\"line\">   &lt;value&gt;<span class=\"number\">10000</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">bin/hiveserver2</span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin# nohup hiveserver2 &gt;/dev/<span class=\"literal\">null</span> <span class=\"number\">2</span>&gt;&amp;<span class=\"number\">1</span>  &amp; </span><br><span class=\"line\">[<span class=\"number\">1</span>] <span class=\"number\">124804</span></span><br><span class=\"line\"></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin# jps</span><br><span class=\"line\"><span class=\"number\">123633</span> NameNode</span><br><span class=\"line\"><span class=\"number\">124131</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">124804</span> RunJar</span><br></pre></td></tr></table></figure>\n<p>测试</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin# beeline </span><br><span class=\"line\">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/apache-hive-<span class=\"number\">3.1</span><span class=\"number\">.3</span>-bin/lib/log4j-slf4j-impl-<span class=\"number\">2.17</span><span class=\"number\">.1</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: Found binding in [jar:file:/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/share/hadoop/common/lib/slf4j-reload4j-<span class=\"number\">1.7</span><span class=\"number\">.36</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class=\"line\">SLF4J: See http:<span class=\"comment\">//www.slf4j.org/codes.html#multiple_bindings for an explanation.</span></span><br><span class=\"line\">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class=\"line\">Beeline version <span class=\"number\">3.1</span><span class=\"number\">.3</span> by Apache Hive</span><br><span class=\"line\">beeline&gt; !connect jdbc:hive2:<span class=\"comment\">//hadoop100:10000</span></span><br><span class=\"line\">Connecting to jdbc:hive2:<span class=\"comment\">//hadoop100:10000</span></span><br><span class=\"line\">Enter username <span class=\"keyword\">for</span> jdbc:hive2:<span class=\"comment\">//hadoop100:10000: root</span></span><br><span class=\"line\">Enter password <span class=\"keyword\">for</span> jdbc:hive2:<span class=\"comment\">//hadoop100:10000: </span></span><br><span class=\"line\">Connected to: Apache <span class=\"title function_\">Hive</span> <span class=\"params\">(version <span class=\"number\">3.1</span><span class=\"number\">.3</span>)</span></span><br><span class=\"line\">Driver: Hive <span class=\"title function_\">JDBC</span> <span class=\"params\">(version <span class=\"number\">3.1</span><span class=\"number\">.3</span>)</span></span><br><span class=\"line\">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class=\"line\"><span class=\"number\">0</span>: jdbc:hive2:<span class=\"comment\">//hadoop100:10000&gt; </span></span><br></pre></td></tr></table></figure>\n<p>使用 datagrip 连接</p>\n<h3 id=\"metastore\"><a class=\"markdownIt-Anchor\" href=\"#metastore\">#</a> metastore</h3>\n<p>Hive 的 metastore 服务的作用是为 Hive CLI 或者 Hiveserver2 提供元数据访问接口。</p>\n<p><strong>metastore 运行模式</strong></p>\n<p>metastore 有两种运行模式，分别为嵌入式模式和独立服务模式。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224157870.png\" alt=\"image-20240804224157870\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224212449.png\" alt=\"image-20240804224212449\"></p>\n<p>生产环境中，不推荐使用嵌入式模式。因为其存在以下两个问题：</p>\n<p>（1）嵌入式模式下，每个 Hive CLI 都需要直接连接元数据库，当 Hive CLI 较多时，数据库压力会比较大。</p>\n<p>（2）每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证。</p>\n<h4 id=\"独立模式部署\"><a class=\"markdownIt-Anchor\" href=\"#独立模式部署\">#</a> 独立模式部署</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hadoop100</span><br><span class=\"line\">nohup hive --service metastore &amp;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Hadoop101</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;thrift:<span class=\"comment\">//hadoop100:9083 &lt;/value&gt;</span></span><br><span class=\"line\">    &lt;description/&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">需要删除jdbc相关参数</span><br><span class=\"line\">hive -e <span class=\"string\">&quot;insert into stu values(2,&#x27;aaa&#x27;)&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">hive -f stu.sql</span><br><span class=\"line\"></span><br><span class=\"line\">hive -hiveconf mapreduce.job.x=<span class=\"number\">10</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">hive&gt; set mapreduce.x.x=<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">hive&gt; set mapreduce.x.x </span><br></pre></td></tr></table></figure>\n<p>配置文件 &lt; 命令行参数 &lt; 参数声明。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Whether to print the names of the columns in query output.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Whether to include the current database in the Hive prompt.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">修改日志路径</span><br><span class=\"line\"></span><br><span class=\"line\">vim hive-log4j2.properties</span><br><span class=\"line\"># list of properties</span><br><span class=\"line\">property.hive.log.level = INFO</span><br><span class=\"line\">property.hive.root.logger = DRFA</span><br><span class=\"line\">property.hive.log.dir = $&#123;sys:java.io.tmpdir&#125;/$&#123;sys:user.name&#125;</span><br><span class=\"line\">property.hive.log.file = hive.log</span><br><span class=\"line\">property.hive.perflogger.log.level = INFO</span><br><span class=\"line\"># 调整堆内存</span><br><span class=\"line\"></span><br><span class=\"line\">vim hive-env.sh</span><br><span class=\"line\">export HADOOP_HEAPSIZE=<span class=\"number\">2048</span></span><br><span class=\"line\">在yarn-site.xml中关闭虚拟内存检查(虚拟内存校验</span><br><span class=\"line\"></span><br><span class=\"line\">yarn-nodemanager.vmem-check-enabled </span><br><span class=\"line\"><span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"ddl\"><a class=\"markdownIt-Anchor\" href=\"#ddl\">#</a> DDL</h3>\n<h4 id=\"库操作\"><a class=\"markdownIt-Anchor\" href=\"#库操作\">#</a> 库操作</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">### databases</span><br><span class=\"line\">（<span class=\"number\">1</span>）创建一个数据库，不指定路径</span><br><span class=\"line\">hive (<span class=\"keyword\">default</span>)&gt; create database db_hive1;</span><br><span class=\"line\">注：若不指定路径，其默认路径为$&#123;hive.metastore.warehouse.dir&#125;/database_name.db</span><br><span class=\"line\">（<span class=\"number\">2</span>）创建一个数据库，指定路径</span><br><span class=\"line\">hive (<span class=\"keyword\">default</span>)&gt; create database db_hive2 location <span class=\"string\">&#x27;/db_hive2&#x27;</span>;</span><br><span class=\"line\">（<span class=\"number\">2</span>）创建一个数据库，带有dbproperties</span><br><span class=\"line\"><span class=\"title function_\">hive</span> <span class=\"params\">(<span class=\"keyword\">default</span>)</span>&gt; create database db_hive3 with <span class=\"title function_\">dbproperties</span><span class=\"params\">(<span class=\"string\">&#x27;create_date&#x27;</span>=<span class=\"string\">&#x27;2022-11-18&#x27;</span>)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">show databases like <span class=\"string\">&#x27;db_hive*&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">desc database db_hive3;</span><br><span class=\"line\">desc database extended db_hive3;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录。</span><br><span class=\"line\">ALTER DATABASE db_hive3 SET <span class=\"title function_\">DBPROPERTIES</span> <span class=\"params\">(<span class=\"string\">&#x27;create_date&#x27;</span>=<span class=\"string\">&#x27;2022-11-20&#x27;</span>)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE];</span><br><span class=\"line\">注：RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。</span><br><span class=\"line\">    CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。</span><br><span class=\"line\">    </span><br><span class=\"line\">use db1;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">### tables</span><br></pre></td></tr></table></figure>\n<h4 id=\"建表\"><a class=\"markdownIt-Anchor\" href=\"#建表\">#</a> 建表</h4>\n<p><strong>1TEMPORARY</strong></p>\n<p>临时表，该表只在当前会话可见，会话结束，表会被删除。</p>\n<p><strong>2EXTERNAL（重点）</strong></p>\n<p>外部表，与之相对应的是内部表（管理表）。管理表意味着 Hive 会完全接管该表，包括元数据和 HDFS 中的数据。而外部表则意味着 Hive 只接管元数据，而不完全接管 HDFS 中的数据。</p>\n<p><strong>3data_type（重点）</strong></p>\n<p>Hive 中的字段类型可分为基本数据类型和复杂数据类型。</p>\n<p>基本数据类型如下：</p>\n<table>\n<thead>\n<tr>\n<th>Hive</th>\n<th>说明</th>\n<th>定义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>tinyint</td>\n<td>1byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>smallint</td>\n<td>2byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>int</td>\n<td>4byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>bigint</td>\n<td>8byte 有符号整数</td>\n<td></td>\n</tr>\n<tr>\n<td>boolean</td>\n<td>布尔类型，true 或者 false</td>\n<td></td>\n</tr>\n<tr>\n<td>float</td>\n<td>单精度浮点数</td>\n<td></td>\n</tr>\n<tr>\n<td>double</td>\n<td>双精度浮点数</td>\n<td></td>\n</tr>\n<tr>\n<td>decimal</td>\n<td>十进制精准数字类型</td>\n<td>decimal(16,2)</td>\n</tr>\n<tr>\n<td>varchar</td>\n<td>字符序列，需指定最大长度，最大长度的范围是 [1,65535]</td>\n<td>varchar(32)</td>\n</tr>\n<tr>\n<td>string</td>\n<td>字符串，无需指定最大长度</td>\n<td></td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>时间类型</td>\n<td></td>\n</tr>\n<tr>\n<td>binary</td>\n<td>二进制数据</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>方式一：隐式转换</strong></p>\n<p>具体规则如下：</p>\n<p>a. 任何整数类型都可以隐式地转换为一个范围更广的类型，如 tinyint 可以转换成 int，int 可以转换成 bigint。</p>\n<p>b. 所有整数类型、float 和 string 类型都可以隐式地转换成 double。</p>\n<p>c. tinyint、smallint、int 都可以转换为 float。</p>\n<p>d. boolean 类型不可以转换为任何其它的类型。</p>\n<p><strong>方式二：显示转换</strong></p>\n<p>可以借助 cast 函数完成显示的类型转换</p>\n<p>a. 语法</p>\n<p>cast(expr as <type>)</p>\n<p>b. 案例</p>\n<p>hive (default)&gt; select ‘1’ + 2, cast(‘1’ as int) + 2;</p>\n<p>数据仓库 (英语：Data Warehouse, 简称数仓、DW), 是一个月用于存储、分析、报告的数据系统。</p>\n<p>数据仓库的目的是构建面向分析的集成化数据环境，分析结果为企业提供决策支持 (Decision Support)。</p>\n<p>关系型数据库 (RDBMS) 是 OLTP 典型应用，比如：Oracle、MySQL、SQL Server 等。</p>\n<p>面向分析、支持分析的系统称之为 OLAP (联机分析处理) 系统。数据仓库是 OLAP 一种。</p>\n<p>联机事务处理 OLTP (On-Line Transaction Processing)。</p>\n<p>传统 RDBMS，mysql，oracle，sqlserver</p>\n<p>联机分析处理 OLAP (On-LineAnalytical Processing)。</p>\n<p>数据仓库，主要用于开展数据分析</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224237355.png\" alt=\"image-20240804224237355\"></p>\n<p>数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。</p>\n<p>OLTP 系统的典型应用就是 RDBMS, 也就是我们俗称的数据库，当然这里要特别强调此数据库表示的是关系型数据库，</p>\n<p>Nosql 数据库并不在讨论范围内。</p>\n<p>OLAP 系统的典型应用就是 DW, 也就是我们俗称的数据仓库。</p>\n<p>数据仓库，数据集市</p>\n<p>数据仓库 (Data Warehouse) 是面向整个集团组织的数据，数据集市 (Data Mart) 是面向单个部门使用的。</p>\n<p>可以认为数据集市是数据仓库的子集，也有人把数据集市叫做小型数据仓库。数据集市通常只涉及一个主题领域，例如市场营销或销售。因为它们较小且更具体，所以它们通常更易于管理和维护，并具有更灵活的结构。</p>\n<p>下图中，各种操作型系统数据和包括文件在内的等其他数据作为数据源，经过 ETL (抽取转换加载) 填充到数据仓库中；数据仓库中有不同主题数据，数据集市则根据部门特点面向指定主题，比如 Purchasing (采购)、Sales 售)、Inventory (库存);</p>\n<p>用户可以基于主题数据开展各种应用：数据分析、数据报表、数据挖掘。</p>\n<p>数据仓库分层思想</p>\n<p>ETL ELT</p>\n<h2 id=\"flink\"><a class=\"markdownIt-Anchor\" href=\"#flink\">#</a> flink</h2>\n<p>Apache Flink 是一个框架和分布式处理引擎，用于对无异和有界数据流进行有状态计算。</p>\n<p>1) 无界数据流:</p>\n<p>有定义流的开始，但没有定义流的结束；</p>\n<p>它们会无休止的产生数据；</p>\n<p>无界流的数据必须持续处理，即数据被摄取后需要立刻处上理。</p>\n<p>我们不能等到所有数据都到达再处理，因为输入是无限的。</p>\n<p>2) 有界数据流:</p>\n<p>有定义流的开始，也有定义流的结束；</p>\n<p>有界流可以在摄取所有数据后再进行计算；</p>\n<p>有界流所有数据可以被排序，所以并不需要有序摄取；</p>\n<p>有界流处理通常被称为批处理。</p>\n<p>把流处理需要的额外数据保存成一个 &quot;状态&quot;, 然户后针对这条数据进行处理，并且更新状态。这就是所谓的 &quot;有状态的流处理&quot;。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224253261.png\" alt=\"image-20240804224253261\"></p>\n<h3 id=\"wordcount\"><a class=\"markdownIt-Anchor\" href=\"#wordcount\">#</a> wordcount</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt;</span><br><span class=\"line\">        &lt;flink.version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/flink.version&gt;</span><br><span class=\"line\">&lt;/properties&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;flink-streaming-java&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">     &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;flink-clients&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class=\"line\">     &lt;/dependency&gt;</span><br><span class=\"line\">&lt;/dependencies&gt;</span><br><span class=\"line\"><span class=\"comment\">// 批处理</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.AggregateOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.FlatMapOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.operators.UnsortedGrouping;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">count</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">ExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        DataSource&lt;String&gt; stringDataSource = env.readTextFile(<span class=\"string\">&quot;data/input&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        FlatMapOperator&lt;String, Tuple2&lt;String, Integer&gt;&gt; stringTuple2FlatMapOperator = stringDataSource.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">flatMap</span><span class=\"params\">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                String[] s1 = s.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (String ss : s1) &#123;</span><br><span class=\"line\">                    Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = Tuple2.of(ss, <span class=\"number\">1</span>);</span><br><span class=\"line\">                    collector.collect(stringIntegerTuple2);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        UnsortedGrouping&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2UnsortedGrouping = stringTuple2FlatMapOperator.groupBy(<span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2UnsortedGrouping.sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 流处理</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.KeyedStream;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">count2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">executionEnvironment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.readTextFile(<span class=\"string\">&quot;data/input&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2SingleOutputStreamOperator = stringDataStreamSource.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">flatMap</span><span class=\"params\">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                String[] s1 = s.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (String string : s1) &#123;</span><br><span class=\"line\">                    Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = Tuple2.of(string, <span class=\"number\">1</span>);</span><br><span class=\"line\">                    collector.collect(stringIntegerTuple2);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2StringKeyedStream = tuple2SingleOutputStreamOperator.keyBy(<span class=\"keyword\">new</span> <span class=\"title class_\">KeySelector</span>&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">getKey</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> stringIntegerTuple2.f0;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2StringKeyedStream.sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\">        </span><br><span class=\"line\">        executionEnvironment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// socket</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"flink-部署\"><a class=\"markdownIt-Anchor\" href=\"#flink-部署\">#</a> flink 部署</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">客户端(Client):代码由客户端获取并做转换,之后提交给JolManger</span><br><span class=\"line\">JobManager就是Flink集群里的<span class=\"string\">&quot;管事人&quot;</span>,对作业进行中央调度管理;而它获取到要执行的作业后,会进一步处理转换,然后分发任务给众多的TaskManager。</span><br><span class=\"line\">TaskManager,就是真正<span class=\"string\">&quot;干活的人&quot;</span>,数据的处理操作都是它它们来做的。</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224309842.png\" alt=\"image-20240804224309842\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget <span class=\"string\">&quot;https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz&quot;</span></span><br><span class=\"line\">tar xzvf flink-<span class=\"number\">1.17</span><span class=\"number\">.2</span>-bin-scala_2<span class=\"number\">.12</span>.tgz</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim flink-conf.yaml</span><br><span class=\"line\">jobmanager.rpc.address: hadoop100</span><br><span class=\"line\">jobmanager.rpc.port: <span class=\"number\">6123</span></span><br><span class=\"line\">jobmanager.bind-host: <span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">rest.address: hadoop100</span><br><span class=\"line\">rest.bind-address: <span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">taskmanager.bind-host: <span class=\"number\">0.0</span><span class=\"number\">.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">taskmanager.host: hadoop100</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim worker</span><br><span class=\"line\">hadoop100</span><br><span class=\"line\">hadoop101</span><br><span class=\"line\">hadoop102</span><br><span class=\"line\"></span><br><span class=\"line\">vim master</span><br><span class=\"line\">hadoop100:<span class=\"number\">8081</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">分发到其他节点，修改flink-conf</span><br><span class=\"line\"></span><br><span class=\"line\">taskmanager.host: hadoop10x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 启动</span><br><span class=\"line\"> start-cluster.sh </span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# jps </span><br><span class=\"line\"><span class=\"number\">140450</span> StandaloneSessionClusterEntrypoint</span><br><span class=\"line\"><span class=\"number\">140861</span> TaskManagerRunner</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">http:<span class=\"comment\">//hadoop100:8081/#/overview</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"web提交任务\"><a class=\"markdownIt-Anchor\" href=\"#web提交任务\">#</a> web 提交任务</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Arrays;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SocketStreamWordCount</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 1. 创建流式执行环境</span></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 2. 读取文本流：hadoop102表示发送端主机名、7777表示端口号</span></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; lineStream = env.socketTextStream(<span class=\"string\">&quot;hadoop100&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 3. 转换、分组、求和，得到统计结果</span></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = lineStream.flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">                    String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                        out.collect(Tuple2.of(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;).returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class=\"line\">                .keyBy(data -&gt; data.f0)</span><br><span class=\"line\">                .sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 4. 打印</span></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 5. 执行</span></span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"># 增加依赖</span><br><span class=\"line\"> &lt;build&gt;</span><br><span class=\"line\">        &lt;plugins&gt;</span><br><span class=\"line\">            &lt;plugin&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;<span class=\"number\">3.2</span><span class=\"number\">.4</span>&lt;/version&gt;</span><br><span class=\"line\">                &lt;executions&gt;</span><br><span class=\"line\">                    &lt;execution&gt;</span><br><span class=\"line\">                        &lt;phase&gt;<span class=\"keyword\">package</span>&lt;/phase&gt;</span><br><span class=\"line\">                        &lt;goals&gt;</span><br><span class=\"line\">                            &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class=\"line\">                        &lt;/goals&gt;</span><br><span class=\"line\">                        &lt;configuration&gt;</span><br><span class=\"line\">                            &lt;artifactSet&gt;</span><br><span class=\"line\">                                &lt;excludes&gt;</span><br><span class=\"line\">                                    &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt;</span><br><span class=\"line\">                                    &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt;</span><br><span class=\"line\">                                    &lt;exclude&gt;log4j:*&lt;/exclude&gt;</span><br><span class=\"line\">                                &lt;/excludes&gt;</span><br><span class=\"line\">                            &lt;/artifactSet&gt;</span><br><span class=\"line\">                            &lt;filters&gt;</span><br><span class=\"line\">                                &lt;filter&gt;</span><br><span class=\"line\">                                    &lt;!-- Do not copy the signatures in the META-INF folder.</span><br><span class=\"line\">                                    Otherwise, <span class=\"built_in\">this</span> might cause SecurityExceptions when using the JAR. --&gt;</span><br><span class=\"line\">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class=\"line\">                                    &lt;excludes&gt;</span><br><span class=\"line\">                                        &lt;exclude&gt;META-INF<span class=\"comment\">/*.SF&lt;/exclude&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                    &lt;/excludes&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                &lt;/filter&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                            &lt;/filters&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                            &lt;transformers combine.children=&quot;append&quot;&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                &lt;transformer</span></span><br><span class=\"line\"><span class=\"comment\">                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                                &lt;/transformer&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                            &lt;/transformers&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                        &lt;/configuration&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                    &lt;/execution&gt;</span></span><br><span class=\"line\"><span class=\"comment\">                &lt;/executions&gt;</span></span><br><span class=\"line\"><span class=\"comment\">            &lt;/plugin&gt;</span></span><br><span class=\"line\"><span class=\"comment\">        &lt;/plugins&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;/build&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">    </span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  可以在前面的依赖中添加</span><br><span class=\"line\">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class=\"line\">  来不打包依赖</span><br><span class=\"line\">  </span><br><span class=\"line\">  添加后需要在edit configuration中include provided，否则idea中无法编译</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224322601.png\" alt=\"image-20240804224322601\"></p>\n<p>创建任务，submit</p>\n<p>nc -lvvp 7777 发送数据查看效果</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224334662.png\" alt=\"image-20240804224334662\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224343308.png\" alt=\"image-20240804224343308\"></p>\n<p>命令行提交作业</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run -m hadoop100:<span class=\"number\">8081</span> -c wordcount.SocketStreamWordCount ./xxx.jar</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署模式\"><a class=\"markdownIt-Anchor\" href=\"#部署模式\">#</a> 部署模式</h3>\n<p>Flink 为各种场景提供了不同的部署模式，主要有以下三种：会话模式（Session Mode）、单作业模式（Per-Job Mode）、应用模式（Application Mode）。</p>\n<p>它们的区别主要在于：集群的生命周期以及资源的分配方式；以及应用的 main 方法到底在哪里执行 —— 客户端（Client）还是 JobManager。</p>\n<h4 id=\"会话模式\"><a class=\"markdownIt-Anchor\" href=\"#会话模式\">#</a> 会话模式</h4>\n<p>会话模式其实最符合常规思维。我们需要先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源就都已经确定，所以所有提交的作业会竞争集群中的资源。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224355062.png\" alt=\"image-20240804224355062\"></p>\n<h4 id=\"单作业模式\"><a class=\"markdownIt-Anchor\" href=\"#单作业模式\">#</a> 单作业模式</h4>\n<p>会话模式因为资源共享会导致很多问题，所以为了更好地隔离资源，我们可以考虑为每个提交的作业启动一个集群，这就是所谓的单作业 (Per-Job) 模式。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224407159.png\" alt=\"image-20240804224407159\"></p>\n<p>需要注意的是，Flink 本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理框架来启动集群，比如 YARN、Kubernetes (K8S)。</p>\n<h4 id=\"应用模式\"><a class=\"markdownIt-Anchor\" href=\"#应用模式\">#</a> 应用模式</h4>\n<p>直接把应用提交到 JobManger 上运行。而这也就代表着，我们需要为每一个提交的应用单独启动一个 JobManager, 也就是创建一个集群。这个 JobManager 只为执行这一个应用而存在，执行结束之后 JobManager 也就关闭了，这就是所谓的应用模式。</p>\n<p>应用模式与单作业模式，都是提交作业之后才创建集群；单单作业模式是通过客户端来提交的，客户端解析出的每一个作业对应一个集群；而应用模式下，是直接由 JobManager 执行应用程序的。</p>\n<h4 id=\"standlone-模式\"><a class=\"markdownIt-Anchor\" href=\"#standlone-模式\">#</a> standlone 模式</h4>\n<p>独立模式是独立运行的，不依赖任何外部的资源管理平台；当然独立也是有代价的：如果资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理。所以独立模式一般只用在开发测试或作业非常少的场景下。</p>\n<p>会话模式部署</p>\n<p>提前启动集群，并通过 Web 页面客户端提交任务（可以多个任务，但是集群资源固定）。</p>\n<p>standlone 不支持单作业部署</p>\n<p>应用模式部署</p>\n<p>启动 jobmanager</p>\n<p>bin/standalone-job.sh start --job-classname com.atguigu.wc.SocketStreamWordCount</p>\n<p>启动 TaskManager, 需要在每个 taskmanager 上启动</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3Rhc2ttYW5hZ2VyLnNo\">taskmanager.sh</span> start</p>\n<p>停止</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3Rhc2ttYW5hZ2VyLnNo\">taskmanager.sh</span> stop</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3N0YW5kYWxvbmUtam9iLnNo\">standalone-job.sh</span> stop</p>\n<h4 id=\"yarn模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn模式\">#</a> yarn 模式</h4>\n<p>客户端把 Flink 应用提交给 Yarn 的 ResourceManager，Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署 JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配 TaskManager 资源。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim /etc/profile</span><br><span class=\"line\"># hadoop</span><br><span class=\"line\">export HADOOP_HOME=/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span></span><br><span class=\"line\">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class=\"line\">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class=\"line\">export HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure>\n<h5 id=\"yarn-会话模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn-会话模式\">#</a> yarn - 会话模式</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yarn-session.sh -d -nm name111</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 提交任务</span><br><span class=\"line\">flink run -c com.wordcount lib/<span class=\"number\">1.</span>jar </span><br></pre></td></tr></table></figure>\n<h5 id=\"yarn-单作业模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn-单作业模式\">#</a> yarn - 单作业模式</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run -d -t yarn-per-job -c com.wordcount lib/<span class=\"number\">1.</span>jar</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/// 停止</span></span><br><span class=\"line\">bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class=\"line\"> </span><br><span class=\"line\">bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br><span class=\"line\">消除classloader告警</span><br><span class=\"line\">vim flink-conf.yaml</span><br><span class=\"line\"> </span><br><span class=\"line\">classloader.check-leaked-classloader: <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<h5 id=\"yarn-应用模式部署\"><a class=\"markdownIt-Anchor\" href=\"#yarn-应用模式部署\">#</a> yarn - 应用模式部署</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-<span class=\"number\">1.0</span>-SNAPSHOT.jar </span><br><span class=\"line\">上传HDFS提交</span><br><span class=\"line\">可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。</span><br><span class=\"line\">（<span class=\"number\">1</span>）上传flink的lib和plugins到HDFS上</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -mkdir /flink-dist</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -put lib/ /flink-dist</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -put plugins/ /flink-dist</span><br><span class=\"line\">（<span class=\"number\">2</span>）上传自己的jar包到HDFS</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -mkdir /flink-jars</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ hadoop fs -put FlinkTutorial-<span class=\"number\">1.0</span>-SNAPSHOT.jar /flink-jars</span><br><span class=\"line\">（<span class=\"number\">3</span>）提交作业</span><br><span class=\"line\">[atguigu<span class=\"meta\">@hadoop102</span> flink-<span class=\"number\">1.17</span><span class=\"number\">.0</span>]$ bin/flink run-application -t yarn-application        -Dyarn.provided.lib.dirs=<span class=\"string\">&quot;hdfs://hadoop102:8020/flink-dist&quot;</span>        -c com.atguigu.wc.SocketStreamWordCount  hdfs:<span class=\"comment\">//hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span></span><br></pre></td></tr></table></figure>\n<p>配置历史服务器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jobmanager.archive.fs.dir: hdfs:<span class=\"comment\">//hadoop100:8020/logs</span></span><br><span class=\"line\"></span><br><span class=\"line\"># The address under which the web-based HistoryServer listens.</span><br><span class=\"line\">historyserver.web.address: hadoop100</span><br><span class=\"line\"></span><br><span class=\"line\"># The port under which the web-based HistoryServer listens.</span><br><span class=\"line\">historyserver.web.port: <span class=\"number\">8082</span></span><br><span class=\"line\"></span><br><span class=\"line\"># Comma separated list of directories to monitor <span class=\"keyword\">for</span> completed jobs.</span><br><span class=\"line\">historyserver.archive.fs.dir: hdfs:<span class=\"comment\">//hadoop100:8020/logs</span></span><br><span class=\"line\"></span><br><span class=\"line\"># Interval in milliseconds <span class=\"keyword\">for</span> refreshing the monitored directories.</span><br><span class=\"line\">historyserver.archive.fs.refresh-interval: <span class=\"number\">10000</span></span><br><span class=\"line\"></span><br><span class=\"line\">historyserver.sh start </span><br></pre></td></tr></table></figure>\n<h3 id=\"系统架构\"><a class=\"markdownIt-Anchor\" href=\"#系统架构\">#</a> 系统架构</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224432790.png\" alt=\"image-20240804224432790\"></p>\n<h4 id=\"并行度\"><a class=\"markdownIt-Anchor\" href=\"#并行度\">#</a> 并行度</h4>\n<p>一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。这样，包含并行子任务的数据流，就是并行数据流，它需要多个分区（stream partition）来分配并行任务。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224445616.png\" alt=\"image-20240804224445616\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;flink-runtime-web&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SocketStreamWordCount</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 1. 创建流式执行环境</span></span><br><span class=\"line\">        <span class=\"comment\">//StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>());</span><br><span class=\"line\">        <span class=\"comment\">// 2. 读取文本流：hadoop102表示发送端主机名、7777表示端口号</span></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; lineStream = env.socketTextStream(<span class=\"string\">&quot;hadoop100&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 3. 转换、分组、求和，得到统计结果</span></span><br><span class=\"line\">        elineStream.flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">                    String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                        out.collect(Tuple2.of(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;).setParallelism(<span class=\"number\">2</span>)</span><br><span class=\"line\">                .returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class=\"line\">                .keyBy(data -&gt; data.f0)</span><br><span class=\"line\">                .sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 4. 打印</span></span><br><span class=\"line\">        sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 5. 执行</span></span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// idea中，不指定并行度，默认就是电脑的线程数</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 全局设置并行度</span></span><br><span class=\"line\">        env.setParallelism(<span class=\"number\">3</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">算子优先级更高 &gt;  env   &gt;  -p  &gt; flink-conf</span><br><span class=\"line\"></span><br><span class=\"line\">提交作业时指定 命令行启动 参数 -p <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">配置文件</span><br><span class=\"line\">flink-conf.yaml</span><br><span class=\"line\">parallelism.<span class=\"keyword\">default</span>: <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"算子链\"><a class=\"markdownIt-Anchor\" href=\"#算子链\">#</a> 算子链</h4>\n<p><strong>（1）一对一（One-to-one，forwarding）</strong></p>\n<p>这种模式下，数据流维护着分区以及元素的顺序。比如图中的 source 和 map 算子，source 算子读取数据之后，可以直接发送给 map 算子做处理，它们之间不需要重新分区，也不需要调整数据的顺序。这就意味着 map 算子的子任务，看到的元素个数和顺序跟 source 算子的子任务产生的完全一样，保证着 “一对一” 的关系。map、filter、flatMap 等算子都是这种 one-to-one 的对应关系。这种关系类似于 Spark 中的窄依赖。</p>\n<p><strong>（2）重分区（Redistributing）</strong></p>\n<p>在这种模式下，数据流的分区会发生改变。比如图中的 map 和后面的 keyBy/window 算子之间，以及 keyBy/window 算子和 Sink 算子之间，都是这样的关系。</p>\n<p>每一个算子的子任务，会根据数据传输的策略，把数据发送到不同的下游目标任务。这些传输方式都会引起重分区的过程，这一过程类似于 Spark 中的 shuffle。</p>\n<p>在 Flink 中，并行度相同的一对一（one to one）算子操作，可以直接链接在一起形成一个 “大” 的任务（task）</p>\n<p>将算子链接成 task 是非常有效的优化：可以减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 全局禁用算子链</span><br><span class=\"line\">env.disableOperatorChaining();</span><br><span class=\"line\"></span><br><span class=\"line\"># 对某个算子禁用算子链</span><br><span class=\"line\">.sum(<span class=\"number\">1</span>).disableChaining();</span><br><span class=\"line\"></span><br><span class=\"line\">禁用算子链之后，只有这个算子相邻算子不能和自己链</span><br><span class=\"line\"></span><br><span class=\"line\"># 从某个算子开启新链条</span><br><span class=\"line\">.sum(<span class=\"number\">1</span>).startNewChain();</span><br><span class=\"line\">算子不与前面链，从当前开始正常链</span><br></pre></td></tr></table></figure>\n<h4 id=\"任务槽\"><a class=\"markdownIt-Anchor\" href=\"#任务槽\">#</a> 任务槽</h4>\n<p>每个任务槽（task slot）其实表示了 TaskManager 拥有计算资源的一个固定大小的子集。这些资源就是用来独立执行一个子任务的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 每个tm slot数，默认是<span class=\"number\">1</span></span><br><span class=\"line\">taskmanager.numberOfTaskSlots: <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>slot 目前仅仅用来隔离内存，不会涉及 CPU 的隔离。在具体应用时，可以将 slot 数量配置为机器的 CPU 核心数，尽量避免不同任务之间对 CPU 的竞争。这也是开发环境默认并行度设为机器 CPU 数量的原因。</p>\n<p>同一个 job 中，不同算子的子任务，才可以共享一个 slot，slot 内是同时在运行的</p>\n<p>属于同一个 slot 共享组，默认是 default</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224459615.png\" alt=\"image-20240804224459615\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.sum(<span class=\"number\">1</span>).slotSharingGroup(<span class=\"string\">&quot;aaa&quot;</span>);</span><br></pre></td></tr></table></figure>\n<p>slot 数量 与 并行度 的关系</p>\n<p>1) slot 是一种静态的概念，表示最大的并发上限</p>\n<p>并行度是一种动态的概念，表示实际运行 占用了几个</p>\n<p>2) 要求:slot 数量 &gt;=job 并行度 (算子最大并行度),job 才能运行</p>\n<p>yarn 模式是动态申请</p>\n<p>申请的 TM 数量 = job 并行度 / 每个 TM 的 slot 数，向上取整</p>\n<h4 id=\"作业提交流程\"><a class=\"markdownIt-Anchor\" href=\"#作业提交流程\">#</a> 作业提交流程</h4>\n<h5 id=\"standalone\"><a class=\"markdownIt-Anchor\" href=\"#standalone\">#</a> standalone</h5>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224512232.png\" alt=\"image-20240804224512232\"></p>\n<p>逻辑流图（StreamGraph）→ 作业图（JobGraph）→ 执行图（ExecutionGraph）→ 物理图（Physical Graph）</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224530055.png\" alt=\"image-20240804224530055\"></p>\n<p><img data-src=\"https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ODUxMzFkZWQ3OWQyMTFjMjhlZWY1NWM3MWQzNzk0YWZfVXM3M2gxRkdSeEFvMWQ4ZWJ6N1pscHplUFZFOTVJVWhfVG9rZW46VWR4WmJTUUJzb0c0MFp4eTdxeWNzVGhrbllnXzE3MjI3ODE5MDk6MTcyMjc4NTUwOV9WNA\" alt=\"img\"></p>\n<h5 id=\"yarn\"><a class=\"markdownIt-Anchor\" href=\"#yarn\">#</a> yarn</h5>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224551577.png\" alt=\"image-20240804224551577\"></p>\n<h4 id=\"datastream-api\"><a class=\"markdownIt-Anchor\" href=\"#datastream-api\">#</a> datastream api</h4>\n<h5 id=\"执行环境\"><a class=\"markdownIt-Anchor\" href=\"#执行环境\">#</a> 执行环境</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">Configuration</span> <span class=\"variable\">configuration</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">    configuration.set(RestOptions.BIND_PORT, <span class=\"string\">&quot;8082&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 自动识别环境是远程还是本地</span></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 流批一体，同一套API，默认STREAMING</span></span><br><span class=\"line\">    <span class=\"comment\">// 一般不在代码写死，再命令行参数指定 -Dexecution.runtime-mode=BATCH</span></span><br><span class=\"line\">    environment.setRuntimeMode(RuntimeExecutionMode.STREAMING);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = environment.socketTextStream(<span class=\"string\">&quot;127.0.0.1&quot;</span>, <span class=\"number\">7777</span>).flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">                String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                    out.collect(Tuple2.of(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;)</span><br><span class=\"line\">            .returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class=\"line\">            .keyBy(data -&gt; data.f0)</span><br><span class=\"line\">            .sum(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    sum.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 触发执行 flink job</span></span><br><span class=\"line\">    <span class=\"comment\">// Flink是由事件驱动的,只有等等到数据到来,才会触发真正的计算,这也被称为&quot;延迟执行&quot;或&quot;懒执行&quot;。</span></span><br><span class=\"line\">    <span class=\"comment\">// 一个main调用多个exec，会在第一个阻塞住</span></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 异步执行，不会阻塞，execsync个数 = 生成flink job数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// yarn-application集群,提交一次,集群里会有几个flink job?</span></span><br><span class=\"line\">    <span class=\"comment\">//取决于调用了n个executeAsync()</span></span><br><span class=\"line\">    <span class=\"comment\">//对应application集群里,会有n个job</span></span><br><span class=\"line\">    <span class=\"comment\">//对应Jobmanager当中,会有 n个JobMaster</span></span><br><span class=\"line\">    environment.executeAsync();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"读取数据\"><a class=\"markdownIt-Anchor\" href=\"#读取数据\">#</a> 读取数据</h5>\n<h6 id=\"从集合读取数据\"><a class=\"markdownIt-Anchor\" href=\"#从集合读取数据\">#</a> 从集合读取数据</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">Configuration</span> <span class=\"variable\">configuration</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">    configuration.set(RestOptions.BIND_PORT, <span class=\"string\">&quot;8082&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 自动识别环境是远程还是本地</span></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromCollection(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>));</span><br><span class=\"line\">    DataStreamSource&lt;Integer&gt; dataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    integerDataStreamSource.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"文件\"><a class=\"markdownIt-Anchor\" href=\"#文件\">#</a> 文件</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-files&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">FileSource&lt;String&gt; build = FileSource.forRecordStreamFormat(<span class=\"keyword\">new</span> <span class=\"title class_\">TextLineInputFormat</span>(), <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(<span class=\"string\">&quot;input/1.txt&quot;</span>)).build();</span><br><span class=\"line\"></span><br><span class=\"line\">DataStreamSink&lt;String&gt; aaaa = environment.fromSource(build, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;aaaa&quot;</span>).print();</span><br></pre></td></tr></table></figure>\n<h6 id=\"kafka\"><a class=\"markdownIt-Anchor\" href=\"#kafka\">#</a> kafka</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">1.17</span><span class=\"number\">.0</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">KafkaSource&lt;String&gt; build = KafkaSource.&lt;String&gt;builder().setBootstrapServers(<span class=\"string\">&quot;hadoop100:9092&quot;</span>).setGroupId(<span class=\"string\">&quot;aaa&quot;</span>).setTopics(<span class=\"string\">&quot;topic_haha&quot;</span>).setValueOnlyDeserializer(<span class=\"keyword\">new</span> <span class=\"title class_\">SimpleStringSchema</span>()).setStartingOffsets(OffsetsInitializer.latest()).build();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">environment.fromSource(build, WatermarkStrategy.noWatermarks(),<span class=\"string\">&quot;kafka&quot;</span>);</span><br></pre></td></tr></table></figure>\n<h6 id=\"数据生成器读取\"><a class=\"markdownIt-Anchor\" href=\"#数据生成器读取\">#</a> 数据生成器读取</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\">environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// GeneratorFunction接口重新long方法</span></span><br><span class=\"line\"><span class=\"comment\">// Long 类型 自动生成数字序列的最大值</span></span><br><span class=\"line\"><span class=\"comment\">// 限速策略</span></span><br><span class=\"line\"><span class=\"comment\">// 返回类型</span></span><br><span class=\"line\">DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 如果有n个并行度,最大值设为a</span></span><br><span class=\"line\"><span class=\"comment\">//将数值均分成n份,</span></span><br><span class=\"line\"><span class=\"comment\">//其中一个是0-49</span></span><br><span class=\"line\"><span class=\"comment\">//a/n,比如,最大100,并行度2,每个并行度生成50个</span></span><br><span class=\"line\">environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(),<span class=\"string\">&quot;datagen&quot;</span>).print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">environment.execute();</span><br></pre></td></tr></table></figure>\n<h5 id=\"flink-数据类型\"><a class=\"markdownIt-Anchor\" href=\"#flink-数据类型\">#</a> flink 数据类型</h5>\n<p>Flink 使用 “类型信息”（TypeInformation）来统一表示数据类型。TypeInformation 类是 Flink 中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p>\n<blockquote>\n<p>泛型类型（GENERIC）</p>\n<p>Flink 支持所有的 Java 类和 Scala 类。不过如果没有按照上面 POJO 类型的要求来定义，就会被 Flink 当作泛型类来处理。Flink 会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由 Flink 本身序列化的，而是由 Kryo 序列化的。</p>\n<p>在这些类型中，元组类型和 POJO 类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO 还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为 Flink 的 POJO 类型。</p>\n<p>Flink 对 POJO 类型的要求如下：</p>\n<ul>\n<li>类是公有（public）的</li>\n<li>有一个无参的构造方法</li>\n<li>所有属性都是公有（public）的</li>\n<li>所有属性的类型都是可以序列化的</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>类型提示（Type Hints）</strong></p>\n<p>Flink 还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于 Java 中泛型擦除的存在，在某些特殊情况下（比如 Lambda 表达式中），自动提取的信息是不够精细的 —— 只告诉 Flink 当前的元素由 “船头、船身、船尾” 构成，根本无法重建出 “大船” 的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p>\n<p>为了解决这类问题，Java API 提供了专门的 “类型提示”（type hints）。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.returns(<span class=\"keyword\">new</span> <span class=\"title class_\">TypeHint</span>&lt;Tuple2&lt;Integer, SomeType&gt;&gt;()&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">.returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"转换算子\"><a class=\"markdownIt-Anchor\" href=\"#转换算子\">#</a> 转换算子</h4>\n<h5 id=\"map\"><a class=\"markdownIt-Anchor\" href=\"#map\">#</a> map</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment(configuration);</span><br><span class=\"line\">environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Object&gt; map = integerDataStreamSource.map(<span class=\"keyword\">new</span> <span class=\"title class_\">MapFunction</span>&lt;Integer, Object&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Object <span class=\"title function_\">map</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> integer;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">map.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Integer&gt; map1 = integerDataStreamSource.map(num -&gt; num * <span class=\"number\">2</span>);</span><br></pre></td></tr></table></figure>\n<h5 id=\"fliter-flatmap\"><a class=\"markdownIt-Anchor\" href=\"#fliter-flatmap\">#</a> fliter \\ flatmap</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Integer&gt; filter = integerDataStreamSource.filter(<span class=\"keyword\">new</span> <span class=\"title class_\">FilterFunction</span>&lt;Integer&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">filter</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// flatmap 可以一进一出，一进多出，一进0出</span></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Object&gt; objectSingleOutputStreamOperator = integerDataStreamSource.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;Integer, Object&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">flatMap</span><span class=\"params\">(Integer integer, Collector&lt;Object&gt; collector)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        collector.collect(integer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">objectSingleOutputStreamOperator.print();</span><br></pre></td></tr></table></figure>\n<script>alert(1)</script>\n<h5 id=\"keyby\"><a class=\"markdownIt-Anchor\" href=\"#keyby\">#</a> keyby</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// keyby 按照id分组，，返回键控流，不是转换算子，只是对数据重分区，不能设置并行度</span></span><br><span class=\"line\"><span class=\"comment\">// 3、keyby分组与分组 与 分区 的关系:</span></span><br><span class=\"line\"><span class=\"comment\">//1) keyby是对数据分组,保证 相同key的数据在同一个分区</span></span><br><span class=\"line\"><span class=\"comment\">//2)分区:一个子任务可以理解为一个分区,一个分区(子任务)中可以存在多个分组(key)</span></span><br><span class=\"line\">KeyedStream&lt;Integer, Tuple&gt; integerObjectKeyedStream = integerDataStreamSource.keyBy(String.valueOf(<span class=\"keyword\">new</span> <span class=\"title class_\">KeySelector</span>&lt;Integer, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">getKey</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;));</span><br></pre></td></tr></table></figure>\n<h5 id=\"简单聚合算子\"><a class=\"markdownIt-Anchor\" href=\"#简单聚合算子\">#</a> 简单聚合算子</h5>\n<p>keyby 之后才能调用</p>\n<p>做分组内聚合，对同一个 key 数据聚合</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum(<span class=\"string\">&quot;title&quot;</span>);  </span><br><span class=\"line\"><span class=\"comment\">// 传位置索引适用于tuple类型</span></span><br><span class=\"line\">min(<span class=\"string\">&quot;vc&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// max 只回取比较字段最大值，非比较字段取第一次的值</span></span><br><span class=\"line\"><span class=\"comment\">// maxby只回取比较字段最大值，非比较字段取当前字段的值</span></span><br><span class=\"line\">max()</span><br><span class=\"line\">maxby()</span><br></pre></td></tr></table></figure>\n<h5 id=\"reduce\"><a class=\"markdownIt-Anchor\" href=\"#reduce\">#</a> reduce</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">、keyby之后调用</span><br><span class=\"line\">、输入类型=输出类型,类型不能变</span><br><span class=\"line\">、每个key的第一条数据来的时候,不会执行reduce方法,在起来,直接输出</span><br><span class=\"line\">reduce 方法中的参数，value1之前计算结果  value2 现在来的数据</span><br></pre></td></tr></table></figure>\n<h5 id=\"udf\"><a class=\"markdownIt-Anchor\" href=\"#udf\">#</a> UDF</h5>\n<p>Flink 暴露了所有 UDF 函数的接口，具体实现方式为接口或者抽象类，例如 MapFunction、FilterFunction、ReduceFunction 等。所以用户可以自定义一个函数类，实现对应的接口</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  DataStream&lt;String&gt; filter = stream.filter(<span class=\"keyword\">new</span> <span class=\"title class_\">UserFilter</span>());</span><br><span class=\"line\">      </span><br><span class=\"line\">        filter.print();</span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UserFilter</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">FilterFunction</span>&lt;WaterSensor&gt; &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">filter</span><span class=\"params\">(WaterSensor e)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> e.id.equals(<span class=\"string\">&quot;sensor_1&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"richxxxfunction\"><a class=\"markdownIt-Anchor\" href=\"#richxxxfunction\">#</a> RichXXXFunction</h5>\n<p>多了生命周期管理方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">............多亏你申请</span><br><span class=\"line\">open():每个子任务,在启动时,调用一次</span><br><span class=\"line\">close():每个子任务,在结束时,调用一次</span><br></pre></td></tr></table></figure>\n<p>多了运行时上下文，可以获取运行时环境信息，比如子任务编号，名称</p>\n<p>无界流，如果 flink 程序异常终止，不会调用 close</p>\n<p>正常调用 web ui cancel，会调用 close</p>\n<h5 id=\"分区算子\"><a class=\"markdownIt-Anchor\" href=\"#分区算子\">#</a> 分区算子</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        Configuration configuration = new Configuration();</span></span><br><span class=\"line\"><span class=\"comment\">//        configuration.set(RestOptions.BIND_PORT, &quot;8082&quot;);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 随机分区</span></span><br><span class=\"line\">        <span class=\"comment\">// localhost.shuffle().print();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// rebalanced 轮询</span></span><br><span class=\"line\">        <span class=\"comment\">// 数据源倾斜场景，source读进来后，调用rebalance可以解决</span></span><br><span class=\"line\">        <span class=\"comment\">//localhost.rebalance().print();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// rescale 缩放</span></span><br><span class=\"line\">        <span class=\"comment\">// 局部组队，比rebalance更高效</span></span><br><span class=\"line\">        localhost.rescale().print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//</span></span><br><span class=\"line\">        localhost.broadcast().print();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将所有数据发到一个子任务</span></span><br><span class=\"line\">        localhost.global().print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// keyby:按指定key去发送,相同key发往同一个子任务</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"自定义分区器\"><a class=\"markdownIt-Anchor\" href=\"#自定义分区器\">#</a> 自定义分区器</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyPart</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Partitioner</span>&lt;String&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">partition</span><span class=\"params\">(String s, <span class=\"type\">int</span> i)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Integer.parseInt(s) % i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">custompartption</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;String&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        localhost.partitionCustom(<span class=\"keyword\">new</span> <span class=\"title class_\">MyPart</span>(),num -&gt; num).print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"分流\"><a class=\"markdownIt-Anchor\" href=\"#分流\">#</a> 分流</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1.</span>使用process算子</span><br><span class=\"line\"><span class=\"number\">2.</span>定义outputtag对象</span><br><span class=\"line\"><span class=\"number\">3.</span>调用ctx.output</span><br><span class=\"line\"><span class=\"number\">4.</span>通过主流获取测流</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SplitStreamByOutputTag</span> &#123;    </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"> </span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class=\"string\">&quot;hadoop102&quot;</span>, <span class=\"number\">7777</span>)</span><br><span class=\"line\">              .map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">        OutputTag&lt;WaterSensor&gt; s1 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;s1&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class=\"line\">        OutputTag&lt;WaterSensor&gt; s2 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;s2&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class=\"line\">       <span class=\"comment\">//返回的都是主流</span></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessFunction</span>&lt;WaterSensor, WaterSensor&gt;()</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">                <span class=\"keyword\">if</span> (<span class=\"string\">&quot;s1&quot;</span>.equals(value.getId())) &#123;</span><br><span class=\"line\">                    ctx.output(s1, value);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (<span class=\"string\">&quot;s2&quot;</span>.equals(value.getId())) &#123;</span><br><span class=\"line\">                    ctx.output(s2, value);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">//主流</span></span><br><span class=\"line\">                    out.collect(value);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"> </span><br><span class=\"line\">        ds1.print(<span class=\"string\">&quot;主流，非s1,s2的传感器&quot;</span>);</span><br><span class=\"line\">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class=\"line\">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class=\"line\"> </span><br><span class=\"line\">        s1DS.printToErr(<span class=\"string\">&quot;s1&quot;</span>);</span><br><span class=\"line\">        s2DS.printToErr(<span class=\"string\">&quot;s2&quot;</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\">        env.execute();</span><br><span class=\"line\"> </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"合流\"><a class=\"markdownIt-Anchor\" href=\"#合流\">#</a> 合流</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># union合流的数据类型必须相同</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">unionn</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        DataStreamSource&lt;Integer&gt; integerDataStreamSource1 = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\">        DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        DataStream&lt;Integer&gt; union = integerDataStreamSource.union(integerDataStreamSource1);</span><br><span class=\"line\"></span><br><span class=\"line\">        union.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">多条流可以用 , 分割</span><br><span class=\"line\">source.union(source1,source2);</span><br><span class=\"line\"></span><br><span class=\"line\">也可以用.union </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 一次可以合并多条流</span><br><span class=\"line\"># connect合流数据类型可以不一样</span><br><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource1 = environment.fromElements(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\">DataStreamSource&lt;Integer&gt; integerDataStreamSource = environment.fromElements(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>);</span><br><span class=\"line\">DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromElements(<span class=\"string\">&quot;111&quot;</span>, <span class=\"string\">&quot;222&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">ConnectedStreams&lt;Integer, String&gt; connect = integerDataStreamSource.connect(stringDataStreamSource);</span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; map = connect.map(<span class=\"keyword\">new</span> <span class=\"title class_\">CoMapFunction</span>&lt;Integer, String, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">map1</span><span class=\"params\">(Integer value)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> value.toString();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">map2</span><span class=\"params\">(String value)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">map.print();</span><br><span class=\"line\"></span><br><span class=\"line\">connect 一次只能连接两条流</span><br><span class=\"line\">连接后可以调用map等处理，但是各处理各的</span><br></pre></td></tr></table></figure>\n<h5 id=\"sink\"><a class=\"markdownIt-Anchor\" href=\"#sink\">#</a> sink</h5>\n<h6 id=\"输出到外部系统\"><a class=\"markdownIt-Anchor\" href=\"#输出到外部系统\">#</a> 输出到外部系统</h6>\n<p>Flink 作为数据处理框架，最终还是要把计算处理的结果写，入外部存储为外部应用提供支持。</p>\n<p>Sink 多数情况下同样并不需要我们自己实现。之前我们一直在使用的 print 方法其实就是一种 Sink, 它表示将数据流写入标准控制台打印输出。Flink 官方为我们提供了一部分</p>\n<h6 id=\"输出到文件\"><a class=\"markdownIt-Anchor\" href=\"#输出到文件\">#</a> 输出到文件</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 同时写入的文件数由并行度决定</span></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;datagen&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    FileSink&lt;String&gt; build = FileSink.forRowFormat(<span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(<span class=\"string\">&quot;/Users/bytedance/Desktop&quot;</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">SimpleStringEncoder</span>&lt;String&gt;(<span class=\"string\">&quot;UTF-8&quot;</span>)).</span><br><span class=\"line\">            withOutputFileConfig(OutputFileConfig.builder().withPartPrefix(<span class=\"string\">&quot;==&quot;</span>).withPartSuffix(<span class=\"string\">&quot;++&quot;</span>).build()).</span><br><span class=\"line\">            withBucketAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">DateTimeBucketAssigner</span>&lt;&gt;(<span class=\"string\">&quot;yyyy-MM-dd HH&quot;</span>, ZoneId.systemDefault())).</span><br><span class=\"line\">            withRollingPolicy(DefaultRollingPolicy.builder().withRolloverInterval(Duration.ofSeconds(<span class=\"number\">10</span>)).withMaxPartSize(<span class=\"keyword\">new</span> <span class=\"title class_\">MemorySize</span>(<span class=\"number\">1024</span> * <span class=\"number\">1024</span>)).build()).build();</span><br><span class=\"line\"></span><br><span class=\"line\">    stringDataStreamSource.sinkTo(build);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"输出到kafka\"><a class=\"markdownIt-Anchor\" href=\"#输出到kafka\">#</a> 输出到 kafka</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 同时写入的文件数由并行度决定</span></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;datagen&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// TODO注意:如果要使用精准一次 写入Kafka,需要满足以下条件,缺一不可</span></span><br><span class=\"line\">    <span class=\"comment\">//1、开启checkpoint(后续介绍)</span></span><br><span class=\"line\">    <span class=\"comment\">//2、设置事务前缀</span></span><br><span class=\"line\">    <span class=\"comment\">//3、设置事务超时时间:</span></span><br><span class=\"line\">    <span class=\"comment\">//checkpoint间隔&lt;</span></span><br><span class=\"line\">    <span class=\"comment\">//事务超时时间&lt;max的15分钟</span></span><br><span class=\"line\">    KafkaSink&lt;String&gt; build = KafkaSink.&lt;String&gt;builder()</span><br><span class=\"line\">            .setBootstrapServers(<span class=\"string\">&quot;hadoop100:9092,hadoop101:9092,hadoop102:9092&quot;</span>)</span><br><span class=\"line\">            .setRecordSerializer(</span><br><span class=\"line\">                    KafkaRecordSerializationSchema.&lt;String&gt;builder()</span><br><span class=\"line\">                            .setTopic(<span class=\"string\">&quot;aaaaa&quot;</span>)</span><br><span class=\"line\">                            .setValueSerializationSchema(<span class=\"keyword\">new</span> <span class=\"title class_\">SimpleStringSchema</span>())</span><br><span class=\"line\">                            .build()</span><br><span class=\"line\"></span><br><span class=\"line\">            )</span><br><span class=\"line\">            .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class=\"line\">            .setTransactionalIdPrefix(<span class=\"string\">&quot;xxxxxx&quot;</span>)</span><br><span class=\"line\">            .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG,<span class=\"number\">10</span> * <span class=\"number\">60</span> * <span class=\"number\">1000</span> + <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">            .build();</span><br><span class=\"line\">    <span class=\"comment\">// //写到kafka的一致性级别:精准一次、至少一次</span></span><br><span class=\"line\">    <span class=\"comment\">//.setDeliveryGuarantee (DeliveryGuarantee.EXAC)TLY_ONCE</span></span><br><span class=\"line\">    <span class=\"comment\">////如果是精准一次,必须设置事务的前缀</span></span><br><span class=\"line\">    <span class=\"comment\">//.setTransactionalIdPrefix(&quot;-&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\">//如果是精准一次,必须设置事务超时时间:大于checkpoint间隔,小于max15分钟</span></span><br><span class=\"line\"></span><br><span class=\"line\">    stringDataStreamSource.sinkTo(build);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 同时写入的文件数由并行度决定</span></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataGeneratorSource&lt;String&gt; stringDataGeneratorSource = <span class=\"keyword\">new</span> <span class=\"title class_\">DataGeneratorSource</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">map</span><span class=\"params\">(Long aLong)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;</span> + aLong;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, Long.MAX_VALUE, RateLimiterStrategy.perSecond(<span class=\"number\">1</span>), Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">    DataStreamSource&lt;String&gt; stringDataStreamSource = environment.fromSource(stringDataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class=\"string\">&quot;datagen&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果要指定写入kafka的key</span></span><br><span class=\"line\">    <span class=\"comment\">//可以自定义反序列器:</span></span><br><span class=\"line\">    <span class=\"comment\">//1、实现一个接口,重写序列化方法</span></span><br><span class=\"line\">    <span class=\"comment\">//2、指定key,转成字节数组</span></span><br><span class=\"line\">    <span class=\"comment\">//3、指定value,转成 字节数组</span></span><br><span class=\"line\">    <span class=\"comment\">//4、返回一个ProducerRecord对象,把key、value放进去</span></span><br><span class=\"line\"></span><br><span class=\"line\">    KafkaSink&lt;String&gt; build = KafkaSink.&lt;String&gt;builder()</span><br><span class=\"line\">            .setBootstrapServers(<span class=\"string\">&quot;hadoop100:9092,hadoop101:9092,hadoop102:9092&quot;</span>)</span><br><span class=\"line\">            .setRecordSerializer(</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> <span class=\"title class_\">KafkaRecordSerializationSchema</span>&lt;String&gt;() &#123;</span><br><span class=\"line\">                        <span class=\"meta\">@Nullable</span></span><br><span class=\"line\">                        <span class=\"meta\">@Override</span></span><br><span class=\"line\">                        <span class=\"keyword\">public</span> ProducerRecord&lt;<span class=\"type\">byte</span>[], <span class=\"type\">byte</span>[]&gt; serialize(String s, KafkaSinkContext kafkaSinkContext, Long aLong) &#123;</span><br><span class=\"line\">                            String[] datas = s.split(<span class=\"string\">&quot;,&quot;</span>);</span><br><span class=\"line\">                            <span class=\"type\">byte</span>[] key = datas[<span class=\"number\">0</span>].getBytes(StandardCharsets.UTF_8);</span><br><span class=\"line\">                            <span class=\"type\">byte</span>[] value = s.getBytes(StandardCharsets.UTF_8);</span><br><span class=\"line\">                            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ProducerRecord</span>&lt;&gt;(<span class=\"string\">&quot;aaa&quot;</span>, key, value);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            )</span><br><span class=\"line\">            .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class=\"line\">            .setTransactionalIdPrefix(<span class=\"string\">&quot;xxxxxx&quot;</span>)</span><br><span class=\"line\">            .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, <span class=\"number\">10</span> * <span class=\"number\">60</span> * <span class=\"number\">1000</span> + <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">            .build();</span><br><span class=\"line\"></span><br><span class=\"line\">    stringDataStreamSource.sinkTo(build);</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>输出到 mysql</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">8.0</span><span class=\"number\">.30</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;flink-connector-jdbc&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">1.17</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;repositories&gt;</span><br><span class=\"line\">    &lt;repository&gt;</span><br><span class=\"line\">        &lt;id&gt;apache-snapshots&lt;/id&gt;</span><br><span class=\"line\">        &lt;name&gt;apache snapshots&lt;/name&gt;</span><br><span class=\"line\">        &lt;url&gt;https:<span class=\"comment\">//repository.apache.org/content/repositories/snapshots/&lt;/url&gt;</span></span><br><span class=\"line\">    &lt;/repository&gt;</span><br><span class=\"line\">&lt;/repositories&gt;</span><br><span class=\"line\">SinkFunction&lt;WaterSensor&gt; jdbcSink = JdbcSink.sink(</span><br><span class=\"line\">                <span class=\"string\">&quot;insert into ws values(?,?,?)&quot;</span>,</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcStatementBuilder</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">accept</span><span class=\"params\">(PreparedStatement preparedStatement, WaterSensor waterSensor)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">                        <span class=\"comment\">//每收到一条WaterSensor，如何去填充占位符</span></span><br><span class=\"line\">                        preparedStatement.setString(<span class=\"number\">1</span>, waterSensor.getId());</span><br><span class=\"line\">                        preparedStatement.setLong(<span class=\"number\">2</span>, waterSensor.getTs());</span><br><span class=\"line\">                        preparedStatement.setInt(<span class=\"number\">3</span>, waterSensor.getVc());</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                JdbcExecutionOptions.builder()</span><br><span class=\"line\">                        .withMaxRetries(<span class=\"number\">3</span>) <span class=\"comment\">// 重试次数</span></span><br><span class=\"line\">                        .withBatchSize(<span class=\"number\">100</span>) <span class=\"comment\">// 批次的大小：条数</span></span><br><span class=\"line\">                        .withBatchIntervalMs(<span class=\"number\">3000</span>) <span class=\"comment\">// 批次的时间</span></span><br><span class=\"line\">                        .build(),</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcConnectionOptions</span>.JdbcConnectionOptionsBuilder()</span><br><span class=\"line\">                        .withUrl(<span class=\"string\">&quot;jdbc:mysql://hadoop102:3306/test?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=UTF-8&quot;</span>)</span><br><span class=\"line\">                        .withUsername(<span class=\"string\">&quot;root&quot;</span>)</span><br><span class=\"line\">                        .withPassword(<span class=\"string\">&quot;000000&quot;</span>)</span><br><span class=\"line\">                        .withConnectionCheckTimeoutSeconds(<span class=\"number\">60</span>) <span class=\"comment\">// 重试的超时时间</span></span><br><span class=\"line\">                        .build()</span><br><span class=\"line\">        );</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\">//          * TODO 写入mysql</span></span><br><span class=\"line\"><span class=\"comment\">//         * 1、只能用老的sink写法： addsink</span></span><br><span class=\"line\"><span class=\"comment\">//         * 2、JDBCSink的4个参数:</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第一个参数： 执行的sql，一般就是 insert into</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第二个参数： 预编译sql， 对占位符填充值</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第三个参数： 执行选项 ---》 攒批、重试</span></span><br><span class=\"line\"><span class=\"comment\">//         *    第四个参数： 连接选项 ---》 url、用户名、密码</span></span><br><span class=\"line\">        sensorDS.addSink(jdbcSink);</span><br></pre></td></tr></table></figure>\n<h6 id=\"自定义sink\"><a class=\"markdownIt-Anchor\" href=\"#自定义sink\">#</a> 自定义 sink</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Mysink</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">RichSinkFunction</span>&lt;String&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">open</span><span class=\"params\">(Configuration parameters)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.open(parameters);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">close</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">invoke</span><span class=\"params\">(String value, Context context)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.invoke(value, context);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"时间和窗口\"><a class=\"markdownIt-Anchor\" href=\"#时间和窗口\">#</a> 时间和窗口</h4>\n<p>Flink 是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的 &quot;数据块&quot; 进行处理，这就是所谓的 &quot;窗口&quot;(Window)。</p>\n<p>在 Flink 中，窗口其实并不是一个 &quot;框&quot;, 应该把窗口理解成一个 &quot;桶&quot;。在 Flink 中，窗口可以把流切割成有限大小的多个 &quot;存储桶&quot;。每个数据都会分发到对应的桶中，当到达窗口结束时间时，就对每个桶中收集的数据进行计算处理。</p>\n<p>Flink 中窗口并不是静态准备好的，而是动态创建 —— 当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。</p>\n<h5 id=\"窗口分类\"><a class=\"markdownIt-Anchor\" href=\"#窗口分类\">#</a> 窗口分类</h5>\n<p>窗口本身是截取有界数据的一种方式，所以窗口一个非常重要的信意其实就是 &quot;怎样截取数据&quot;。换句话说，就是以什么标准来开始和结束数据的截取，我们把它叫作窗口的 &quot;驱动类型&quot;。</p>\n<h6 id=\"1按照驱动类型分\"><a class=\"markdownIt-Anchor\" href=\"#1按照驱动类型分\">#</a> 1. 按照驱动类型分</h6>\n<p>(1) 时间窗口 (TimeWindow)</p>\n<p>时间窗口以时间点来定义窗口的开始 (start) 和结束 (end), 所以截取出的就是某一时间段的数据。到达结束时间时，窗口不再收集数据，触发计算输出结果，并将窗口关闭销毁。所以可以说基本思路就是 &quot;定点发车&quot;。</p>\n<p>(2) 计数窗口 (CountWindow)</p>\n<p>计数窗口基于元素的个数来截取数据，到达固定的个数时就触发计算并关闭窗口。每个窗口截取数据的个数，就是窗口的大小。基本思路是 &quot;人齐发车&quot;。</p>\n<h6 id=\"2按照窗口分配数据的规则分类\"><a class=\"markdownIt-Anchor\" href=\"#2按照窗口分配数据的规则分类\">#</a> 2. 按照窗口分配数据的规则分类</h6>\n<p>（1）滚动窗口</p>\n<p>滚动窗口有固定的大小，是一种对数据进行 &quot;均匀切片&quot; 的划分方式。窗口之间没有重叠，也不会有间隔，是 &quot;首尾相接&quot; 的状态。这是最简单的窗口形式，每个数据都都会被分配到一个窗口，而且只会属于一个窗口。</p>\n<p>（2）滑动窗口</p>\n<p>滑动窗口的大小也是固定的。但是窗口之间并不是首尾相接的，而是可以 &quot;错开&quot; 一定的位置。定义滑动窗口的参数有两个：除去窗口大小 (windowsize) 之之外，还有一个 &quot;滑动步长&quot;(windowslide), 它其实就代表了窗口计算的频率。窗口在结束时间触发计算输出结果，那么滑动步长就代表了计算频率。</p>\n<p>（3）会话窗口</p>\n<p>会话窗口，是基于 &quot;会话&quot;(session) 来来对数据进行分分组的。会话窗口只能基于时间来定义。</p>\n<p>会话窗口中，最重要的参数就是会话的超时时间，也就是两个会话窗口之间的最小距离。如果相邻两个数据到来的时间间隔 (Gap) 小于指定的大小 (size), 那说明还在保保持会话，它们就属于同一个窗口；如果 gap 大于 size, 那么新来的数据就应该属于新的会话窗口，而前一个窗口就应该关闭了。</p>\n<p>（4）全局窗口</p>\n<p>“全局窗口”, 这种窗口全局有效，会把相同 key 的所有数据都分配到同一个窗口中，这种窗口没有结束的时候，默认是不会做触发计算的。如果希望它能对数据进行计算处理，还需要自定义 &quot;触发器&quot;(Trigger)。</p>\n<h5 id=\"窗口api\"><a class=\"markdownIt-Anchor\" href=\"#窗口api\">#</a> 窗口 API</h5>\n<h6 id=\"带keyby和不带keyby\"><a class=\"markdownIt-Anchor\" href=\"#带keyby和不带keyby\">#</a> 带 keyby 和不带 keyby</h6>\n<p>1.1 没有 keyby 的窗口：窗口内的所有数据进入同一个子任务，并行度只能为 1     //sensorDS.windowAll ()</p>\n<p>1.2 有 keyby 的窗口：每个 key 上都定义了一组窗口，各自独立地进行统计计算   //sensorDS.window ()</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//1.1没有keyby的窗口:窗口内的所有数据进入同一个子任务,并行度只能为1</span></span><br><span class=\"line\">sensorDS.windowAll()</span><br><span class=\"line\"><span class=\"comment\">//1.2有keyby的窗口:每个key上都定义了一组窗口,各自独立地进行统计计算</span></span><br><span class=\"line\">基于时间的</span><br><span class=\"line\">sensorKS.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))<span class=\"comment\">////滚动窗口长度10s</span></span><br><span class=\"line\">sensorKS.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>),Time.seconds(<span class=\"number\">2</span>)))<span class=\"comment\">// 滑动窗口,窗口长度10s,滑动步长2s</span></span><br><span class=\"line\">sensorKS.window(ProcessingTimeSessionWindows.wiithGap(Time.seconds(<span class=\"number\">5</span>)))<span class=\"comment\">// 会话窗口,超时间隔5s</span></span><br><span class=\"line\"><span class=\"comment\">//基于计数的</span></span><br><span class=\"line\">sensorKS.countWindow(<span class=\"number\">5</span>) <span class=\"comment\">// 滚动窗口,窗口长度=5个元素</span></span><br><span class=\"line\">sensorKS.countWindow(<span class=\"number\">5</span>,<span class=\"number\">2</span>)<span class=\"comment\">//滑动窗口长度=5个元素,滑动步长=2个元素</span></span><br><span class=\"line\">sensorKS.window(Globalwindows.create()),_<span class=\"comment\">//全局窗口,计数窗口的底层就是用的这个,需要自定义触发器</span></span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224618279.png\" alt=\"image-20240804224618279\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// TODO 1.指定窗口分配器，用哪一种窗口</span></span><br><span class=\"line\"><span class=\"comment\">// 没有keyby的窗口</span></span><br><span class=\"line\"><span class=\"comment\">// 窗口内的所有数据进入同一个子任务,并行度只能为1</span></span><br><span class=\"line\"><span class=\"comment\">//waterSensorStringKeyedStream.windowAll()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 有keyby 窗口</span></span><br><span class=\"line\"><span class=\"comment\">// 每个key上都定义了一组窗口,各自独立地进行统计计算</span></span><br><span class=\"line\"><span class=\"comment\">//waterSensorStringKeyedStream.window()</span></span><br><span class=\"line\"><span class=\"comment\">// 基于时间</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));       <span class=\"comment\">// 滚动</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window1 = waterSensorStringKeyedStream.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>), Time.seconds(<span class=\"number\">2</span>)));      <span class=\"comment\">// 滑动</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window2 = waterSensorStringKeyedStream.window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class=\"number\">5</span>)));       <span class=\"comment\">// 会话</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 基于计数的</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream = waterSensorStringKeyedStream.countWindow(<span class=\"number\">5</span>);   <span class=\"comment\">// 滚动，窗口长度为5</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream1 = waterSensorStringKeyedStream.countWindow(<span class=\"number\">5</span>, <span class=\"number\">2</span>);   <span class=\"comment\">// 滑动，长度5 步长 2</span></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; window3 = waterSensorStringKeyedStream.window(GlobalWindows.create());     <span class=\"comment\">// 全局窗口，计数窗口底层，需要自定义触发器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// TODO 2.指定窗口函数,计算逻辑</span></span><br><span class=\"line\"><span class=\"comment\">// 增量聚合函数：来一条算一条.窗口触发的时候输出计算结果</span></span><br><span class=\"line\">window.reduce();</span><br><span class=\"line\">window.aggregate();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 全窗口函数，数据来了不计算。窗口触发的时候，计算并输出</span></span><br><span class=\"line\">window.process();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">environment.execute();</span><br></pre></td></tr></table></figure>\n<h5 id=\"窗口函数\"><a class=\"markdownIt-Anchor\" href=\"#窗口函数\">#</a> 窗口函数</h5>\n<h6 id=\"增量聚合reduce\"><a class=\"markdownIt-Anchor\" href=\"#增量聚合reduce\">#</a> 增量聚合 reduce</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 窗口的reduce</span></span><br><span class=\"line\"><span class=\"comment\">// 1.相同的key来的第一条数据不会调用reduce</span></span><br><span class=\"line\"><span class=\"comment\">// 2.增量聚合：来一条数据计算一次，不会输出</span></span><br><span class=\"line\"><span class=\"comment\">// 3.窗口触发的时候，才会输出窗口的最终计算结果</span></span><br><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; reduce = window.reduce(<span class=\"keyword\">new</span> <span class=\"title class_\">ReduceFunction</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> WaterSensor <span class=\"title function_\">reduce</span><span class=\"params\">(WaterSensor waterSensor, WaterSensor t1)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;调用reduce，&quot;</span> + waterSensor + <span class=\"string\">&quot;======&quot;</span> + t1);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensor</span>(waterSensor.getId(), t1.getTs(), waterSensor.getVc() + t1.getVc());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">reduce.print();</span><br><span class=\"line\"></span><br><span class=\"line\">environment.execute();</span><br></pre></td></tr></table></figure>\n<h6 id=\"聚合函数aggregate\"><a class=\"markdownIt-Anchor\" href=\"#聚合函数aggregate\">#</a> 聚合函数 aggregate</h6>\n<p>ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。</p>\n<p>Flink WindowAPI 中的 aggregate 就突破了这个限制，可以定义务更加灵活的窗口聚合操作。</p>\n<p>这个方法需要传入一个 AggregateFunction 的实现类作为参数。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 第一条数据来，创建窗口，创建累加器</span></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; aggregate = window.aggregate(<span class=\"keyword\">new</span> <span class=\"title class_\">AggregateFunction</span>&lt;WaterSensor, Integer, String&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * 输入类型</span></span><br><span class=\"line\"><span class=\"comment\">         * 累加器类型（存储中间结果类型）</span></span><br><span class=\"line\"><span class=\"comment\">         * 输出类型</span></span><br><span class=\"line\"><span class=\"comment\">         * pod</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> Integer <span class=\"title function_\">createAccumulator</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;创建累加器&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> Integer <span class=\"title function_\">add</span><span class=\"params\">(WaterSensor waterSensor, Integer integer)</span> &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;调用add&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> integer + waterSensor.getVc();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> String <span class=\"title function_\">getResult</span><span class=\"params\">(Integer integer)</span> &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;result&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> integer.toString();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> Integer <span class=\"title function_\">merge</span><span class=\"params\">(Integer integer, Integer acc1)</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 会话窗口会用到</span></span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;merge&quot;</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    aggregate.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"全窗口函数\"><a class=\"markdownIt-Anchor\" href=\"#全窗口函数\">#</a> 全窗口函数</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; apply = window.apply(<span class=\"keyword\">new</span> <span class=\"title class_\">WindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         *</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> s The key for which this window is evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> window The window that is being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> input The elements in the window being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> out A collector for emitting elements.</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">apply</span><span class=\"params\">(String s, TimeWindow window, Iterable&lt;WaterSensor&gt; input, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; process = window.process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         *</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> s The key for which this window is evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> context The context in which the window is being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> elements The elements in the window being evaluated.</span></span><br><span class=\"line\"><span class=\"comment\">         * <span class=\"doctag\">@param</span> out A collector for emitting elements.</span></span><br><span class=\"line\"><span class=\"comment\">         * 全窗口函数计算逻辑，窗口触发是才会调用一次，统一计算</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">            <span class=\"type\">long</span> <span class=\"variable\">start</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">            <span class=\"type\">long</span> <span class=\"variable\">end</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">            DateFormatUtils.format(start,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">            DateFormatUtils.format(end,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"type\">long</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\">            out.collect(elements.toString()+l);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    process.print();</span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"结合使用\"><a class=\"markdownIt-Anchor\" href=\"#结合使用\">#</a> 结合使用</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增量聚合Aggregate+全窗口process</span></span><br><span class=\"line\"><span class=\"comment\">     * 1、增量聚合函数处理数据:来一条计算一条</span></span><br><span class=\"line\"><span class=\"comment\">     * 2、窗口触发时,增量聚合的结果(只有一条)</span></span><br><span class=\"line\"><span class=\"comment\">     * 3、经过全窗口函数的处理包装后,输出</span></span><br><span class=\"line\"><span class=\"comment\">     * 传递给全窗口函数</span></span><br><span class=\"line\"><span class=\"comment\">     * 结合两者的优点:</span></span><br><span class=\"line\"><span class=\"comment\">     * 1、增量聚合:来一条计算一条,存储中间的计算结果,占用的空间少</span></span><br><span class=\"line\"><span class=\"comment\">     * 2、全窗口函数:可以通过上下文实现灵活的功能</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;String&gt; aggregate1 = window.aggregate(<span class=\"keyword\">new</span> <span class=\"title class_\">MyAgg</span>(), <span class=\"keyword\">new</span> <span class=\"title class_\">MyProcWin</span>());</span><br><span class=\"line\">    aggregate1.print();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.execute();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyAgg</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">AggregateFunction</span>&lt;WaterSensor, Integer, String&gt; &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Integer <span class=\"title function_\">createAccumulator</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;创建累加器&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Integer <span class=\"title function_\">add</span><span class=\"params\">(WaterSensor waterSensor, Integer integer)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;调用add&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> integer + waterSensor.getVc();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">getResult</span><span class=\"params\">(Integer integer)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;result&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> integer.toString();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Integer <span class=\"title function_\">merge</span><span class=\"params\">(Integer integer, Integer acc1)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 会话窗口会用到</span></span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;merge&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyProcWin</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;String,String,String,TimeWindow&gt;&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;String, String, String, TimeWindow&gt;.Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">long</span> <span class=\"variable\">start</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">        <span class=\"type\">long</span> <span class=\"variable\">end</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">        DateFormatUtils.format(start,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">        DateFormatUtils.format(end,<span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">long</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\">        out.collect(elements.toString()+l);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"动态会话窗口\"><a class=\"markdownIt-Anchor\" href=\"#动态会话窗口\">#</a> 动态会话窗口</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">    environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">    environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">    SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    WindowedStream&lt;WaterSensor, String, TimeWindow&gt; window = waterSensorStringKeyedStream.window(ProcessingTimeSessionWindows.withDynamicGap(</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">SessionWindowTimeGapExtractor</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">                <span class=\"meta\">@Override</span></span><br><span class=\"line\">                <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extract</span><span class=\"params\">(WaterSensor element)</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 根据ts的值动态变化会话窗口</span></span><br><span class=\"line\">                    <span class=\"keyword\">return</span> element.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">    ));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"计数窗口\"><a class=\"markdownIt-Anchor\" href=\"#计数窗口\">#</a> 计数窗口</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = localhost.keyBy(WaterSensor::getId);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 窗口分配器</span></span><br><span class=\"line\">        <span class=\"comment\">// 滚动，窗口长度5条数据</span></span><br><span class=\"line\"><span class=\"comment\">//        WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream = waterSensorStringKeyedStream.countWindow(5);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        WindowedStream&lt;WaterSensor, String, GlobalWindow&gt; waterSensorStringGlobalWindowWindowedStream = waterSensorStringKeyedStream.countWindow(<span class=\"number\">5</span>, <span class=\"number\">2</span>);</span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorStringGlobalWindowWindowedStream.process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, GlobalWindow&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, GlobalWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> context.window().maxTimestamp();</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">l1</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\">                out.collect(l + <span class=\"string\">&quot;===&quot;</span> + l1);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        process.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"触发器-移除器\"><a class=\"markdownIt-Anchor\" href=\"#触发器-移除器\">#</a> 触发器 移除器</h6>\n<p>触发器：触发计算和输出</p>\n<p>窗口触发：</p>\n<p>时间进展 &gt;= 窗口的最大时间戳 (end-1ms)</p>\n<p>移除器：</p>\n<p>定义移除数据的逻辑</p>\n<p>触发器和移除器都有默认实现，一般不需要自定义</p>\n<blockquote>\n<p>以时间类型的滚动窗口为例，分析原理:</p>\n<p>TODO1、窗口什么时候触发输出？</p>\n<p>时间进展 &gt;= 窗口的最大时间戳 (end-1ms)</p>\n<p>TODO2、窗口是怎么划分的？</p>\n<p>start = 向下取整，取窗口长度的整数倍</p>\n<p>end=start + 窗口长度</p>\n<p>窗口左闭右开 ==》</p>\n<p>属于本窗口的最大时间戳 = end-1ms</p>\n<p>TODO 3、窗口的生命周期？</p>\n<p>创建：属于本窗口的第一条数据来的时候，现 new 的，放入一一个 singleton 单例的集合中</p>\n<p>销毁 (关窗): 时间进展 &gt;= 窗口的最大时间戳 (end-1ms)+ 允许迟到的时间 (默认 0)</p>\n</blockquote>\n<h5 id=\"时间\"><a class=\"markdownIt-Anchor\" href=\"#时间\">#</a> 时间</h5>\n<p>事件时间：数据产生的时间</p>\n<p>处理时间：数据被处理的时间</p>\n<h5 id=\"水位线\"><a class=\"markdownIt-Anchor\" href=\"#水位线\">#</a> 水位线</h5>\n<p>具体实现上，水位线可以看作一条特殊的数据记录，它是插入到数据流中的一个标记点，主要内容就是一个时间戳！用来指示当前的事件时间。</p>\n<p>2) 乱序流中的水位线</p>\n<p>在分布式系统中，数据在节点间传输，会因为网络传输延迟的不确定性，导致顺序发生改变，这就是所谓的 &quot;乱序数据&quot;</p>\n<p>乱序 + 数据量小：我们还是靠数据来驱动，每来一个数据就提取它的时间戳、插入一个水位线。不过现在的情况是数据乱序，所以插入新的水位线时，要先判断一了下时间戳是否比之前的大，否则就不再生成新的水位线。也就是说，只有数据的时间戳比当前时钟大，才能推动动时钟前进，这时才插入水位线。</p>\n<p>乱序 + 数据量大：如果考虑到大量数据同时到来的处理效率，我们同样可以周期性地生成水位线。这时只需要保存一下之前所有数据中的最大时间戳，需要插入水位线时，就直接以它作为时间戳生成新的水位线。</p>\n<p>乱序 + 迟到数据：我们无法正确处理 &quot;迟到&quot; 的数据。为了让窗口能够正确收集到迟到的数据，我们也可以等上一段时间，比如 2 秒；也就是用当前已有数据的最大大时间戳减去 2 秒，就是要插入的水位线的时间戳。这样的话，9 秒的数据到来之后，事件时钟不会直接推进到 9 秒，而是进展到了 7 秒；必须等到 11 秒的数据到来之后，事件时钟才会进展到 9 秒，这时迟到数据也都已收集齐，0~9 秒的窗口就可以正确计算结果了。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224636418.png\" alt=\"image-20240804224636418\"></p>\n<p>3) 水位线特性</p>\n<p>水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据</p>\n<p>水位线主要的内容是一个时间戳，用来表示当前事件时间的进展</p>\n<p>水位线是基于数据的时间戳生成的</p>\n<p>水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进</p>\n<p>水位线可以通过设置延迟，来保证正确处理乱序数据</p>\n<p>一个水位线 Watermark (t), 表示在当前流中事件时间已经达到了时间戳 t, 这代表 t 之前的所有数据都到齐了，之后流中不会出现时间戳 t’&lt;t 的数据</p>\n<p>水位线是 Flink 流处理中保证结果正确性的核心机制，它往往会跟窗口一起配合，完成对乱序数据的正确处理。</p>\n<p>正确理解：在 Flink 中，窗口其实并不是一个 &quot;框&quot;, 应该把窗口理解成一个 &quot;桶&quot;。在 Flink 中，窗口可以把流切割成有限大小的多个 &quot;存储桶&quot;(bucket); 每个数据都会分发到对应的桶中，当到达窗口结束时间时，就对每个桶中收集的数据进行计算处理。</p>\n<h6 id=\"生成水位线\"><a class=\"markdownIt-Anchor\" href=\"#生成水位线\">#</a> 生成水位线</h6>\n<p>一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。</p>\n<p>内置水位线，有序流水位线</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">watermarkdemo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">                <span class=\"comment\">// 升序的watermark，没有等待时间</span></span><br><span class=\"line\">                .&lt;WaterSensor&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                <span class=\"comment\">// 指定时间传分配器</span></span><br><span class=\"line\">                .withTimestampAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">SerializableTimestampAssigner</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extractTimestamp</span><span class=\"params\">(WaterSensor waterSensor, <span class=\"type\">long</span> l)</span> &#123;</span><br><span class=\"line\">                System.out.println(waterSensor + <span class=\"string\">&quot;=======&quot;</span> + l);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> waterSensor.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = localhost.assignTimestampsAndWatermarks(waterSensorWatermarkStrategy);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">                <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">                .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                        out.collect(windowStart + windowEnd);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>内置 watermark 都是周期性生成的  environment.getConfig ().setAutoWatermarkInterval ();  默认 200 毫秒</p>\n<p>有序流 watermark = 当前最大的事件时间 - 1ms</p>\n<p>乱序流 watermark = 当前最大的事件时间 - 延迟时间 -1ms</p>\n<p>升序 watermark 就是等待时间为 0 的乱序 watermark</p>\n<p>乱序流设置水位线</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.time.Duration;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">watermarkOOOdemo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">                <span class=\"comment\">// 乱序的watermark，等三秒</span></span><br><span class=\"line\">                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class=\"number\">3</span>))</span><br><span class=\"line\">                <span class=\"comment\">// 指定时间传分配器</span></span><br><span class=\"line\">                .withTimestampAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">SerializableTimestampAssigner</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extractTimestamp</span><span class=\"params\">(WaterSensor waterSensor, <span class=\"type\">long</span> l)</span> &#123;</span><br><span class=\"line\">                System.out.println(waterSensor + <span class=\"string\">&quot;=======&quot;</span> + l);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> waterSensor.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = localhost.assignTimestampsAndWatermarks(waterSensorWatermarkStrategy);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">                <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">                .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                        out.collect(windowStart + windowEnd);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>水位线生成器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyPeriodWaterMark</span>&lt;T&gt; <span class=\"keyword\">implements</span> <span class=\"title class_\">WatermarkGenerator</span>&lt;T&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> maxTs;</span><br><span class=\"line\">    <span class=\"comment\">// 保存当前为止最大时间</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> delayTs;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">MyPeriodWaterMark</span><span class=\"params\">(<span class=\"type\">long</span> delayTs)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.delayTs = delayTs;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.maxTs = Long.MIN_VALUE + <span class=\"built_in\">this</span>.delayTs + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onEvent</span><span class=\"params\">(T t, <span class=\"type\">long</span> l, WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\">        maxTs = Math.max(maxTs, l);</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;最大时间穿&quot;</span> + maxTs);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onPeriodicEmit</span><span class=\"params\">(WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\">        watermarkOutput.emitWatermark(<span class=\"keyword\">new</span> <span class=\"title class_\">Watermark</span>(maxTs-delayTs -<span class=\"number\">1</span>));</span><br><span class=\"line\">        System.out.println(maxTs-delayTs -<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">watermarkCuustomdemo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">environment</span> <span class=\"operator\">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">        environment.enableCheckpointing(<span class=\"number\">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class=\"line\"></span><br><span class=\"line\">        environment.getConfig().setAutoWatermarkInterval(<span class=\"number\">200</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">                <span class=\"comment\">// 指定自定义生成器</span></span><br><span class=\"line\">                .&lt;WaterSensor&gt;forGenerator(<span class=\"keyword\">new</span> <span class=\"title class_\">WatermarkGeneratorSupplier</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> WatermarkGenerator&lt;WaterSensor&gt; <span class=\"title function_\">createWatermarkGenerator</span><span class=\"params\">(Context context)</span> &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MyPeriodWaterMark</span>&lt;&gt;(<span class=\"number\">3000L</span>);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;)</span><br><span class=\"line\">                <span class=\"comment\">// 指定时间传分配器</span></span><br><span class=\"line\">                .withTimestampAssigner(<span class=\"keyword\">new</span> <span class=\"title class_\">SerializableTimestampAssigner</span>&lt;WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">extractTimestamp</span><span class=\"params\">(WaterSensor waterSensor, <span class=\"type\">long</span> l)</span> &#123;</span><br><span class=\"line\">                System.out.println(waterSensor + <span class=\"string\">&quot;=======&quot;</span> + l);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> waterSensor.getTs() * <span class=\"number\">1000L</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = localhost.assignTimestampsAndWatermarks(waterSensorWatermarkStrategy);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">                <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">                .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                        out.collect(windowStart + windowEnd);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        environment.execute();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h6 id=\"断点式水位线生成器\"><a class=\"markdownIt-Anchor\" href=\"#断点式水位线生成器\">#</a> 断点式水位线生成器</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyPuntuatedPeriodWaterMark</span>&lt;T&gt; <span class=\"keyword\">implements</span> <span class=\"title class_\">WatermarkGenerator</span>&lt;T&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> maxTs;</span><br><span class=\"line\">    <span class=\"comment\">// 保存当前为止最大时间</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span>  <span class=\"type\">long</span> delayTs;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">MyPuntuatedPeriodWaterMark</span><span class=\"params\">(<span class=\"type\">long</span> delayTs)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.delayTs = delayTs;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.maxTs = Long.MIN_VALUE + <span class=\"built_in\">this</span>.delayTs + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onEvent</span><span class=\"params\">(T t, <span class=\"type\">long</span> l, WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\">        maxTs = Math.max(maxTs, l);</span><br><span class=\"line\">        watermarkOutput.emitWatermark(<span class=\"keyword\">new</span> <span class=\"title class_\">Watermark</span>(maxTs - delayTs - <span class=\"number\">1</span>));</span><br><span class=\"line\">        System.out.println( maxTs);</span><br><span class=\"line\">        System.out.println( maxTs - delayTs - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onPeriodicEmit</span><span class=\"params\">(WatermarkOutput watermarkOutput)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">WatermarkStrategy&lt;WaterSensor&gt; waterSensorWatermarkStrategy = WatermarkStrategy</span><br><span class=\"line\">        <span class=\"comment\">// 指定自定义生成器</span></span><br><span class=\"line\">        .&lt;WaterSensor&gt;forGenerator(ctx -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">MyPuntuatedPeriodWaterMark</span>&lt;&gt;(<span class=\"number\">3000L</span>))</span><br></pre></td></tr></table></figure>\n<h6 id=\"水位线传递\"><a class=\"markdownIt-Anchor\" href=\"#水位线传递\">#</a> 水位线传递</h6>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224656483.png\" alt=\"image-20240804224656483\"></p>\n<p>收到上游多个，取最小</p>\n<p>往下游多个发送，广播</p>\n<p>空闲等待：超过时间上有还没水位过来，后续不使用</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;Integer&gt; localhost = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>)</span><br><span class=\"line\">        .partitionCustom(<span class=\"keyword\">new</span> <span class=\"title class_\">MyPart</span>(), r -&gt; r)</span><br><span class=\"line\">        .map(Integer::parseInt)</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Integer&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((r, ts) -&gt; r * <span class=\"number\">1000L</span>)</span><br><span class=\"line\">                <span class=\"comment\">// 空闲等待五秒</span></span><br><span class=\"line\">                .withIdleness(Duration.ofSeconds(<span class=\"number\">5</span>)));</span><br></pre></td></tr></table></figure>\n<p>允许迟到</p>\n<p>推迟关窗时间，在关窗之前，会计算迟到数据，来一条计算一次。</p>\n<p>关窗口迟到数据不会被计算</p>\n<p>乱序：数据顺序乱了，出现时间小的比时间大的后来</p>\n<p>迟到：当前数据的时间戳 &lt; 当前的 watermark</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">        <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">        .allowedLateness(Time.seconds(<span class=\"number\">2</span>))   <span class=\"comment\">// 允许推迟两秒关窗</span></span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                out.collect(windowStart + windowEnd);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br></pre></td></tr></table></figure>\n<p>侧输出流</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OutputTag&lt;WaterSensor&gt; latedata = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;latedata&quot;</span>, Types.POJO(WaterSensor.class));</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">        <span class=\"comment\">// 事件语义窗口</span></span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">10</span>)))</span><br><span class=\"line\">        .allowedLateness(Time.seconds(<span class=\"number\">2</span>))   <span class=\"comment\">// 允许推迟两秒关窗</span></span><br><span class=\"line\">        <span class=\"comment\">// 关窗后迟到数据放入侧输出流</span></span><br><span class=\"line\">        .sideOutputLateData(latedata)</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessWindowFunction</span>&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">process</span><span class=\"params\">(String s, ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;.Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">startTs</span> <span class=\"operator\">=</span> context.window().getStart();</span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">endTs</span> <span class=\"operator\">=</span> context.window().getEnd();</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowStart</span> <span class=\"operator\">=</span> DateFormatUtils.format(startTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">windowEnd</span> <span class=\"operator\">=</span> DateFormatUtils.format(endTs, <span class=\"string\">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> elements.spliterator().estimateSize();</span><br><span class=\"line\"></span><br><span class=\"line\">                out.collect(windowStart + windowEnd);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        <span class=\"comment\">// 从主流获取侧输出流打印</span></span><br><span class=\"line\">        process.getSideOutput(latedata).printToErr();</span><br><span class=\"line\">        process.print();</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224716145.png\" alt=\"image-20240804224716145\"></p>\n<h5 id=\"合流-2\"><a class=\"markdownIt-Anchor\" href=\"#合流-2\">#</a> 合流</h5>\n<h6 id=\"基于时间的合流-双流联结\"><a class=\"markdownIt-Anchor\" href=\"#基于时间的合流-双流联结\">#</a> 基于时间的合流 - 双流联结</h6>\n<p>窗口联结</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; operator1 = environment.fromElements(Tuple2.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>),Tuple2.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>),Tuple2.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; operator2 = environment.fromElements(Tuple3.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">11</span>), Tuple3.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>, <span class=\"number\">22</span>), Tuple3.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">33</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple3&lt;String, Integer, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 落在同一个时间范围内才能匹配</span></span><br><span class=\"line\"><span class=\"comment\">// 根据keyby的key，来进行关联</span></span><br><span class=\"line\"><span class=\"comment\">// 只能拿到匹配上的数据，类似inner join</span></span><br><span class=\"line\">DataStream&lt;String&gt; apply = operator1.join(operator2)</span><br><span class=\"line\">        .where(r1 -&gt; r1.f0)</span><br><span class=\"line\">        .equalTo(r2 -&gt; r2.f0)</span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class=\"number\">5</span>)))</span><br><span class=\"line\">        .apply(<span class=\"keyword\">new</span> <span class=\"title class_\">JoinFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">             * 关联上的数据，调用join</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> stringIntegerTuple2</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> stringIntegerIntegerTuple3</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">join</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2, Tuple3&lt;String, Integer, Integer&gt; stringIntegerIntegerTuple3)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> stringIntegerTuple2 + <span class=\"string\">&quot;&quot;</span> + stringIntegerIntegerTuple3;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">apply.print();</span><br></pre></td></tr></table></figure>\n<p>间隔联结</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; operator1 = environment.fromElements(Tuple2.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>),Tuple2.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>),Tuple2.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; operator2 = environment.fromElements(Tuple3.of(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">11</span>), Tuple3.of(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>, <span class=\"number\">22</span>), Tuple3.of(<span class=\"string\">&quot;c&quot;</span>, <span class=\"number\">1</span>, <span class=\"number\">33</span>))</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy</span><br><span class=\"line\">                .&lt;Tuple3&lt;String, Integer, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class=\"line\">                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2StringKeyedStream = operator1.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; tuple3StringKeyedStream = operator2.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = tuple2StringKeyedStream.intervalJoin(tuple3StringKeyedStream)</span><br><span class=\"line\">        .between(Time.seconds(-<span class=\"number\">2</span>), Time.seconds(<span class=\"number\">2</span>))</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessJoinFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">             * 两条流数据匹配，调用方法</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> left The left element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> right The right element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> ctx A context that allows querying the timestamps of the left, right and joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             *     In addition, this context allows to emit elements on a side output.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> out The collector to emit resulting elements to.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;.Context ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                out.collect(left + <span class=\"string\">&quot;=====&quot;</span> + right);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.print();</span><br></pre></td></tr></table></figure>\n<p>处理迟到数据</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2StringKeyedStream = operator1.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; tuple3StringKeyedStream = operator2.keyBy(r2 -&gt; r2.f0);</span><br><span class=\"line\"></span><br><span class=\"line\">OutputTag&lt;Tuple2&lt;String, Integer&gt;&gt; tupleOutputTag = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;ks1-late&quot;</span>, Types.TUPLE(Types.STRING, Types.INT));</span><br><span class=\"line\">OutputTag&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; tupleOutputTag2 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;ks2-late&quot;</span>, Types.TUPLE(Types.STRING, Types.INT, Types.INT));</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = tuple2StringKeyedStream.intervalJoin(tuple3StringKeyedStream)</span><br><span class=\"line\">        .between(Time.seconds(-<span class=\"number\">2</span>), Time.seconds(<span class=\"number\">2</span>))</span><br><span class=\"line\">        .sideOutputLeftLateData(tupleOutputTag)</span><br><span class=\"line\">        .sideOutputRightLateData(tupleOutputTag2)</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">ProcessJoinFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">             * 两条流数据匹配，调用方法</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> left The left element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> right The right element of the joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> ctx A context that allows querying the timestamps of the left, right and joined pair.</span></span><br><span class=\"line\"><span class=\"comment\">             *     In addition, this context allows to emit elements on a side output.</span></span><br><span class=\"line\"><span class=\"comment\">             * <span class=\"doctag\">@param</span> out The collector to emit resulting elements to.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;.Context ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                out.collect(left + <span class=\"string\">&quot;=====&quot;</span> + right);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.getSideOutput(tupleOutputTag).printToErr();</span><br><span class=\"line\">process.getSideOutput(tupleOutputTag2).printToErr();</span><br><span class=\"line\">process.print();</span><br></pre></td></tr></table></figure>\n<h4 id=\"处理函数\"><a class=\"markdownIt-Anchor\" href=\"#处理函数\">#</a> 处理函数</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KeyedStream&lt;WaterSensor, String&gt; waterSensorStringKeyedStream = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId);</span><br><span class=\"line\">SingleOutputStreamOperator&lt;String&gt; process = waterSensorStringKeyedStream.process(<span class=\"keyword\">new</span> <span class=\"title class_\">KeyedProcessFunction</span>&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> value The input value.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> ctx A &#123;<span class=\"doctag\">@link</span> Context&#125; that allows querying the timestamp of the element and getting a</span></span><br><span class=\"line\"><span class=\"comment\">     *     &#123;<span class=\"doctag\">@link</span> TimerService&#125; for registering timers and querying the time. The context is only</span></span><br><span class=\"line\"><span class=\"comment\">     *     valid during the invocation of this method, do not store it.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> out The collector for returning result values.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(WaterSensor value, KeyedProcessFunction&lt;String, WaterSensor, String&gt;.Context ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 提取事件事件</span></span><br><span class=\"line\">        <span class=\"type\">Long</span> <span class=\"variable\">timestamp</span> <span class=\"operator\">=</span> ctx.timestamp();</span><br><span class=\"line\">        <span class=\"comment\">// 定时器</span></span><br><span class=\"line\">        <span class=\"type\">TimerService</span> <span class=\"variable\">timerService</span> <span class=\"operator\">=</span> ctx.timerService();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 注册定时器</span></span><br><span class=\"line\">        timerService.registerEventTimeTimer(<span class=\"number\">5000L</span>);</span><br><span class=\"line\">        System.out.println(timestamp + <span class=\"string\">&quot;===&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> timestamp The timestamp of the firing timer.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> ctx An &#123;<span class=\"doctag\">@link</span> OnTimerContext&#125; that allows querying the timestamp, the &#123;<span class=\"doctag\">@link</span></span></span><br><span class=\"line\"><span class=\"comment\">     *     TimeDomain&#125;, and the key of the firing timer and getting a &#123;<span class=\"doctag\">@link</span> TimerService&#125; for</span></span><br><span class=\"line\"><span class=\"comment\">     *     registering timers and querying the time. The context is only valid during the invocation</span></span><br><span class=\"line\"><span class=\"comment\">     *     of this method, do not store it.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> out The collector for returning result values.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onTimer</span><span class=\"params\">(<span class=\"type\">long</span> timestamp, KeyedProcessFunction&lt;String, WaterSensor, String&gt;.OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.onTimer(timestamp, ctx, out);</span><br><span class=\"line\">        System.out.println(timestamp + <span class=\"string\">&quot;=======---&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>定时器：</p>\n<p>keyed 才有</p>\n<p>事件时间定时器，通过 watermark 触发</p>\n<p>watermark &gt;=  注册时间</p>\n<p>watermark = 当前最大时间 - 等待时间 - 1</p>\n<p>在 process 中获取 watermark，显示的上一次的 watermaark，因为 process 还没收到这条新数据的 watermark</p>\n<h5 id=\"侧输出流\"><a class=\"markdownIt-Anchor\" href=\"#侧输出流\">#</a> 侧输出流</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorSingleOutputStreamOperator = environment.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">7777</span>).map(<span class=\"keyword\">new</span> <span class=\"title class_\">WaterSensorMapFunction</span>())</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class=\"number\">3</span>))</span><br><span class=\"line\">                .withTimestampAssigner((ele, ts) -&gt; ele.getTs() * <span class=\"number\">1000L</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">OutputTag&lt;String&gt; warn11 = <span class=\"keyword\">new</span> <span class=\"title class_\">OutputTag</span>&lt;&gt;(<span class=\"string\">&quot;warn&quot;</span>, Types.STRING);</span><br><span class=\"line\"></span><br><span class=\"line\">SingleOutputStreamOperator&lt;WaterSensor&gt; process = waterSensorSingleOutputStreamOperator.keyBy(WaterSensor::getId)</span><br><span class=\"line\">        .process(<span class=\"keyword\">new</span> <span class=\"title class_\">KeyedProcessFunction</span>&lt;String, WaterSensor, WaterSensor&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(WaterSensor value, KeyedProcessFunction&lt;String, WaterSensor, WaterSensor&gt;.Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 侧输出流</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (value.getVc() &gt; <span class=\"number\">10</span>) &#123;</span><br><span class=\"line\">                    ctx.output(warn11, <span class=\"string\">&quot;sss&quot;</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">// 主流</span></span><br><span class=\"line\">                out.collect(value);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.getSideOutput(warn11).printToErr();</span><br><span class=\"line\">process.print();</span><br></pre></td></tr></table></figure>\n<h4 id=\"状态管理\"><a class=\"markdownIt-Anchor\" href=\"#状态管理\">#</a> 状态管理</h4>\n<p>在 Flink 中，算子任务可以分为无状态和有状态两种情况。</p>\n<p>无状态的算子任务只需要观察每个独立事件，根据当前输入的数故据直接转换输出结果。我们之前讲到的基本转换算子，如 map、filter、flatMap, 计算时不依赖其他数据，就都属于无状态的算子。</p>\n<p>而有状态的算子任务，则除当前数据之外，还需要一些其他也数据来得到计算结果。这里的 &quot;其他数据&quot;, 就是所谓的状态 (state)。我们之前讲到的算子中，聚合算子、窗口算子都属于有状态的算子。</p>\n<p><strong>托管状态（Managed State）和原始状态（Raw State）</strong></p>\n<p>Flink 的状态有两种：托管状态（Managed State）和原始状态（Raw State）。托管状态就是由 Flink 统一管理的，状态的存储访问、故障恢复和重组等一系列问题都由 Flink 实现，我们只要调接口就可以；而原始状态则是自定义的，相当于就是开辟了一块内存，需要我们自己管理，实现状态的序列化和故障恢复。</p>\n<p>通常我们采用 Flink 托管状态来实现需求。</p>\n<p>托管状态分为算子状态和按键分区状态</p>\n<p>keyby 后的是按键分区状态，其他称为算子状态</p>\n<p><strong>算子状态（Operator State）和按键分区状态（Keyed State）</strong></p>\n<p>接下来我们的重点就是托管状态（Managed State）。</p>\n<p>我们知道在 Flink 中，一个算子任务会按照并行度分为多个并行子任务执行，而不同的子任务会占据不同的任务槽（task slot）。由于不同的 slot 在计算资源上是物理隔离的，所以 Flink 能管理的状态在并行任务间是无法共享的，每个状态只能针对当前子任务的实例有效。</p>\n<p>而很多有状态的操作（比如聚合、窗口）都是要先做 keyBy 进行按键分区的。按键分区之后，任务所进行的所有计算都应该只针对当前 key 有效，所以状态也应该按照 key 彼此隔离。在这种情况下，状态的访问方式又会有所不同。</p>\n<p>基于这样的想法，我们又可以将托管状态分为两类：算子状态和按键分区状态。</p>\n<p>算子状态可以用在所有算子上，使用的时候其实就跟一个本地变量没什么区别–因为本地变量的作用域也是当前任务实例。在使用时，我们还需进一步实现 ChneckpointedFunction 接口。</p>\n<h4 id=\"容错机制\"><a class=\"markdownIt-Anchor\" href=\"#容错机制\">#</a> 容错机制</h4>\n<p>在流处理中，我们可以用存档读档的思路，就是将之前某个日时间点所有的状态保存下来，这份 &quot;存档&quot; 就是所谓的 &quot;检查点&quot;(checkpoint)。</p>\n<p><strong>1）周期性的触发保存</strong></p>\n<p>“随时存档” 确实恢复起来方便，可是需要我们不停地做存档操作。如果每处理一条数据就进行检查点的保存，当大量数据同时到来时，就会耗费很多资源来频繁做检查点，数据处理的速度就会受到影响。所以在 Flink 中，检查点的保存是周期性触发的，间隔时间可以进行设置。</p>\n<p><strong>2）保存的时间点</strong></p>\n<p>我们应该在所有任务（算子）都恰好处理完一个相同的输入数据的时候，将它们的状态保存下来。</p>\n<p>这样做可以实现一个数据被所有任务（算子）完整地处理完，状态得到了保存。</p>\n<p>如果出现故障，我们恢复到之前保存的状态，故障时正在处理的所有数据都需要重新处理；我们只需要让源（source）任务向数据源重新提交偏移量、请求重放数据就可以了。当然这需要源任务可以把偏移量作为算子状态保存下来，而且外部数据源能够重置偏移量</p>\n<p>当发生故障时，就需要找到最近一次成功保存的检查点来恢复状态。</p>\n<p>(1) 重启应用</p>\n<p>遇到故障之后，第一步当然就是重启。我们将应用重新启动动后，所有任务的状态会清空。</p>\n<p>(2) 读取检查点，重置状态</p>\n<p>找到最近一次保存的检查点，从中读出每个算子任务状态的快照，分别填充到对应的状态中。这样，Flink 内部所有任务的状态，就恢复到了保存检查点的那一时刻，也就是刚好处理完第三个数据的时候。</p>\n<p>(3) 重置偏移量</p>\n<p>从检查点恢复状态后还有一个问题：如果直接继续处理数据，那么保存检查点之后、到发生故障这段时间内的数据，也就是第 4、5 个数据就相当于丢掉了；这会造成计算结果的错误。</p>\n<p>为了不丢数据，我们应该从保存检查点后开始重新读取数据，这可以通过 Source 任务向外部数据源重新提交偏移量 (offset) 来实现。</p>\n<p>(4) 继续处理数据</p>\n<p>接下来，我们就可以正常处理数据了。首先是重放第 4、5 个一数据，然后继续读取后面的数据。</p>\n<h4 id=\"flink-sql\"><a class=\"markdownIt-Anchor\" href=\"#flink-sql\">#</a> Flink SQL</h4>\n",
            "tags": [
                "大数据"
            ]
        },
        {
            "id": "http://example.com/2024/04/01/spark/",
            "url": "http://example.com/2024/04/01/spark/",
            "title": "spark",
            "date_published": "2024-04-01T05:38:45.000Z",
            "content_html": "<h2 id=\"spark\"><a class=\"markdownIt-Anchor\" href=\"#spark\">#</a> spark</h2>\n<p>分布式计算引擎框架，基于 mapreduce 开发</p>\n<p>单机：单进程，单节点</p>\n<p>伪分布式：多进程，单节点</p>\n<p>分布式：多进程，多节点</p>\n<p>分布式计算核心：切分数据，减少数据规模</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223237028.png\" alt=\"image-20240804223237028\"></p>\n<p>spark 分布式集群采用集群中心化</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223253379.png\" alt=\"image-20240804223253379\"></p>\n<p>框架：不完整的计算机程序 (核心功能已经开发完毕，但是是和业务相关的代码未开发)(MR，spark)</p>\n<p>系统：完整的计算机程序 (HDFS,Kafka)</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223306758.png\" alt=\"image-20240804223306758\"></p>\n<p>引擎：核心功能</p>\n<p>spark 基于 mr 开发，两者区别</p>\n<p>1. 开发语言：mr：java，不适合进行大量数据处理。spark：scala，适合大量数据处理，封装大量功能</p>\n<p>2. 处理方式：hadoop 出现的早，只考虑单一的计算操作</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223319497.png\" alt=\"image-20240804223319497\"></p>\n<p>spark 优化了计算过程</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223331386.png\" alt=\"image-20240804223331386\"></p>\n<p>回顾：Hadoop 主要解决，海量数据的存储和海量数据的分析计算。</p>\n<p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223347611.png\" alt=\"image-20240804223347611\"></p>\n<p>spark 内置模块</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223358385.png\" alt=\"image-20240804223358385\"></p>\n<h3 id=\"部署spark集群\"><a class=\"markdownIt-Anchor\" href=\"#部署spark集群\">#</a> 部署 spark 集群</h3>\n<p>部署 Spark 其实指的就是 Spark 的程序逻辑在什么资源中执行</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223413816.png\" alt=\"image-20240804223413816\"></p>\n<p>如果资源是当前单节点提供的，那么就称之为单机模式</p>\n<p>如果资源是当前多节点提供的，那么就称之为分布式模式</p>\n<p>如果资源是由 Yarn 提供的，那么就称之为 Yarn 部署环境</p>\n<p>如果资源是由 Spark 提供的，那么就称之为 Spark 部署环境 (Standalone</p>\n<p>生产环境中主要采用：yarn+spark  也称之为（spark on yarna）</p>\n<p>(1) Local 模式：在本地部署单个 Spark 服务</p>\n<p>(2) Standalone 模式：Spark 自带的任务调度模式。(国内不常用)</p>\n<p>(3) YARN 模式：Spark 使用 Hadoop 的 YARN 组件进行资源与任务调度。(国内最常用)</p>\n<p>(4) Mesos 模式：Spark 使用 Mesos 平台进行资源与任务的调度。(国内很少用)</p>\n<h4 id=\"部署local\"><a class=\"markdownIt-Anchor\" href=\"#部署local\">#</a> 部署 local</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https:<span class=\"comment\">//dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz</span></span><br><span class=\"line\">tar xzvf spark-<span class=\"number\">3.4</span><span class=\"number\">.3</span>-bin-hadoop3.tgz</span><br><span class=\"line\"></span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master local[<span class=\"number\">2</span>] ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">--master 指定资源提供者</span><br><span class=\"line\">local 单线程</span><br><span class=\"line\">local[<span class=\"number\">2</span>] 两个线程执行</span><br><span class=\"line\">local[*] 使用全部核</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO DAGScheduler: Job <span class=\"number\">0</span> finished: reduce at SparkPi.scala:<span class=\"number\">38</span>, took <span class=\"number\">1.055199</span> s</span><br><span class=\"line\">Pi is roughly <span class=\"number\">3.1425071142507113</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO SparkContext: SparkContext is stopping with exitCode <span class=\"number\">0.</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO SparkUI: Stopped Spark web UI at http:<span class=\"comment\">//hadoop100:4040</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO MemoryStore: MemoryStore cleared</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO BlockManager: BlockManager stopped</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO SparkContext: Successfully stopped SparkContext</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">12</span> 08:09:<span class=\"number\">06</span> INFO ShutdownHookManager: Shutdown hook called</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223435814.png\" alt=\"image-20240804223435814\"></p>\n<p>Spark 在运行时，会启动进程，申请资源，执行计算，但是一旦计算完毕，那么进程会停止，资源会释放掉</p>\n<p>Stopped Spark web UI at <span class=\"exturl\" data-url=\"aHR0cDovL2hhZG9vcDEwMDo0MDQw\">http://hadoop100:4040</span></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223450585.png\" alt=\"image-20240804223450585\"></p>\n<h4 id=\"yarn模式\"><a class=\"markdownIt-Anchor\" href=\"#yarn模式\">#</a> yarn 模式</h4>\n<p>编辑启动关闭脚本</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">echo <span class=\"string\">&quot;========hadoop100==========&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;/root/jdk8u352-b08/bin/jps&quot;</span></span><br><span class=\"line\">echo <span class=\"string\">&quot;========hadoop101==========&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop101</span> <span class=\"string\">&quot;jps&quot;</span></span><br><span class=\"line\">echo <span class=\"string\">&quot;========hadoop102==========&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop102</span> <span class=\"string\">&quot;jps&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/start-dfs.sh&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop101</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/start-yarn.sh&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/bin/mapred --daemon start historyserver&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/bin/mapred --daemon stop historyserver&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop101</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/stop-yarn.sh&quot;</span></span><br><span class=\"line\">ssh root<span class=\"meta\">@hadoop100</span> <span class=\"string\">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/stop-dfs.sh&quot;</span></span><br><span class=\"line\">vim spark-env.sh</span><br><span class=\"line\">YARN_CONF_DIR=/root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/etc/hadoop/</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop-start.sh</span><br><span class=\"line\"></span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master yarn ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# jpsall </span><br><span class=\"line\">========hadoop100==========</span><br><span class=\"line\"><span class=\"number\">42388</span> DataNode</span><br><span class=\"line\"><span class=\"number\">42683</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">43820</span> Jps</span><br><span class=\"line\"><span class=\"number\">42894</span> JobHistoryServer</span><br><span class=\"line\"><span class=\"number\">43583</span> SparkSubmit</span><br><span class=\"line\"><span class=\"number\">42191</span> NameNode</span><br><span class=\"line\">========hadoop101==========</span><br><span class=\"line\"><span class=\"number\">37904</span> Jps</span><br><span class=\"line\"><span class=\"number\">37169</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">36818</span> ResourceManager</span><br><span class=\"line\"><span class=\"number\">36610</span> DataNode</span><br><span class=\"line\"><span class=\"number\">37843</span> YarnCoarseGrainedExecutorBackend</span><br><span class=\"line\"><span class=\"number\">37721</span> ExecutorLauncher</span><br><span class=\"line\">========hadoop102==========</span><br><span class=\"line\"><span class=\"number\">37362</span> DataNode</span><br><span class=\"line\"><span class=\"number\">37509</span> SecondaryNameNode</span><br><span class=\"line\"><span class=\"number\">37626</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">38093</span> Jps</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223505654.png\" alt=\"image-20240804223505654\"></p>\n<p>配置历史服务</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim spark-defaults.conf</span><br><span class=\"line\">spark.eventLog.enabled <span class=\"literal\">true</span></span><br><span class=\"line\">spark.eventLog.dir hdfs:<span class=\"comment\">//hadoop100:8020/directory</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">vim spark-env.sh</span><br><span class=\"line\">export SPARK_HISTORY_OPTS=<span class=\"string\">&quot;-Dspark.history.ui.port=18080 -Dspark.history.fs.logDirectory=hdfs://hadoop100:8020/directory -Dspark.history.retainedApplications=30&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">vim spark-defaults.conf</span><br><span class=\"line\">spark.yarn.historyServer.address=hadoop100:<span class=\"number\">18080</span></span><br><span class=\"line\">spark.history.ui.port=<span class=\"number\">18080</span></span><br></pre></td></tr></table></figure>\n<p>hdfs 中创建 /directory</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223520765.png\" alt=\"image-20240804223520765\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sbin/start-history-server.sh</span><br><span class=\"line\"></span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master yarn ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">将spark历史记录保存到了hadoop history中</span><br></pre></td></tr></table></figure>\n<p>运行时，会将 yarn 需要用到的 lib 和 conf 上传到 hdfs 中</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">32</span>:<span class=\"number\">57</span> INFO Client: Uploading resource file:/tmp/spark-4dbf287c-<span class=\"number\">9986</span>-44f0-89c1-7ede052800c0/__spark_libs__5882056198812431005.zip -&gt; hdfs:<span class=\"comment\">//hadoop100:8020/user/root/.sparkStaging/application_1718182778487_0005/__spark_libs__5882056198812431005.zip</span></span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">32</span>:<span class=\"number\">58</span> INFO Client: Uploading resource file:/tmp/spark-4dbf287c-<span class=\"number\">9986</span>-44f0-89c1-7ede052800c0/__spark_conf__3646583648306474590.zip -&gt; hdfs:<span class=\"comment\">//hadoop100:8020/user/root/.sparkStaging/application_1718182778487_0005/__spark_conf__.zip</span></span><br></pre></td></tr></table></figure>\n<p>运行结束会删除文件</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">33</span>:<span class=\"number\">07</span> INFO ShutdownHookManager: Deleting directory /tmp/spark-f3d8debb-<span class=\"number\">7304</span>-<span class=\"number\">4251</span>-b80b-11b1ab91f45c</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">14</span> <span class=\"number\">06</span>:<span class=\"number\">33</span>:<span class=\"number\">07</span> INFO ShutdownHookManager: Deleting directory /tmp/spark-4dbf287c-<span class=\"number\">9986</span>-44f0-89c1-7ede052800c0</span><br></pre></td></tr></table></figure>\n<p>yarn 模式中有 client 和 cluster 模式，主要区别在于：Driver 程序的运行节点。</p>\n<p>yarn-client:Driver 程序运行在客户端，适用于交互、调试，希望立即看到 Japp 的输出。</p>\n<p>yarn-cluster:Driver 程序运行在由 ResourceManager 启动的 APPIMaster, 适用于生产环境。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223534005.png\" alt=\"image-20240804223534005\"></p>\n<p>默认使用的客户端模式</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 使用cluster模式</span><br><span class=\"line\">bin/spark-submit --<span class=\"keyword\">class</span> <span class=\"title class_\">org</span>.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2<span class=\"number\">.12</span>-<span class=\"number\">3.4</span><span class=\"number\">.3</span>.jar <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">========hadoop100==========</span><br><span class=\"line\"><span class=\"number\">42388</span> DataNode</span><br><span class=\"line\"><span class=\"number\">51081</span> Jps</span><br><span class=\"line\"><span class=\"number\">42683</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">44508</span> HistoryServer</span><br><span class=\"line\"><span class=\"number\">42894</span> JobHistoryServer</span><br><span class=\"line\"><span class=\"number\">50959</span> SparkSubmit</span><br><span class=\"line\"><span class=\"number\">42191</span> NameNode</span><br><span class=\"line\">========hadoop101==========</span><br><span class=\"line\"><span class=\"number\">37169</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">36818</span> ResourceManager</span><br><span class=\"line\"><span class=\"number\">36610</span> DataNode</span><br><span class=\"line\"><span class=\"number\">40596</span> Jps</span><br><span class=\"line\">========hadoop102==========</span><br><span class=\"line\"><span class=\"number\">37362</span> DataNode</span><br><span class=\"line\"><span class=\"number\">37509</span> SecondaryNameNode</span><br><span class=\"line\"><span class=\"number\">42121</span> Jps</span><br><span class=\"line\"><span class=\"number\">37626</span> NodeManager</span><br><span class=\"line\"><span class=\"number\">41995</span> ApplicationMaster</span><br></pre></td></tr></table></figure>\n<h4 id=\"standalone模式\"><a class=\"markdownIt-Anchor\" href=\"#standalone模式\">#</a> standalone 模式</h4>\n<p>Standalone 模式是 Spark 自带的资源调度引擎，构建一个由 Master+VVorker 构成的 Spark 集群，Spark 运行在集群中。</p>\n<p>这个要和 Hadoop 中的 Standalone 区别开来。这里的 Standalone 是指只用 Spark 来搭建一个集群，不需要借助 Hadoop 的 Yarn 和 Mesos 等其他框架。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223542959.png\" alt=\"image-20240804223542959\"></p>\n<h4 id=\"mesos模式\"><a class=\"markdownIt-Anchor\" href=\"#mesos模式\">#</a> mesos 模式</h4>\n<p>Spark 客户端直接连接 Mesos; 不需要额外构建 Spark 集群。国内应用比较少，更多的是运用 Yarn 调度。</p>\n<h4 id=\"模式对比\"><a class=\"markdownIt-Anchor\" href=\"#模式对比\">#</a> 模式对比</h4>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223557692.png\" alt=\"image-20240804223557692\"></p>\n<h4 id=\"端口号\"><a class=\"markdownIt-Anchor\" href=\"#端口号\">#</a> 端口号</h4>\n<p>1) Spark 查看当前 Spark-shell 运行任务情况端口号：4040</p>\n<p>2) Spark 历史服务器端口号：18080 (类比于 Hadoop 历史服务器端口号：19888)</p>\n<h3 id=\"rdd\"><a class=\"markdownIt-Anchor\" href=\"#rdd\">#</a> rdd</h3>\n<p>RDD: 分布式计算模型</p>\n<p>1. 一定是一个对象</p>\n<p>2. 一定封装了大量方法和属性</p>\n<p>3. 一定需要适合进行分布式处理 (减小数据规模，并行计算算)</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223609055.png\" alt=\"image-20240804223609055\"></p>\n<h4 id=\"rdd编程\"><a class=\"markdownIt-Anchor\" href=\"#rdd编程\">#</a> RDD 编程</h4>\n<p>在 Spark 中创建 RDD 的创建方式可以分为三种：从集合中创建] RDD、从外部存储创建 RDD、从其他 RDD 创建。</p>\n<p>RDD 的处理方式和 JavalO 流完全一样，也采用装饰者设计式来实现功能的</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spark-core_2<span class=\"number\">.12</span>&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">3.3</span><span class=\"number\">.1</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.example;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Main</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"comment\">//JavaSparkContext javaSparkContext = new JavaSparkContext(&quot;local&quot;,&quot;spark&quot;);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"对接内存数据构建rdd对象\"><a class=\"markdownIt-Anchor\" href=\"#对接内存数据构建rdd对象\">#</a> 对接内存数据构建 RDD 对象</h4>\n<p>parallelize 方法可以传递参数：集合</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package org.<span class=\"property\">example</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.<span class=\"property\">apache</span>.<span class=\"property\">spark</span>.<span class=\"property\">SparkConf</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.<span class=\"property\">apache</span>.<span class=\"property\">spark</span>.<span class=\"property\">api</span>.<span class=\"property\">java</span>.<span class=\"property\">JavaRDD</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.<span class=\"property\">apache</span>.<span class=\"property\">spark</span>.<span class=\"property\">api</span>.<span class=\"property\">java</span>.<span class=\"property\">JavaSparkContext</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.<span class=\"property\">util</span>.<span class=\"property\">Arrays</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.<span class=\"property\">util</span>.<span class=\"property\">List</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">public <span class=\"keyword\">class</span> <span class=\"title class_\">Main</span> &#123;</span><br><span class=\"line\">    public <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span>(<span class=\"params\"><span class=\"built_in\">String</span>[] args</span>) &#123;</span><br><span class=\"line\">        <span class=\"title class_\">System</span>.<span class=\"property\">out</span>.<span class=\"title function_\">println</span>(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"title class_\">SparkConf</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setMaster</span>(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setAppName</span>(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"title class_\">JavaSparkContext</span> javaSparkContext = <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"comment\">//JavaSparkContext javaSparkContext = new JavaSparkContext(&quot;local&quot;,&quot;spark&quot;);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 对接数据源</span></span><br><span class=\"line\">        <span class=\"title class_\">List</span>&lt;<span class=\"title class_\">String</span>&gt; names = <span class=\"title class_\">Arrays</span>.<span class=\"title function_\">asList</span>(<span class=\"string\">&quot;zhangsan&quot;</span>, <span class=\"string\">&quot;lisi&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">JavaRDD</span>&lt;<span class=\"title class_\">String</span>&gt; rdd = javaSparkContext.<span class=\"title function_\">parallelize</span>(names);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">List</span>&lt;<span class=\"title class_\">String</span>&gt; collect = rdd.<span class=\"title function_\">collect</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        collect.<span class=\"title function_\">forEach</span>(<span class=\"title class_\">System</span>.<span class=\"property\">out</span>::println);</span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.<span class=\"title function_\">close</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"对接磁盘数据\"><a class=\"markdownIt-Anchor\" href=\"#对接磁盘数据\">#</a> 对接磁盘数据</h4>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public <span class=\"keyword\">class</span> <span class=\"title class_\">Main1</span> &#123;</span><br><span class=\"line\">    public <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span>(<span class=\"params\"><span class=\"built_in\">String</span>[] args</span>) &#123;</span><br><span class=\"line\">        <span class=\"title class_\">System</span>.<span class=\"property\">out</span>.<span class=\"title function_\">println</span>(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"title class_\">SparkConf</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setMaster</span>(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.<span class=\"title function_\">setAppName</span>(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"title class_\">JavaSparkContext</span> javaSparkContext = <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">JavaRDD</span>&lt;<span class=\"title class_\">String</span>&gt; stringJavaRDD = javaSparkContext.<span class=\"title function_\">textFile</span>(<span class=\"string\">&quot;C:\\\\Users\\\\Administrator\\\\IdeaProjects\\\\MapReduceDemo\\\\data\\\\text&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title class_\">List</span>&lt;<span class=\"title class_\">String</span>&gt; collect = stringJavaRDD.<span class=\"title function_\">collect</span>();</span><br><span class=\"line\">        collect.<span class=\"title function_\">forEach</span>(<span class=\"title class_\">System</span>.<span class=\"property\">out</span>::println);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.<span class=\"title function_\">close</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"磁盘数据分区\"><a class=\"markdownIt-Anchor\" href=\"#磁盘数据分区\">#</a> 磁盘数据分区</h4>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">partition</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Hello world!&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建spark配置对象</span></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 构建spark运行环境</span></span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// idea开发时相对路径默认以项目根路径为基准</span></span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(&quot;C:\\\\Users\\\\Administrator\\\\IdeaProjects\\\\MapReduceDemo\\\\data\\\\text&quot;);</span></span><br><span class=\"line\">        <span class=\"comment\">// textfile 第二个参数最小分区数，不传递的时候使用默认值</span></span><br><span class=\"line\">        <span class=\"comment\">// 1.textFile可以传递第二个参数:minPartitions(最小分区数)</span></span><br><span class=\"line\">        <span class=\"comment\">//参数可以不需要传递的,那么Spark会采用默认值</span></span><br><span class=\"line\">        <span class=\"comment\">//minPartitions = math.min(defaultParallelism,2)</span></span><br><span class=\"line\">        <span class=\"comment\">//2.使用配置参数:spark.default.parallelism=&gt;4=&gt; 4=&gt; math.min(参数,2)</span></span><br><span class=\"line\">        <span class=\"comment\">//3.采用环境默认总核值=&gt;math.min(总核数,2)</span></span><br><span class=\"line\">        </span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data\\\\text&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        stringJavaRDD.saveAsTextFile(<span class=\"string\">&quot;output1222&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// 释放资源</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"内存数据源分区数据分配\"><a class=\"markdownIt-Anchor\" href=\"#内存数据源分区数据分配\">#</a> 内存数据源，分区数据分配</h4>\n<p>​      数据分配方式       (i*length)/numSlices,(((i + 1) * 1) * length) /numSlices)</p>\n<h4 id=\"磁盘数据源\"><a class=\"markdownIt-Anchor\" href=\"#磁盘数据源\">#</a> 磁盘数据源</h4>\n<p>Spark 不支持文件操作的。文件操作都是由 Hadoop 完成的</p>\n<p>Hadoop 进行文件切片数量的计算和文件数据存储计算规则不样</p>\n<p>1. 分区数量计算的时候，考虑的是尽可能的平均：按字节来计算</p>\n<p>2. 分区数据的存储是考虑业务数据的完整性：按照行来读取</p>\n<p>读取数据时，还需要考虑数据偏移量，偏移量从 0 开始的。</p>\n<p>读取数据时，相同的偏移量不能重复读取。</p>\n<p>使用 spark 时，数据不能全放一行，会造成数据倾斜</p>\n<h4 id=\"transformation-转换算子\"><a class=\"markdownIt-Anchor\" href=\"#transformation-转换算子\">#</a> transformation 转换算子</h4>\n<h5 id=\"map\"><a class=\"markdownIt-Anchor\" href=\"#map\">#</a> map</h5>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223623531.png\" alt=\"image-20240804223623531\"></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Scala</span>语言中可以将无关的数据封装在一起,形成一个整体,称之为元素的组合,简称为【元组】</span><br><span class=\"line\">如果想要访问元组中的数据,必须采用顺序号</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> kv1 = (<span class=\"string\">&quot;haha&quot;</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"type\">JDK1</span><span class=\"number\">.8</span>以后也存在元组,采用特殊的类:<span class=\"type\">TupleX</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">String</span>&gt; tuple2 = <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;abc&quot;</span>, <span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">System</span>.out.println(tuple2._1);</span><br><span class=\"line\">        <span class=\"type\">System</span>.out.println(tuple2._2());</span><br><span class=\"line\">        tuple中最大容量为<span class=\"number\">22</span></span><br><span class=\"line\">        使用时可以 ._1 也可以 ._1()</span><br></pre></td></tr></table></figure>\n<h6 id=\"函数式编程\"><a class=\"markdownIt-Anchor\" href=\"#函数式编程\">#</a> 函数式编程</h6>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.rdd;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Arrays;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">operator3</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>),<span class=\"number\">2</span>).map(NumberTest::mul2).collect().forEach(System.out::println);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NumberTest</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span>  <span class=\"type\">int</span> <span class=\"title function_\">mul2</span><span class=\"params\">(Integer num2)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num2 *= <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223648424.png\" alt=\"image-20240804223648424\"></p>\n<p>RDD 不会保存数据，不会等每一个 rdd 执行完再执行下一个</p>\n<h5 id=\"filter\"><a class=\"markdownIt-Anchor\" href=\"#filter\">#</a> filter</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">filter</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// TODO filter 过滤 对数据源中数据进行筛选  满足保留，不满足丢弃</span></span><br><span class=\"line\">        <span class=\"comment\">// 返回结果为true满足 ，返回false不满足</span></span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Integer&gt; filter = parallelize111.filter(new Function&lt;Integer, Boolean&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//            @Override</span></span><br><span class=\"line\"><span class=\"comment\">//            public Boolean call(Integer num) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                return true;</span></span><br><span class=\"line\"><span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;);</span></span><br><span class=\"line\"><span class=\"comment\">//        filter.collect().forEach(System.out::println);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; filterrdd = parallelize111.filter(</span><br><span class=\"line\">                num -&gt; <span class=\"literal\">true</span></span><br><span class=\"line\">        );</span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Integer&gt; filterrdd111 = parallelize111.filter(</span></span><br><span class=\"line\"><span class=\"comment\">//                num -&gt; (num % 2 ==1)</span></span><br><span class=\"line\"><span class=\"comment\">//        );</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">/// filter 执行过程中可能会造成数据倾斜</span></span><br><span class=\"line\">        filterrdd.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"flatmap\"><a class=\"markdownIt-Anchor\" href=\"#flatmap\">#</a> flatmap</h5>\n<p>数据扁平化，扁平映射（整体变为个体）</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">flatmap</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>), Arrays.asList(<span class=\"number\">3</span>, <span class=\"number\">4</span>)), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Integer&gt; integerJavaRDD = parallelizerdd.flatMap(new FlatMapFunction&lt;List&lt;Integer&gt;, Integer&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                     @Override</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                     public Iterator&lt;Integer&gt; call(List&lt;Integer&gt; integers) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                         return integers.iterator();</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                     &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//                                                                 &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        );</span></span><br><span class=\"line\"></span><br><span class=\"line\">                JavaRDD&lt;Integer&gt; integerJavaRDD = parallelizerdd.flatMap(<span class=\"keyword\">new</span> <span class=\"title class_\">FlatMapFunction</span>&lt;List&lt;Integer&gt;, Integer&gt;() &#123;</span><br><span class=\"line\">                                                                     <span class=\"meta\">@Override</span></span><br><span class=\"line\">                                                                     <span class=\"keyword\">public</span> Iterator&lt;Integer&gt; <span class=\"title function_\">call</span><span class=\"params\">(List&lt;Integer&gt; integers)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                                                                         List&lt;Integer&gt; objectArrayList = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;&gt;();</span><br><span class=\"line\">                                                                         integers.forEach(num -&gt; objectArrayList.add(num * <span class=\"number\">2</span>));</span><br><span class=\"line\">                                                                         <span class=\"keyword\">return</span> objectArrayList.iterator();</span><br><span class=\"line\">                                                                     &#125;</span><br><span class=\"line\">                                                                 &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">        integerJavaRDD.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">data/text：</span><br><span class=\"line\">hadoop python</span><br><span class=\"line\">java php golang</span><br><span class=\"line\">B</span><br><span class=\"line\">V</span><br><span class=\"line\">D</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">flatmap</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>), Arrays.asList(<span class=\"number\">3</span>, <span class=\"number\">4</span>)), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data/text&quot;</span>);</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"groupby\"><a class=\"markdownIt-Anchor\" href=\"#groupby\">#</a> groupby</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">groupby</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 按照指定的规则分组数据</span></span><br><span class=\"line\"><span class=\"comment\">//        parallelizerdd.groupBy(new Function&lt;Integer, Object&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//            public Object call(Integer integers) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                // 返回值是数据对应组的名称，相同名称的数据防止在同一个组中</span></span><br><span class=\"line\"><span class=\"comment\">//                if (integers % 2 == 0)</span></span><br><span class=\"line\"><span class=\"comment\">//                &#123;return &quot;123&quot;;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">//                else &#123;return &quot;456&quot;;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;).collect().forEach(System.out::println);</span></span><br><span class=\"line\"><span class=\"comment\">//(123,[2, 4, 6])</span></span><br><span class=\"line\"><span class=\"comment\">//(456,[1])</span></span><br><span class=\"line\">        parallelizerdd.groupBy(num -&gt; num % <span class=\"number\">2</span> == <span class=\"number\">0</span>).collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"shuffle\"><a class=\"markdownIt-Anchor\" href=\"#shuffle\">#</a> shuffle</h5>\n<p>默认情况下，数据处理后，所在的分区不会发生变化，但是 groupBy 方法例外</p>\n<p>Spark 在数据处理中，要求同一个组的数据必须在同一个分区中</p>\n<p>所以分组操作会将数据分区打乱重新组合，在 spark 中称为 shuffle</p>\n<p>一个分区可以存放多个组，，所有数据必须分组后才能继续执行操作</p>\n<p>RDD 对象不能保存数据，当前 groupBy 操作会将数据保存到磁盘文件中，保证数据全部分组后执行后续操作</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223703724.png\" alt=\"image-20240804223703724\"></p>\n<p>shuffle 操作一定会落盘</p>\n<p>shuffle 操作有可能会导致资源浪费</p>\n<p>Spark 中含有 shuffle 操作的方法都有改变分区的能力</p>\n<p>RDD 的分区和 task 有关系</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">groupby2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>), <span class=\"number\">3</span>);</span><br><span class=\"line\">        parallelizerdd.groupBy(num -&gt; num % <span class=\"number\">2</span> == <span class=\"number\">0</span>,<span class=\"number\">2</span>).collect().forEach(System.out::println);</span><br><span class=\"line\">        Thread.sleep(<span class=\"number\">100000L</span>);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>shuffle 会将完整的计算流程一分为二，其中一部分任务会写磁盘，另外一部分的任务会读磁盘</p>\n<p>写磁盘的操作不完成，不允许读磁盘</p>\n<h5 id=\"distinct\"><a class=\"markdownIt-Anchor\" href=\"#distinct\">#</a> distinct</h5>\n<p>hashset 是单点去重</p>\n<p>distinct 是分布式去重，采用分组 + shuffle 方式</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">distinct</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>), <span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        parallelizerdd.distinct().collect().forEach(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//parallelizerdd.groupBy(num -&gt; num % 2 == 0,2).collect().forEach(System.out::println);</span></span><br><span class=\"line\">        <span class=\"comment\">//Thread.sleep(100000L);</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"sortby\"><a class=\"markdownIt-Anchor\" href=\"#sortby\">#</a> sortby</h5>\n<p>按照指定规则排序</p>\n<p>第一个参数表示排序规则</p>\n<p>Spark 会为每一个数据增加一个标记，然后按照标记对数据进行排序</p>\n<p>第二个参数表示排序的方式：升序 (true), 降序 (false)</p>\n<p>第三个参数表示分区数量</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sortby</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">12</span>, <span class=\"number\">52</span>, <span class=\"number\">4</span> ,<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>), <span class=\"number\">3</span>);</span><br><span class=\"line\">        parallelizerdd.saveAsTextFile(<span class=\"string\">&quot;sort222&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        parallelizerdd.sortBy(<span class=\"keyword\">new</span> <span class=\"title class_\">Function</span>&lt;Integer, Object&gt;() &#123;</span><br><span class=\"line\">                    <span class=\"meta\">@Override</span></span><br><span class=\"line\">                    <span class=\"keyword\">public</span> Object <span class=\"title function_\">call</span><span class=\"params\">(Integer integerssss)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> integerssss;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;,<span class=\"literal\">true</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">               <span class=\"comment\">// .collect()</span></span><br><span class=\"line\">                <span class=\"comment\">//.forEach(System.out::println);</span></span><br><span class=\"line\">                .saveAsTextFile(<span class=\"string\">&quot;sort333&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//parallelizerdd.groupBy(num -&gt; num % 2 == 0,2).collect().forEach(System.out::println);</span></span><br><span class=\"line\">        <span class=\"comment\">//Thread.sleep(100000L);</span></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>return “”+integerssss;    // 按照字典排序</p>\n<h5 id=\"coalesce\"><a class=\"markdownIt-Anchor\" href=\"#coalesce\">#</a> coalesce</h5>\n<p>缩减分区</p>\n<p>coalesce 方法默认没有 shuffle 功能，所以数据不会被打击乱重新组合，所以如果要扩大分区是无法实现的</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filterrdd.coalesce(<span class=\"number\">3</span>,<span class=\"literal\">true</span>);    <span class=\"comment\">// shuffle 为true的时候可以扩大分区</span></span><br></pre></td></tr></table></figure>\n<h5 id=\"repartition\"><a class=\"markdownIt-Anchor\" href=\"#repartition\">#</a> repartition</h5>\n<p>重分区，就是设定 shuffle=true 的 coalesce 方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filterrdd.repartition(<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"kv\"><a class=\"markdownIt-Anchor\" href=\"#kv\">#</a> kv</h4>\n<p>Spark RDD 会整体数据的处理就称之为单值类型的数据处理</p>\n<p>Spark RDD 会 KV 数据个体的处理就称之为 KV 类型的数据处理：K 和 V 不作为整体使用</p>\n<p>mapValues 方法只对 V 进行处理，K 不做任何操作</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kv</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(1, 2, 4 ,6, 7, 8, 1, 2), 3);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        Tuple2&lt;String, Integer&gt; a = <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>);</span><br><span class=\"line\">        Tuple2&lt;String, Integer&gt; b = <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Arrays.asList(a, b);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaPairRDD&lt;String, Integer&gt; parallelized = javaSparkContext.parallelizePairs(list);</span><br><span class=\"line\">        parallelized.mapValues(num -&gt; num*<span class=\"number\">2</span>).collect().forEach(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kv2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(1, 2, 4 ,6, 7, 8, 1, 2), 3);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        List&lt;Integer&gt; integerList = Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; integerJavaRDD = javaSparkContext.parallelize(integerList);</span><br><span class=\"line\"></span><br><span class=\"line\">        integerJavaRDD.mapToPair(</span><br><span class=\"line\">                num -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(num, num*<span class=\"number\">2</span>)</span><br><span class=\"line\">        ).collect().forEach(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        integerJavaRDD.mapToPair(</span></span><br><span class=\"line\"><span class=\"comment\">//                num -&gt; new Tuple2&lt;&gt;(num, num*2)</span></span><br><span class=\"line\"><span class=\"comment\">//        ).mapValues(num-&gt;num*2).collect().forEach(System.out::println);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"mapvalue\"><a class=\"markdownIt-Anchor\" href=\"#mapvalue\">#</a> mapvalue</h5>\n<h5 id=\"groupbykey\"><a class=\"markdownIt-Anchor\" href=\"#groupbykey\">#</a> groupbykey</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kv3_groupbykey</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local[*]&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; parallelized = javaSparkContext.parallelize(Arrays.asList(new Tuple2&lt;&gt;(&quot;a&quot;, 1), new Tuple2&lt;&gt;(&quot;b&quot;, 2), new Tuple2&lt;&gt;(&quot;a&quot;, 3), new Tuple2&lt;&gt;(&quot;b&quot;, 4)));</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        JavaPairRDD&lt;String, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; stringIterableJavaPairRDD = parallelized.groupBy(t -&gt; t._1);</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        stringIterableJavaPairRDD.collect().forEach(System.out::println);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// (a,[(a,1), (a,3)])</span></span><br><span class=\"line\">        <span class=\"comment\">//(b,[(b,2), (b,4)])</span></span><br><span class=\"line\"></span><br><span class=\"line\">       javaSparkContext.parallelizePairs((Arrays.asList(<span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">1</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">2</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;a&quot;</span>, <span class=\"number\">3</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(<span class=\"string\">&quot;b&quot;</span>, <span class=\"number\">4</span>)))).groupByKey().collect().forEach(System.out::println);</span><br><span class=\"line\">       javaSparkContext.close();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//(a,[1, 3])</span></span><br><span class=\"line\"><span class=\"comment\">//(b,[2, 4])</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>groupby 底层调用 greoupbykey</p>\n<p>groupbykey 有 shuffle</p>\n<h5 id=\"reducebykey\"><a class=\"markdownIt-Anchor\" href=\"#reducebykey\">#</a> reducebykey</h5>\n<p>reduceByKey 方法的作用：将 KV 类型的数据按照 K 对 V 进行 reduce (将多个值聚合成 1 个值) 操作</p>\n<p>基本思想：两两计算</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaPairRDD&lt;String ,Integer&gt; wordcountrdd = parallelized.reduceByKey(Integer::sum);</span><br></pre></td></tr></table></figure>\n<h5 id=\"sortbykey\"><a class=\"markdownIt-Anchor\" href=\"#sortbykey\">#</a> sortbykey</h5>\n<p>groupByKey : 按照 K 对 V 进行分组</p>\n<p>reduceByKey 按照 K 对进行两两聚合</p>\n<p>sortByKey 按照 K 排序</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaPairRDD&lt;String ,Integer&gt; sortrdd = parallelized.sortByKey();</span><br></pre></td></tr></table></figure>\n<h4 id=\"rdd行动算子\"><a class=\"markdownIt-Anchor\" href=\"#rdd行动算子\">#</a> RDD 行动算子</h4>\n<p>RDD 的行动算子会触发作业 (Job) 的执行</p>\n<p>转换算子的目的：将旧的 RDD 转换成新的 RDD, 为了组合多个 RDD 的功能</p>\n<p>返回值是 rdd，是转换算子。具体值是行动算子</p>\n<p>collect 将 executor 执行的结果按照分区的数据拉取回到 driver，将结果组合成集合对象</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaRDD&lt;Integer&gt; integerJavaRDD = parallelize111.map(num -&gt; num * <span class=\"number\">2</span>);</span><br><span class=\"line\"><span class=\"comment\">// collect 是行动算子</span></span><br><span class=\"line\">List&lt;Integer&gt; collect = integerJavaRDD.collect();</span><br><span class=\"line\">collect.forEach(System.out::println);</span><br></pre></td></tr></table></figure>\n<p>Spark 在编写代码时，调用转换算子，并不会真正执行，因为只是在 Driver 端组合功能</p>\n<p>所以当前的代码其实就是在 Driver 端执行</p>\n<p>所以当前 main 方法也称之为 driver 方法，当前运行 main 纟我程，也称之 Driver 线程</p>\n<p>转换算子中的逻辑代码是在 Executor 端执行的。并不会在 tDriver 端调用和执行。</p>\n<p>RDD 封装的逻辑其实就是转换算子中的逻辑</p>\n<p>集合数据</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223718854.png\" alt=\"image-20240804223718854\"></p>\n<p>文件：读取切片</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223732201.png\" alt=\"image-20240804223732201\"></p>\n<p>collect 方法就是将 Executor 端执行的结果按照分区的顺序位取 (采集) 回到 Driver 端，将结果组合成集合对象</p>\n<p>collect 方法可能会导致多个 Executor 的大量数据拉取到 Driiver 端，导致内存溢出，所以生成环境慎用</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>),<span class=\"number\">2</span>);</span><br><span class=\"line\"> JavaRDD&lt;Integer&gt; integerJavaRDD = parallelize111.map(num -&gt; num * <span class=\"number\">2</span>);</span><br><span class=\"line\"> <span class=\"comment\">// collect 是行动算子,用于采集数据</span></span><br><span class=\"line\"> List&lt;Integer&gt; collect = integerJavaRDD.collect();</span><br><span class=\"line\"> collect.forEach(System.out::println);</span><br><span class=\"line\"> <span class=\"comment\">// count获取结果数量</span></span><br><span class=\"line\"> <span class=\"type\">long</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> integerJavaRDD.count();</span><br><span class=\"line\"> <span class=\"comment\">// 获取结果的第一个</span></span><br><span class=\"line\"> <span class=\"type\">Integer</span> <span class=\"variable\">first</span> <span class=\"operator\">=</span> integerJavaRDD.first();</span><br><span class=\"line\"> <span class=\"comment\">// 从结果中获取前n个</span></span><br><span class=\"line\"> List&lt;Integer&gt; take = integerJavaRDD.take(<span class=\"number\">3</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\">// countbykey 将结果按照key计算数量</span></span><br><span class=\"line\"> <span class=\"comment\">// &#123;4=1, 2=1, 1=1, 3=1&#125;</span></span><br><span class=\"line\"> JavaPairRDD&lt;Integer, Integer&gt; integerIntegerJavaPairRDD = parallelize111.mapToPair(num -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(num, num));</span><br><span class=\"line\"> Map&lt;Integer, Long&gt; integerLongMap = integerIntegerJavaPairRDD.countByKey();</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 保存</span></span><br><span class=\"line\"> integerIntegerJavaPairRDD.saveAsTextFile(<span class=\"string\">&quot;a&quot;</span>);</span><br><span class=\"line\"> <span class=\"comment\">// 保存对象</span></span><br><span class=\"line\"> integerIntegerJavaPairRDD.saveAsObjectFile(<span class=\"string\">&quot;bb&quot;</span>);</span><br><span class=\"line\"> <span class=\"comment\">// 单线循环</span></span><br><span class=\"line\"> parallelize111.collect().forEach(System.out::println);</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\">// 分布式循环，占内存比较小，执行效率低</span></span><br><span class=\"line\"> parallelize111.foreach(</span><br><span class=\"line\">         System.out::println</span><br><span class=\"line\"> );</span><br><span class=\"line\"> <span class=\"comment\">// 执行效率高，依托于内存大小</span></span><br><span class=\"line\"> parallelize111.foreachPartition(System.out::println);</span><br><span class=\"line\"></span><br><span class=\"line\"> javaSparkContext.close();</span><br></pre></td></tr></table></figure>\n<p>main 方法也叫 driver 方法，foreach 是在 executor 执行</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">action_serialize</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>), <span class=\"number\">2</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Student1</span> <span class=\"variable\">s</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Student1</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        parallelize111.foreach(</span><br><span class=\"line\">                num -&gt; &#123;</span><br><span class=\"line\">                    System.out.println(s.age + num);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student1</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"variable\">age</span> <span class=\"operator\">=</span> <span class=\"number\">30</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Student 想 new ，得在 executor 上拉取</p>\n<p>在 Executor 端循环遍历的时候使用到了 Driver 端对象</p>\n<p>运行过程中，就需要将 Driver 端的对象通过网络传递到 Executor 端，否则无法使用</p>\n<p>传输的对象必须要实现可序列化接口，否则无法传递</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        JavaRDD&lt;String&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class=\"string\">&quot;haha&quot;</span>,<span class=\"string\">&quot;Haha&quot;</span>), <span class=\"number\">2</span>);</span><br><span class=\"line\">        <span class=\"type\">Search</span> <span class=\"variable\">search</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Search</span>(<span class=\"string\">&quot;H&quot;</span>);</span><br><span class=\"line\">        search.match(parallelize111);</span><br><span class=\"line\"></span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Search</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String query;            <span class=\"comment\">// query是成员变量，需要序列化</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Search</span><span class=\"params\">(String query)</span>&#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.query = query;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span>  <span class=\"title function_\">match</span><span class=\"params\">(JavaRDD&lt;String&gt; rdd)</span> &#123;</span><br><span class=\"line\">        rdd.filter(</span><br><span class=\"line\">                s -&gt; s.startsWith(query)</span><br><span class=\"line\">        ).collect().forEach(System.out::println);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">class Search implements Serializable &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    private String query;            // query是成员变量，需要序列化</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    public Search(String query)&#123;</span></span><br><span class=\"line\"><span class=\"comment\">        this.query = query;</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    public void  match(JavaRDD&lt;String&gt; rdd) &#123;</span></span><br><span class=\"line\"><span class=\"comment\">        String q = this.query;         // q是局部变量，在栈中，和search无关，不需要序列化</span></span><br><span class=\"line\"><span class=\"comment\">        rdd.filter(</span></span><br><span class=\"line\"><span class=\"comment\">                s -&gt; s.startsWith(q)</span></span><br><span class=\"line\"><span class=\"comment\">        ).collect().forEach(System.out::println);</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;</span></span><br></pre></td></tr></table></figure>\n<p>rdd 算子 (方法) 的逻辑代码是在 executo 执行的，其他的是在 driver 执行的</p>\n<p>collect 是行动算子，没有逻辑代码</p>\n<p>filter 中的成为逻辑代码</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//        parallelize111.foreach(</span></span><br><span class=\"line\"><span class=\"comment\">//             executor端执行</span></span><br><span class=\"line\"><span class=\"comment\">//                num -&gt; System.out.println(num)</span></span><br><span class=\"line\"><span class=\"comment\">//        );</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// jdk1.8 函数式编程采用对象模拟,使用这种方式会报错，但是系统的类无法改写继承序列化</span></span><br><span class=\"line\">        parallelize111.foreach(</span><br><span class=\"line\">        <span class=\"comment\">// 在driver端创建</span></span><br><span class=\"line\">                System.out::println</span><br><span class=\"line\">                <span class=\"comment\">// PrintStream out = System.out;</span></span><br><span class=\"line\">                <span class=\"comment\">// out::println</span></span><br><span class=\"line\">        );</span><br></pre></td></tr></table></figure>\n<h4 id=\"kryo\"><a class=\"markdownIt-Anchor\" href=\"#kryo\">#</a> kryo</h4>\n<p>Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;kryo&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">5.0</span><span class=\"number\">.3</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.atguigu.bean.User;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.function.Function;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Arrays;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Test02_Kryo</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> ClassNotFoundException &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 1.创建配置对象</span></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkCore&quot;</span>)</span><br><span class=\"line\">                <span class=\"comment\">// 替换默认的序列化机制</span></span><br><span class=\"line\">                .set(<span class=\"string\">&quot;spark.serializer&quot;</span>, <span class=\"string\">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class=\"line\">                <span class=\"comment\">// 注册需要使用kryo序列化的自定义类</span></span><br><span class=\"line\">                .registerKryoClasses(<span class=\"keyword\">new</span> <span class=\"title class_\">Class</span>[]&#123;Class.forName(<span class=\"string\">&quot;com.atguigu.bean.User&quot;</span>)&#125;);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 2. 创建sparkContext</span></span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">sc</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(conf);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 3. 编写代码</span></span><br><span class=\"line\">        <span class=\"type\">User</span> <span class=\"variable\">zhangsan</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">User</span>(<span class=\"string\">&quot;zhangsan&quot;</span>, <span class=\"number\">13</span>);</span><br><span class=\"line\">        <span class=\"type\">User</span> <span class=\"variable\">lisi</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">User</span>(<span class=\"string\">&quot;lisi&quot;</span>, <span class=\"number\">13</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">        JavaRDD&lt;User&gt; userJavaRDD = sc.parallelize(Arrays.asList(zhangsan, lisi), <span class=\"number\">2</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">        JavaRDD&lt;User&gt; mapRDD = userJavaRDD.map(<span class=\"keyword\">new</span> <span class=\"title class_\">Function</span>&lt;User, User&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> User <span class=\"title function_\">call</span><span class=\"params\">(User v1)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">User</span>(v1.getName(), v1.getAge() + <span class=\"number\">1</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"> </span><br><span class=\"line\">        mapRDD. collect().forEach(System.out::println);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// 4. 关闭sc</span></span><br><span class=\"line\">        sc.stop();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"依赖\"><a class=\"markdownIt-Anchor\" href=\"#依赖\">#</a> 依赖</h4>\n<p>RDD 转换算子 (方法):RDD 可以通过方法将旧的 RDD 转换成新的 RDD</p>\n<p>RDD 依赖：Spark 中相邻的 2 个 RDD 之间存在的依赖关系</p>\n<p>连续的依赖关系称为血缘关系</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223744879.png\" alt=\"image-20240804223744879\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">flatmap</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>();</span><br><span class=\"line\">        sparkConf.setMaster(<span class=\"string\">&quot;local&quot;</span>);</span><br><span class=\"line\">        sparkConf.setAppName(<span class=\"string\">&quot;spark&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">        <span class=\"comment\">//JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4)), 2);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//System.out.println(parallelizerdd.toDebugString());</span></span><br><span class=\"line\"></span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data/text&quot;</span>);</span><br><span class=\"line\">        System.out.println(stringJavaRDD.toDebugString());</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;============&quot;</span>);</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\">        System.out.println(stringJavaRDD1.toDebugString());</span><br><span class=\"line\">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">1</span>) data/text MapPartitionsRDD[<span class=\"number\">1</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\"> |  data/text HadoopRDD[<span class=\"number\">0</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\">============</span><br><span class=\"line\">(<span class=\"number\">1</span>) MapPartitionsRDD[<span class=\"number\">2</span>] at flatMap at flatmap.java:<span class=\"number\">23</span> []</span><br><span class=\"line\"> |  data/text MapPartitionsRDD[<span class=\"number\">1</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\"> |  data/text HadoopRDD[<span class=\"number\">0</span>] at textFile at flatmap.java:<span class=\"number\">20</span> []</span><br><span class=\"line\"> </span><br><span class=\"line\"> shuffle +-  代表两段流程</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class=\"string\">&quot;data/text&quot;</span>);</span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(stringJavaRDD.toDebugString());</span></span><br><span class=\"line\">        System.out.println(stringJavaRDD.rdd().dependencies());</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;============&quot;</span>);</span><br><span class=\"line\">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(stringJavaRDD1.toDebugString());</span></span><br><span class=\"line\">        System.out.println(stringJavaRDD1.rdd().dependencies());</span><br><span class=\"line\"></span><br><span class=\"line\">        stringJavaRDD1.groupBy(num -&gt; num);</span><br><span class=\"line\">        System.out.println(stringJavaRDD1.rdd().dependencies());</span><br><span class=\"line\">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class=\"line\">        javaSparkContext.close();</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">List(org.apache.spark.OneToOneDependency@4b5a078a)</span><br><span class=\"line\">============</span><br><span class=\"line\">List(org.apache.spark.OneToOneDependency@33f2df51)</span><br><span class=\"line\"><span class=\"number\">24</span>/<span class=\"number\">06</span>/<span class=\"number\">25</span> <span class=\"number\">17</span>:<span class=\"number\">16</span>:<span class=\"number\">52</span> INFO FileInputFormat: Total input files to process : <span class=\"number\">1</span></span><br><span class=\"line\">List(org.apache.spark.OneToOneDependency@33f2df51)</span><br></pre></td></tr></table></figure>\n<p>onetoonedep 窄依赖</p>\n<p>shuffledep  宽依赖</p>\n<p>rdd 的依赖关系是 rdd 对象中分区数据的关系</p>\n<p>窄依赖：如果上游 rdd 一个分区的数据被下游 rdd 的一个分区独享</p>\n<p>宽依赖：如果上游 rdd 一个分区的数据被下游 rdd 的多个分区共享。会将分区数据打乱重新组合，所以此层存在 shuffle 操作</p>\n<p>依赖关系和任务数量，阶段数量</p>\n<p>作业 (Job): 行动算子执行时，会触发作业的执行 (ActiveJob)</p>\n<p>阶段 (Stage): 一个 job 中 RDD 的计算流程，默认就一个完整的阶段，但是如果计算流程中存在 shuffle, 那么流程就会一分为二。分开的每一段就称之为 stage，前一个阶段不执行完，后一个阶段不允许执行</p>\n<p>任务 (Task): 每个 Executor 执行的计算单元</p>\n<p>阶段的数量和 shuffle 依赖的数量有关系：1+shuffle 依赖的数量</p>\n<p>任务（分区）的数量就是每个阶段分区的数量之和，一般推荐分区数量为资源核数的 2-3 倍</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223805444.png\" alt=\"image-20240804223805444\"></p>\n<p>任务 (Task): 每个 Executor 执行的计算单元</p>\n<p>任务的数量其实就是每个阶段最后一个 RDD 分区的数量之和</p>\n<p>移动数据不如移动计算</p>\n<h4 id=\"持久化\"><a class=\"markdownIt-Anchor\" href=\"#持久化\">#</a> 持久化</h4>\n<p>持久化：将对象长时间的保存</p>\n<p>序列化：内存中对象 =&gt;byte 序列 (byte 数组)</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223856875.png\" alt=\"image-20240804223856875\"></p>\n<p>maprdd.cache();</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223912457.png\" alt=\"image-20240804223912457\"></p>\n<p>maprdd.persist(Storage.MEMORY_ONLY());</p>\n<p>cache 方法底层调用 persist.  maprdd.persist (Storage.MEMORY_ONLY ());  ===  cache ()</p>\n<p>// 落盘持久化</p>\n<p>maprdd.persist(Storage.DISK_ONLY());</p>\n<p>MEMORY_ONLY 超出数据直接丢弃</p>\n<p>MEMORY_AND_DISK.  内存满了放磁盘</p>\n<p>MEMORY_ONLY_SER 序列化后再存内存</p>\n<p>MEMORY_ONLY_SER_2. 副本 2 份</p>\n<h5 id=\"checkpoint\"><a class=\"markdownIt-Anchor\" href=\"#checkpoint\">#</a> checkpoint</h5>\n<p>将计算结果保存到 hdfs 或者本地文件路径，实现不同进程共享</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkConf);</span><br><span class=\"line\">javaSparkContext.setCheckpointDir(<span class=\"string\">&quot;cp&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">mapRDD.cache();</span><br><span class=\"line\">mapRDD.checkPoint();</span><br></pre></td></tr></table></figure>\n<p>检查点目的是 rdd 结果长时间保存，所以需要保证数据安全，会从头再跑一遍。把第二遍结果放到里面</p>\n<p>性能比较低，可以在检查点之前执行 cache，将数据缓存</p>\n<p>cache 方法会在血缘关系中增加依赖关系</p>\n<p>checkpoint 方法改变血缘关系</p>\n<p>每个 shuffle 都自动带有缓存，为了提高 shuffle 算子的性能</p>\n<p>如果重复调用相同规则的 shuffle 算子，第二个 shfulle 算子不会有相同 shuffle 操作</p>\n<p>使用完缓存，可以使用 unpersist 释放缓存</p>\n<h4 id=\"分区器\"><a class=\"markdownIt-Anchor\" href=\"#分区器\">#</a> 分区器</h4>\n<p>数据分区的规则</p>\n<p>计算后数据所在的分区是通过 Spark 的内部计算 (分区) 完成我。尽可能均衡一些（hash）</p>\n<p>reducebykey 需要传递两个参数，第一个参数是数据分区的规则，第二个参数是数据聚合逻辑</p>\n<p>第一个参数可以不用传递，使用时会使用默认分区规则。默认分区规则中使用 HashPartitioner</p>\n<p>hashpartitioner 中有一个方法叫 getpartition，需要传递一个参数 key，返回一个值，表示分区编号，从 0 开始</p>\n<p>得到一个分区编号，key.hashcode % partnum (hash 取余)</p>\n<p>只有 kv 类型的有分区器</p>\n<p>rdd.partitioner()</p>\n<p>Spark 目前支持 Hash 分区、Range 分区和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区和 Reduce 的个数。</p>\n<p>自定义分区器</p>\n<p>1. 创建自定义类</p>\n<p>2. 继承抽象类 Partitioner</p>\n<p>3. 重写方法 (2)</p>\n<p>4. 构建对象，在算子中使用</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Part</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Partitioner</span> &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 指定分区数量</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">numPartitions</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">3</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 很久数据的key来获取数据存储的分区编号</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getPartition</span><span class=\"params\">(Object key)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;a&quot;</span>.equals(key))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (<span class=\"string\">&quot;b&quot;</span>.equals(key)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">mapRDD.reduceByKey(<span class=\"keyword\">new</span> <span class=\"title class_\">Part</span>(),Integer::sum)</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Part</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Partitioner</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> numPart;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Part</span><span class=\"params\">(<span class=\"type\">int</span> num)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.numPart = num;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 指定分区数量</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">numPartitions</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">3</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 很久数据的key来获取数据存储的分区编号</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getPartition</span><span class=\"params\">(Object key)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;a&quot;</span>.equals(key))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (<span class=\"string\">&quot;b&quot;</span>.equals(key)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">hashCode</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> numPart;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">equals</span><span class=\"params\">(Object obj)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (obj <span class=\"keyword\">instanceof</span> Part) &#123;</span><br><span class=\"line\">            <span class=\"type\">Part</span> <span class=\"variable\">other</span> <span class=\"operator\">=</span> (Part)obj;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"built_in\">this</span>.numPart == other.numPart;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RDD 在 foreach 循环时，逻辑代码和操作全部都是在 Executor 端完成的，那么结果不会拉取回到 Driver 端</p>\n<p>RDD 无法实现数据拉取操作</p>\n<p>如果 Executor 端使用了 Driver 端数据，那么需要从 Driver 端将数据拉取到 Executor 端</p>\n<p>数据拉取的单位是 Task (任务)</p>\n<p>默认数据传输以 Task 为单位进行传输，如果想要以 Executor 为单位传输，那么需要进行包装 (封装)</p>\n<p>Spark 需要采用特殊的数据模型实现数据传输：广播变量</p>\n<p>jsc.broadcast(list);</p>\n<p>rdd.filter(s -&gt; broadcast.value())</p>\n<h4 id=\"sparksql\"><a class=\"markdownIt-Anchor\" href=\"#sparksql\">#</a> sparksql</h4>\n<p>Spark SQL: 结构化数据处理模块</p>\n<p>SQL: 为了数据库数据访问开发的语言</p>\n<p>Spark 封装模块的目的就是在结构化数据的场合，处理起来方便</p>\n<p>结构化数据：特殊结构的数据 =&gt;(table,json)</p>\n<p>半结构化数据:xml,html</p>\n<p>非结构化数据：压缩文件，图形文件，视频，音频文件</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spark-sql_2<span class=\"number\">.12</span>&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">3.3</span><span class=\"number\">.1</span>&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">envv</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//        SparkConf sparkConf = new SparkConf();</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkConf.setMaster(&quot;local&quot;);</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkConf.setAppName(&quot;SparkSQL&quot;);</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        SparkContext sc = new SparkContext(sparkConf);</span></span><br><span class=\"line\"><span class=\"comment\">//        SparkSession sparkSession = new SparkSession(sc);</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkSession.close();</span></span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用sql文的方式操作</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select avg(age) from user&quot;</span>;</span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\">        rowDataset1.show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"环境之间转换\"><a class=\"markdownIt-Anchor\" href=\"#环境之间转换\">#</a> 环境之间转换</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// core:sparkcontext -&gt; sql:sparksession</span></span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"title class_\">sparkSession</span>( <span class=\"keyword\">new</span> <span class=\"title class_\">SparkContect</span>(conf));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// sql -&gt; core:sparkcontext</span></span><br><span class=\"line\">sparksession.sparkContext().parallelize();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//sql -&gt; core:javasparkcontext</span></span><br><span class=\"line\">        <span class=\"type\">SparkContext</span> <span class=\"variable\">sparkContext</span> <span class=\"operator\">=</span> sparkSession.sparkContext();</span><br><span class=\"line\">        <span class=\"type\">JavaSparkContext</span> <span class=\"variable\">javaSparkContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaSparkContext</span>(sparkContext);</span><br><span class=\"line\">        javaSparkContext.parallelize(Arrays.asList(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>));</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//rowDataset.foreach(</span></span><br><span class=\"line\"><span class=\"comment\">//        row -&gt;&#123;</span></span><br><span class=\"line\"> <span class=\"comment\">//           System.out.println(row.getInt(2));</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 数据模型中的数据类型进行转换，将row转换成其他对象处理</span></span><br><span class=\"line\">        Dataset&lt;User&gt; userDataset = rowDataset.as(Encoders.bean(User.class));</span><br><span class=\"line\">        </span><br><span class=\"line\">        userDataset.foreach(</span><br><span class=\"line\">                user -&gt; &#123;</span><br><span class=\"line\">                    System.out.println(user.getname());</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">User</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> id;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> age;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String name;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    User(<span class=\"type\">int</span> id, <span class=\"type\">int</span> age, String name) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.id = id;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.age = age;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.name = name;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">model</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;select * from user&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">model2</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.select(<span class=\"string\">&quot;*&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"自定义udf\"><a class=\"markdownIt-Anchor\" href=\"#自定义udf\">#</a> 自定义 UDF</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_3</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用sql文的方式操作</span></span><br><span class=\"line\">        <span class=\"comment\">// SparkSQL提供了一种特殊的方式,可以在SQL中增加自定义方法来实现复杂的逻辑</span></span><br><span class=\"line\">        <span class=\"comment\">//如果想要自定义的方法能够在SQL中使用,那么必须在SPark中进行声明和注册</span></span><br><span class=\"line\">        <span class=\"comment\">// register方法需要传递3个参数</span></span><br><span class=\"line\">        <span class=\"comment\">//第一个参数表示SQL中使用的方法名</span></span><br><span class=\"line\">        <span class=\"comment\">//第二个参数表示逻辑:IN=&gt;OUT</span></span><br><span class=\"line\">        <span class=\"comment\">//第三个参数表示返回的数据类型  ,DataType类型数据，需要使用scala语法操作</span></span><br><span class=\"line\">        sparkSession.udf().register(<span class=\"string\">&quot;prefixName&quot;</span>, <span class=\"keyword\">new</span> <span class=\"title class_\">UDF1</span>&lt;String, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Name:&quot;</span> + s;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;, StringType$.MODULE$);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select prefixName(name) from user&quot;</span>;</span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\">        rowDataset1.show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.apache.spark.sql.types.DataTypes.StringType;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_udf</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用sql文的方式操作</span></span><br><span class=\"line\">        <span class=\"comment\">// SparkSQL提供了一种特殊的方式,可以在SQL中增加自定义方法来实现复杂的逻辑</span></span><br><span class=\"line\">        <span class=\"comment\">//如果想要自定义的方法能够在SQL中使用,那么必须在SPark中进行声明和注册</span></span><br><span class=\"line\">        <span class=\"comment\">// register方法需要传递3个参数</span></span><br><span class=\"line\">        <span class=\"comment\">//第一个参数表示SQL中使用的方法名</span></span><br><span class=\"line\">        <span class=\"comment\">//第二个参数表示逻辑:IN=&gt;OUT</span></span><br><span class=\"line\">        <span class=\"comment\">//第三个参数表示返回的数据类型  ,DataType类型数据，需要使用scala语法操作</span></span><br><span class=\"line\"><span class=\"comment\">//        sparkSession.udf().register(&quot;prefixName&quot;, new UDF1&lt;String, String&gt;() &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//            @Override</span></span><br><span class=\"line\"><span class=\"comment\">//            public String call(String s) throws Exception &#123;</span></span><br><span class=\"line\"><span class=\"comment\">//                return &quot;Name:&quot; + s;</span></span><br><span class=\"line\"><span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">//        &#125;, DataTypes.StringType);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.udf().register(<span class=\"string\">&quot;prefixName&quot;</span>, <span class=\"keyword\">new</span> <span class=\"title class_\">UDF1</span>&lt;String, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Name:&quot;</span> + s;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;, StringType);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select prefixName(name) from user&quot;</span>;</span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\">        rowDataset1.show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>UDF 函数是每一行数据会都用一次函数</p>\n<p>UDAF 函数是所有的数据产生一个结果数据</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223942980.png\" alt=\"image-20240804223942980\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224006080.png\" alt=\"image-20240804224006080\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.sparksql;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">AvgAgeBuffer</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">long</span> total;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">long</span> count;</span><br><span class=\"line\"></span><br><span class=\"line\">    AvgAgeBuffer() &#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    AvgAgeBuffer(<span class=\"type\">long</span> total, <span class=\"type\">long</span> count) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.total = total;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.count = count;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">getCount</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> count;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setCount</span><span class=\"params\">(<span class=\"type\">long</span> count)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.count = count;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">long</span> <span class=\"title function_\">getTotal</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> total;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setTotal</span><span class=\"params\">(<span class=\"type\">long</span> total)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.total = total;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.example.sparksql;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.Encoder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.Encoders;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 自定义UDAF函数,实现年龄的平均值</span></span><br><span class=\"line\"><span class=\"comment\">//1.创建自定义的【公共】类</span></span><br><span class=\"line\"><span class=\"comment\">//2.继承 org.apache.spark.sql.expressions.Aggreegator</span></span><br><span class=\"line\"><span class=\"comment\">//3.设定泛型</span></span><br><span class=\"line\"><span class=\"comment\">//IN:输入数据类型</span></span><br><span class=\"line\"><span class=\"comment\">//BUFF:缓冲区的数据类型</span></span><br><span class=\"line\"><span class=\"comment\">//OUT:输出数据类型</span></span><br><span class=\"line\"><span class=\"comment\">//4.重写方法(4(计算)+2(状态))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MyAvgAgeUDAF</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Aggregator</span>&lt;Long, AvgAgeBuffer, Long&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 缓冲区初始化操作</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> AvgAgeBuffer <span class=\"title function_\">zero</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">AvgAgeBuffer</span>(<span class=\"number\">0L</span>, <span class=\"number\">0L</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 把输入年龄和缓冲器数据聚合</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> AvgAgeBuffer <span class=\"title function_\">reduce</span><span class=\"params\">(AvgAgeBuffer buffer, Long age)</span> &#123;</span><br><span class=\"line\">        buffer.setTotal(buffer.getTotal() + age);</span><br><span class=\"line\">        buffer.setCount(buffer.getCount()+<span class=\"number\">1</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> buffer;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 合并缓冲区数据</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> AvgAgeBuffer <span class=\"title function_\">merge</span><span class=\"params\">(AvgAgeBuffer b1, AvgAgeBuffer b2)</span> &#123;</span><br><span class=\"line\">        b1.setTotal(b1.getTotal()+b2.getTotal());</span><br><span class=\"line\">        b1.setCount(b1.getCount()+b2.getCount());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> b1;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"comment\">// 计算最终结果</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Long <span class=\"title function_\">finish</span><span class=\"params\">(AvgAgeBuffer reduction)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> reduction.getTotal() / reduction.getCount();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Encoder&lt;AvgAgeBuffer&gt; <span class=\"title function_\">bufferEncoder</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Encoders.bean(AvgAgeBuffer.class);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Encoder&lt;Long&gt; <span class=\"title function_\">outputEncoder</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Encoders.LONG();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.example.sparksql;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_udaf</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class=\"line\">        <span class=\"comment\">// 构建器模式:构建对象</span></span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class=\"line\">        <span class=\"comment\">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.rdd();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将数据模型转换成表</span></span><br><span class=\"line\">        rowDataset.createOrReplaceTempView(<span class=\"string\">&quot;user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//SparkSQL采用特殊的方式将UDAF转换成UDF使用</span></span><br><span class=\"line\"><span class=\"comment\">//UDAF使用时需要创建自定义聚合对象</span></span><br><span class=\"line\">        <span class=\"comment\">// 两个恶参数，第一个UADF对象，第二个表示UADF对象</span></span><br><span class=\"line\">        sparkSession.udf().register(<span class=\"string\">&quot;avgage&quot;</span>, functions.udaf(<span class=\"keyword\">new</span> <span class=\"title class_\">MyAvgAgeUDAF</span>(),Encoders.LONG()));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">sql</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;select avgage(age) from user&quot;</span>;</span><br><span class=\"line\">        sparkSession.sql(sql).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 展示数据模型效果</span></span><br><span class=\"line\"><span class=\"comment\">//        rowDataset1.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"数据加载与保存\"><a class=\"markdownIt-Anchor\" href=\"#数据加载与保存\">#</a> 数据加载与保存</h5>\n<p>SparkSQL 读取和保存的文件一般为三种，JSON 文件、CSV 文文件和列式存储的文件，同时可以通过添加参数，来识别不同的存储和压缩格式。</p>\n<p>csv</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// csv 文件讲数据采用逗号分隔,可以被excel打开</span></span><br><span class=\"line\">        <span class=\"comment\">// csv 带有header，以,分隔.   可以以, _ \\t 分隔      \\t时叫做tsv</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; csv = sparkSession.read().option(<span class=\"string\">&quot;header&quot;</span>,<span class=\"string\">&quot;true&quot;</span>).option(<span class=\"string\">&quot;sep&quot;</span>,<span class=\"string\">&quot;,&quot;</span>).csv(<span class=\"string\">&quot;data/user.csv&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// +----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//| _c0|     _c1|_c2|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//|1001|zhangsan| 30|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1002|    lisi| 31|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1003|  wangwu| 32|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//|  id|    name|age|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//|1001|zhangsan| 30|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1002|    lisi| 31|</span></span><br><span class=\"line\">        <span class=\"comment\">//|1003|  wangwu| 32|</span></span><br><span class=\"line\">        <span class=\"comment\">//+----+--------+---+</span></span><br><span class=\"line\">        <span class=\"comment\">//csv.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 输出目录已经存在，默认会发生异常。不希望出错，可以修改配置 保存模式</span></span><br><span class=\"line\">        csv.write().mode(<span class=\"string\">&quot;append&quot;</span>).option(<span class=\"string\">&quot;header&quot;</span>,<span class=\"string\">&quot;true&quot;</span>).csv(<span class=\"string\">&quot;output&quot;</span>); </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//  DataFrameWriter var2;</span></span><br><span class=\"line\">        <span class=\"comment\">//        if (&quot;overwrite&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            var2 = this.mode(SaveMode.Overwrite);</span></span><br><span class=\"line\">        <span class=\"comment\">//        &#125; else if (&quot;append&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            var2 = this.mode(SaveMode.Append);</span></span><br><span class=\"line\">        <span class=\"comment\">//        &#125; else if (&quot;ignore&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            var2 = this.mode(SaveMode.Ignore);</span></span><br><span class=\"line\">        <span class=\"comment\">//        &#125; else &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//            boolean var3;</span></span><br><span class=\"line\">        <span class=\"comment\">//            if (&quot;error&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = true;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125; else if (&quot;errorifexists&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = true;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125; else if (&quot;default&quot;.equals(var4)) &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = true;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125; else &#123;</span></span><br><span class=\"line\">        <span class=\"comment\">//                var3 = false;</span></span><br><span class=\"line\">        <span class=\"comment\">//            &#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>json</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source_json</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// js object natation</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果是对象，用&#123;&#125;  数组用[]  json文件符合json格式</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL其实就是对Spark Core RDD的封装。RDD读取文件采用用的是Hadoop,hadoop是按行读取。</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可,无需整个文件符合JSON格式</span></span><br><span class=\"line\">        <span class=\"comment\">// 底层是dataset，可以读csv，写json</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class=\"string\">&quot;data/user.json&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.write().json(<span class=\"string\">&quot;output&quot;</span>);</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>行式存储</p>\n<p>存在主键，可以快速定位。查询快，统计慢</p>\n<p>列式存储</p>\n<p>查询快，统计快</p>\n<p>mysql 行存储，hive 列存储</p>\n<p>spark 列存储 parquet</p>\n<p>列存储可以被压缩，snappy</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ublic <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source_parquet</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// js object natation</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果是对象，用&#123;&#125;  数组用[]  json文件符合json格式</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL其实就是对Spark Core RDD的封装。RDD读取文件采用用的是Hadoop,hadoop是按行读取。</span></span><br><span class=\"line\">        <span class=\"comment\">//SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可,无需整个文件符合JSON格式</span></span><br><span class=\"line\">        <span class=\"comment\">// 底层是dataset，可以读csv，写json</span></span><br><span class=\"line\">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().parquet(<span class=\"string\">&quot;data/user&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//rowDataset.show();</span></span><br><span class=\"line\"></span><br><span class=\"line\">        rowDataset.write().json(<span class=\"string\">&quot;output&quot;</span>);</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"mysql交互\"><a class=\"markdownIt-Anchor\" href=\"#mysql交互\">#</a> mysql 交互</h5>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">8.0</span><span class=\"number\">.18</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_source_mysql</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession.builder().master(<span class=\"string\">&quot;local[*]&quot;</span>).appName(<span class=\"string\">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Properties</span> <span class=\"variable\">properties</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Properties</span>();</span><br><span class=\"line\">        properties.setProperty(<span class=\"string\">&quot;user&quot;</span>,<span class=\"string\">&quot;admin&quot;</span>);</span><br><span class=\"line\">        properties.setProperty(<span class=\"string\">&quot;password&quot;</span>,<span class=\"string\">&quot;Chaitin@123&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        Dataset&lt;Row&gt; jdbc = sparkSession.read().jdbc(<span class=\"string\">&quot;jdbc:mysql://hadoop100:3306/metastore?useSSL=false&quot;</span>,<span class=\"string\">&quot;TYPES&quot;</span>,properties);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        jdbc.write().mode(<span class=\"string\">&quot;append&quot;</span>).jdbc(<span class=\"string\">&quot;jdbc:mysql://hadoop100:3306/metastore?useSSL=false&quot;</span>,<span class=\"string\">&quot;TYPES_123&quot;</span>,properties);</span><br><span class=\"line\">        jdbc.show();</span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"hive交互\"><a class=\"markdownIt-Anchor\" href=\"#hive交互\">#</a> hive 交互</h5>\n<p>复制 hive-site.yaml 到 resources 中</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spark-hive_2<span class=\"number\">.12</span>&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;<span class=\"number\">3.3</span><span class=\"number\">.1</span>&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">sql_hive</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 编码前设定hadoop访问用户</span></span><br><span class=\"line\">        System.setProperty(<span class=\"string\">&quot;HADOOP_USER_NAME&quot;</span>,<span class=\"string\">&quot;root&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">SparkSession</span> <span class=\"variable\">sparkSession</span> <span class=\"operator\">=</span> SparkSession</span><br><span class=\"line\">                .builder()</span><br><span class=\"line\">                .enableHiveSupport()</span><br><span class=\"line\">                .master(<span class=\"string\">&quot;local[*]&quot;</span>)</span><br><span class=\"line\">                .appName(<span class=\"string\">&quot;sparkSQL&quot;</span>)</span><br><span class=\"line\">                .getOrCreate();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;show tables&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;create table user_info(name string, age int)&quot;</span>);</span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;insert into  table user_info values (&#x27;haha&#x27;,100)&quot;</span>);</span><br><span class=\"line\">        sparkSession.sql(<span class=\"string\">&quot;select * from user_info&quot;</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        sparkSession.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果 reources 中有 hive-site.xml 但是 target/classes 中没有，需要手工拷贝到改目录</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224020783.png\" alt=\"image-20240804224020783\"></p>\n<h4 id=\"spark-streaming\"><a class=\"markdownIt-Anchor\" href=\"#spark-streaming\">#</a> spark streaming</h4>\n<p>有界数据流</p>\n<p>无界数据流</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224044892.png\" alt=\"image-20240804224044892\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224058079.png\" alt=\"image-20240804224058079\"></p>\n<p>spark streaming 底层还是 spark core，在流式数据处理中进行的封装</p>\n<p>从数据处理方式的角度</p>\n<p>流式数据处理：一个数据一个数据的处理</p>\n<p>微批量数据处理：一小批数据处理</p>\n<p>批量数据处理：一批数据一批数据的处理</p>\n<p>从数据处理延迟的角度</p>\n<p>实时数据处理：数据处理的延迟以毫秒为单位</p>\n<p>准实时处理：以秒和分钟为单位</p>\n<p>离线数据处理：数据处理的延迟以小时，天为单位</p>\n<p>Spark 是批量，离线数据处理框架</p>\n<p>spark streaming 是个 微批量 准实时数据处理框架</p>\n<p>streaming 按照时间来定义一小批</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224112151.png\" alt=\"image-20240804224112151\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">env</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 数据采集器是长期执行的任务，不能停止，也不能释放资源</span></span><br><span class=\"line\">        <span class=\"comment\">// javaStreamingContext.close();</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>socket</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m http.server <span class=\"number\">8001</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">socket</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 通过环境对象对接socket数据源</span></span><br><span class=\"line\">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">9999</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        socketTextStream.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kafka</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.SparkConf;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.api.java.function.Function;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.Duration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.api.java.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.*;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">kafka</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 创建配置参数</span></span><br><span class=\"line\">        HashMap&lt;String, Object&gt; map = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class=\"string\">&quot;hadoop102:9092,hadoop103:9092,hadoop104:9092&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class=\"string\">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class=\"string\">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.GROUP_ID_CONFIG,<span class=\"string\">&quot;atguigu&quot;</span>);</span><br><span class=\"line\">        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class=\"string\">&quot;latest&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 需要消费的主题</span></span><br><span class=\"line\">        ArrayList&lt;String&gt; strings = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;&gt;();</span><br><span class=\"line\">        strings.add(<span class=\"string\">&quot;topic_db&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));</span><br><span class=\"line\"></span><br><span class=\"line\">        directStream.map(<span class=\"keyword\">new</span> <span class=\"title class_\">Function</span>&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> String <span class=\"title function_\">call</span><span class=\"params\">(ConsumerRecord&lt;String, String&gt; v1)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> v1.value();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).print(<span class=\"number\">100</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Dstream</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">function</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 通过环境对象对接socket数据源</span></span><br><span class=\"line\">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">9999</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        socketTextStream.print();</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaDStream&lt;String&gt; stringJavaDStream = socketTextStream.flatMap(</span><br><span class=\"line\">                line -&gt; Arrays.asList(line.split(<span class=\"string\">&quot; &quot;</span>)).iterator()</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaPairDStream&lt;String, Integer&gt; stringIntegerJavaPairDStream = stringJavaDStream.mapToPair(</span><br><span class=\"line\">                word -&gt; <span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(word, <span class=\"number\">1</span>)</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        JavaPairDStream&lt;String, Integer&gt; stringIntegerJavaPairDStream1 = stringIntegerJavaPairDStream.reduceByKey(</span><br><span class=\"line\">                Integer::sum</span><br><span class=\"line\">        );</span><br><span class=\"line\"></span><br><span class=\"line\">        stringIntegerJavaPairDStream1.foreachRDD(</span><br><span class=\"line\">                rdd -&gt; &#123;</span><br><span class=\"line\">                    rdd.sortByKey().collect().forEach(System.out::println);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>window</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">窗口是可以移动的，成为移动窗口，但是窗口移动是有复幅度的，默认移动幅度就是采集周期</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">窗口:其实就是数据的范围(时间)</span><br><span class=\"line\">window方法可以改变窗口的数据范围(默认数据范围为采集周期</span><br><span class=\"line\">window方法可以传递<span class=\"number\">2</span>个参数</span><br><span class=\"line\">第一个参数表示窗口的数据范围(时间)</span><br><span class=\"line\">第二个参数表示窗口的移动幅度(时间),可以不用传递,默认使用的就是采集周期</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">数据窗口范围和窗口移动幅度一致(3s),数据不会有重复</span><br><span class=\"line\"></span><br><span class=\"line\">数据窗口范围比窗口移动幅度大,数据可能会有重复</span><br><span class=\"line\"></span><br><span class=\"line\">数据窗口范围比窗口移动幅度小,数据可能会有遗漏</span><br></pre></td></tr></table></figure>\n<p>close</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">close</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">SparkConf</span> <span class=\"variable\">sparkConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SparkConf</span>().setMaster(<span class=\"string\">&quot;local[*]&quot;</span>).setAppName(<span class=\"string\">&quot;sparkstreaming&quot;</span>);</span><br><span class=\"line\">        <span class=\"comment\">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class=\"line\">        <span class=\"type\">JavaStreamingContext</span> <span class=\"variable\">javaStreamingContext</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">JavaStreamingContext</span>(sparkConf, <span class=\"keyword\">new</span> <span class=\"title class_\">Duration</span>(<span class=\"number\">3000</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 通过环境对象对接socket数据源</span></span><br><span class=\"line\">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"number\">9999</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        socketTextStream.print();</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 启动数据采集器</span></span><br><span class=\"line\">        javaStreamingContext.start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 不能在main中关闭</span></span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">Runnable</span>() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 关闭SparkStreaming的时候,需要在程序运行的过程中,通过外部操作进行关闭</span></span><br><span class=\"line\">                    Thread.sleep(<span class=\"number\">000</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(e);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">///javaStreamingContext.close();           // 强制关闭</span></span><br><span class=\"line\">                javaStreamingContext.stop(<span class=\"literal\">true</span>,<span class=\"literal\">true</span>);       <span class=\"comment\">//graceful stop</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">Runnable</span>() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// 关闭SparkStreaming的时候,需要在程序运行的过程中,通过外部操作进行关闭</span></span><br><span class=\"line\">                    Thread.sleep(<span class=\"number\">3000</span>);</span><br><span class=\"line\">                    <span class=\"comment\">// 使用zk，redis，mysql，hdfs实现中转</span></span><br><span class=\"line\">                    <span class=\"type\">boolean</span> <span class=\"variable\">flag</span> <span class=\"operator\">=</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(e);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">///javaStreamingContext.close();           // 强制关闭</span></span><br><span class=\"line\">                javaStreamingContext.stop(<span class=\"literal\">true</span>,<span class=\"literal\">true</span>);       <span class=\"comment\">//graceful stop</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).start();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class=\"line\">        javaStreamingContext.awaitTermination();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"type\">FileSystem</span> <span class=\"variable\">fs</span> <span class=\"operator\">=</span> FileSystem.get(<span class=\"keyword\">new</span> <span class=\"title class_\">URI</span>(<span class=\"string\">&quot;hdfs://hadoop102:8020&quot;</span>), <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>(), <span class=\"string\">&quot;atguigu&quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)&#123;</span><br><span class=\"line\">                    Thread.sleep(<span class=\"number\">5000</span>);</span><br><span class=\"line\">                    <span class=\"type\">boolean</span> <span class=\"variable\">exists</span> <span class=\"operator\">=</span> fs.exists(<span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(<span class=\"string\">&quot;hdfs://hadoop102:8020/stopSpark&quot;</span>));</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (exists)&#123;</span><br><span class=\"line\">                        <span class=\"type\">StreamingContextState</span> <span class=\"variable\">state</span> <span class=\"operator\">=</span> javaStreamingContext.getState();</span><br><span class=\"line\">                        <span class=\"comment\">// 获取当前任务是否正在运行</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (state == StreamingContextState.ACTIVE)&#123;</span><br><span class=\"line\">                            <span class=\"comment\">// 优雅关闭</span></span><br><span class=\"line\">                            javaStreamingContext.stop(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">                            System.exit(<span class=\"number\">0</span>);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">catch</span> (Exception e)&#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "大数据"
            ]
        },
        {
            "id": "http://example.com/2024/03/01/hadoop_new/",
            "url": "http://example.com/2024/03/01/hadoop_new/",
            "title": "Hadoop",
            "date_published": "2024-03-01T05:38:45.000Z",
            "content_html": "<h2 id=\"hadoop\"><a class=\"markdownIt-Anchor\" href=\"#hadoop\">#</a> hadoop</h2>\n<p>Hadoop 是 Apache 软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构。</p>\n<ul>\n<li>Hadoop 是基于 Java 语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中；</li>\n<li>Hadoop 的核心是<strong>分布式文件系统 HDFS（Hadoop Distributed File System）和 MapReduce；</strong></li>\n<li>Hadoop 被公认为行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力；</li>\n</ul>\n<p>解决海量数据存储和分析计算问题</p>\n<p>优势： 维护多副本</p>\n<p>在集群间分配任务数据，方便扩展</p>\n<p>并行工作</p>\n<p>自动将失败任务重新分配</p>\n<h4 id=\"hadoop-版本演进\"><a class=\"markdownIt-Anchor\" href=\"#hadoop-版本演进\">#</a> hadoop 版本演进</h4>\n<p>Apache Hadoop 版本分为两代：第一代 Hadoop 称为 Hadoop 1.0，第二代 Hadoop 称为 Hadoop 2.0。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221529201.png\" alt=\"image-20240804221529201\"></p>\n<p><strong>第一代 Hadoop 包含三个大版本，分别是 0.20.x、0.21.x、0.22.x</strong>。</p>\n<ul>\n<li>0.20.x 最后演化成 1.0.x，变成了稳定版。</li>\n<li>0.21.x 和 0.22.x 则增加了 NameNode HA 等新的重大特性。</li>\n</ul>\n<p><strong>第二代 Hadoop 包含两个大版本，分别是 0.23.x、2.x</strong>。</p>\n<ul>\n<li>它们完全不同于 Hadoop 1.0，是一套全新的架构，均包含 HDFS Federation 和 YARN 两个系统。</li>\n<li>相比于 0.23.x，2.x 增加了 NameNode HA 和 Wire-compatibility 两个重大特性。</li>\n</ul>\n<h4 id=\"组件\"><a class=\"markdownIt-Anchor\" href=\"#组件\">#</a> 组件</h4>\n<p>HDFS</p>\n<p>NameNode：</p>\n<ul>\n<li>NameNode 是 HDFS 的主节点，负责管理文件系统的命名空间和元数据信息。</li>\n<li>它维护了整个文件系统的目录树结构以及文件和数据块的映射关系。</li>\n<li>NameNode 还负责处理客户端的读写请求，包括打开、关闭、重命名和删除文件等操作。</li>\n</ul>\n<p>DataNode：</p>\n<ul>\n<li>DataNode 是 HDFS 的数据节点，负责存储实际的数据块。</li>\n<li>它接收来自客户端或其他 DataNode 的数据写入请求，并将数据块存储在本地磁盘上。</li>\n<li>DataNode 还负责处理客户端的数据读取请求，将数据块传输给客户端。</li>\n</ul>\n<p>Standby Namenode(2NN)：</p>\n<ul>\n<li>辅助 namenode。作为备用的 NameNode。当活动的 NameNode 失效时，Standby NameNode 可以接管其工作，从而提高了系统的可用性。</li>\n</ul>\n<p>1) NameNode (nn): 存储文件的元数据，如文件名，文件目录结构，文件属性 (生成时间、副本数、文件权限), 以及每个文件的块列表和块所在 DataNode 等。</p>\n<p>2) DataNode (dn): 在本地文件系统存储文件块数据，以及块数据的校验和。</p>\n<p>3）Secondary NameNode (2nn): 每隔一段时间对 NameNode 元数据备份</p>\n<p><strong>YARN 组件</strong></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221615769.png\" alt=\"\"></p>\n<p>1) ResourceManager (RM): 整个集群资源 (内存、CPU 等) 的老大</p>\n<p>2) NodeManager (NM): 单个节点服务器资源老大</p>\n<p>3) ApplicationMaster (AM): 单个任务运行的老大</p>\n<p>4) Container: 容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。</p>\n<p>说明 1: 客户端可以有多个</p>\n<p>说明 2: 集群上可以运行多个 ApplicationMaster</p>\n<p>说明 3: 每个 NodeManager 上可以有多个 Container</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221919615.png\" alt=\"image-20240804221919615\"></p>\n<h4 id=\"部署hadoop\"><a class=\"markdownIt-Anchor\" href=\"#部署hadoop\">#</a> 部署 hadoop</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">adduser hadoop</span><br><span class=\"line\"></span><br><span class=\"line\">vim /etc/sudoers</span><br><span class=\"line\"><span class=\"comment\"># Allow members of group sudo to execute any command</span></span><br><span class=\"line\">%sudo   ALL=(ALL:ALL) ALL</span><br><span class=\"line\">hadoop  ALL=(ALL:ALL) NOPASSWD:ALL</span><br><span class=\"line\"></span><br><span class=\"line\">hostnamectl</span><br><span class=\"line\"></span><br><span class=\"line\">vim /etc/hosts</span><br><span class=\"line\"></span><br><span class=\"line\">apt update</span><br><span class=\"line\"></span><br><span class=\"line\">apt install vim net-tools lrzsz bash-com* -y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">apt install openjdk-8-jre-headless</span><br><span class=\"line\"></span><br><span class=\"line\">wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</span><br><span class=\"line\">tar xzvf hadoop-3.3.6.tar.gz</span><br><span class=\"line\"></span><br><span class=\"line\">vim /etc/profile</span><br><span class=\"line\"><span class=\"built_in\">export</span> HADOOP_HOME=/root/hadoop-3.3.6</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$HADOOP_HOME</span>/bin</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$HADOOP_HOME</span>/sbin</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$JAVA_HOME</span>/bin</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">source</span> /etc/profile</span><br></pre></td></tr></table></figure>\n<p>部署模式：</p>\n<p>local: 数据存储在本地</p>\n<p>pseudo-distributed：数据存储在 hdfs</p>\n<p>fully-distributed：数据存储在 hdfs，多台服务器工作</p>\n<p>本地运行 example wordcount</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> input</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;aa bb cc cc dd dd dd ee ee&quot;</span> &gt; input/111.txt</span><br><span class=\"line\"></span><br><span class=\"line\">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount input/ output/</span><br></pre></td></tr></table></figure>\n<h4 id=\"完全分布式部署\"><a class=\"markdownIt-Anchor\" href=\"#完全分布式部署\">#</a> 完全分布式部署：</h4>\n<p>NameNode 和 SecondaryNameNode 不要安装在同一台服务器</p>\n<p>ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台机器上</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221900758.png\" alt=\"image-20240804221900758\"></p>\n<p>Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认。配置值时，才需要修改自定义配置文件，更改相应属性值。</p>\n<p>默认配置文件</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221943124.png\" alt=\"image-20240804221943124\"></p>\n<p>自定义配置文件</p>\n<p>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个配置文件存放在</p>\n<p>SHADOOP_HOME/etc/kadop, 这个路径上，用户可以根据项目需求重新进行修改配置。</p>\n<p>在集群上所有节点配置</p>\n<p>core-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=<span class=\"string\">&quot;text/xsl&quot;</span> href=<span class=\"string\">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>fs.defaultFS<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hdfs://hadoop100:8020<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The name of the default file system.  A URI whose</span></span><br><span class=\"line\"><span class=\"language-xml\">      scheme and authority determine the FileSystem implementation.  The</span></span><br><span class=\"line\"><span class=\"language-xml\">      uri&#x27;s scheme determines the config property (fs.SCHEME.impl) naming</span></span><br><span class=\"line\"><span class=\"language-xml\">      the FileSystem implementation class.  The uri&#x27;s authority is used to</span></span><br><span class=\"line\"><span class=\"language-xml\">      determine the host, port, etc. for a filesystem.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop-$&#123;user.name&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>A base for other temporary directories.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"comment\">&lt;!-- Static Web User Filter properties. --&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.http.staticuser.user<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>root<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        The user name to filter as, on static web filters</span></span><br><span class=\"line\"><span class=\"language-xml\">        while rendering content. An example use is the HDFS</span></span><br><span class=\"line\"><span class=\"language-xml\">        web UI (user to be used for browsing files).</span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>vim hdfs-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=<span class=\"string\">&quot;text/xsl&quot;</span> href=<span class=\"string\">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.http-address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hadoop100:9870<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        The address and the base port where the dfs namenode web ui will listen on.</span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hadoop102:9868<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        The secondary namenode http server address and port.</span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span> </span><br></pre></td></tr></table></figure>\n<p>vim yarn-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>A comma separated list of services where service name should only</span></span><br><span class=\"line\"><span class=\"language-xml\">      contain a-zA-Z0-9_ and can not start with numbers<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\">    &lt;!--<span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span>--&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The hostname of the RM.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hadoop101<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span>   </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Environment variables that containers may override rather than use NodeManager&#x27;s default.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span>  </span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRER_HOME<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>vim mapred-site.xml</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span> </span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The runtime framework for executing MapReduce jobs.</span></span><br><span class=\"line\"><span class=\"language-xml\">  Can be one of local, classic or yarn.</span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span></span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure>\n<p>vim worker</p>\n<p>不能有空格</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop100</span><br><span class=\"line\">hadoop101</span><br><span class=\"line\">hadoop102</span><br></pre></td></tr></table></figure>\n<p>初始化 namenode</p>\n<p>只有集群第一次启动需要初始化，此时会创建数据目录</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs namenode -format </span><br></pre></td></tr></table></figure>\n<p>启动集群</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_NAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_DATANODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_SECONDARYNAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_RESOURCEMANAGER_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_NODEMANAGER_USER</span>=root</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 只有在这里的java_home 他启动的时候才会认，他不认/etc/profile 和 ~/.<span class=\"property\">bashrc</span></span><br><span class=\"line\">vim etc/hadoop/hadoop-env.<span class=\"property\">sh</span></span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">JAVA_HOME</span>=<span class=\"regexp\">/root/</span>jdk8u352-b08</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">sbin/start-dfs.<span class=\"property\">sh</span></span><br><span class=\"line\"></span><br><span class=\"line\">jps确认 </span><br><span class=\"line\"></span><br><span class=\"line\">curl <span class=\"attr\">http</span>:<span class=\"comment\">//192.168.13.190:9870/dfshealth.html#tab-overview</span></span><br><span class=\"line\">启动resourcemanager </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_NAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_DATANODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">HDFS_SECONDARYNAMENODE_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_RESOURCEMANAGER_USER</span>=root</span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"variable constant_\">YARN_NODEMANAGER_USER</span>=root</span><br><span class=\"line\"></span><br><span class=\"line\">设置主机互信</span><br><span class=\"line\"></span><br><span class=\"line\">sbin/start-yarn.<span class=\"property\">sh</span></span><br><span class=\"line\"></span><br><span class=\"line\">测试</span><br><span class=\"line\">hadoop fs -mkdir /input</span><br><span class=\"line\">hadoop fs -put /root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/input/<span class=\"number\">1.</span>txt /input </span><br><span class=\"line\">存储位置</span><br><span class=\"line\">/data/hadoop-root/dfs/data/current/<span class=\"variable constant_\">BP</span>-<span class=\"number\">897483346</span>-<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.190</span>-<span class=\"number\">1716822540714</span>/current/finalized/subdir0/subdir0</span><br><span class=\"line\"></span><br><span class=\"line\">测试：</span><br><span class=\"line\">hadoop jar /root/hadoop-<span class=\"number\">3.3</span><span class=\"number\">.6</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-<span class=\"number\">3.3</span><span class=\"number\">.6</span>.<span class=\"property\">jar</span> wordcount /input /output</span><br><span class=\"line\"></span><br><span class=\"line\">如果有报错，是需要往mapred-site.<span class=\"property\">xml</span>中加东西的，那么添加如下字段，在重新执行</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.application.classpath<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.map.env<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.env<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221955345.png\" alt=\"image-20240804221955345\"></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbWlyYWNsZS1sdW5hL3AvMTc3ODUzMTAuaHRtbA==\">https://www.cnblogs.com/miracle-luna/p/17785310.html</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZmFucWlzb2Z0L3AvMTc4NTkwODYuaHRtbA==\">https://www.cnblogs.com/fanqisoft/p/17859086.html</span></p>\n<h4 id=\"故障处理\"><a class=\"markdownIt-Anchor\" href=\"#故障处理\">#</a> 故障处理：</h4>\n<p>停进程，删除数据目录，格式化</p>\n<h4 id=\"记录历史运行情况\"><a class=\"markdownIt-Anchor\" href=\"#记录历史运行情况\">#</a> 记录历史运行情况</h4>\n<p>启动 history server</p>\n<p>mapred-site.xml</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;hadoop100:10020&lt;/value&gt;</span><br><span class=\"line\">  &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;hadoop100:19888&lt;/value&gt;</span><br><span class=\"line\">  &lt;description&gt;MapReduce JobHistory Server Web UI host:port&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure>\n<p>集群内同步该配置文件</p>\n<p>重启 yarn</p>\n<p>启动历史服务器，在 namenode</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>\n<p>测试历史功能：</p>\n<p>hadoop jar /root/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222012877.png\" alt=\"image-20240804222012877\"></p>\n<h4 id=\"日志汇聚\"><a class=\"markdownIt-Anchor\" href=\"#日志汇聚\">#</a> 日志汇聚</h4>\n<p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222029398.png\" alt=\"image-20240804222029398\"></p>\n<p>namenode:</p>\n<p>yarn-site.xml</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class=\"line\">      each container&#x27;s logs and moves these logs onto a file-system, for e.g.</span><br><span class=\"line\">      HDFS, after the application completes. Users can configure the</span><br><span class=\"line\">      &quot;yarn.nodemanager.remote-app-log-dir&quot; and</span><br><span class=\"line\">      &quot;yarn.nodemanager.remote-app-log-dir-suffix&quot; properties to determine</span><br><span class=\"line\">      where these logs are moved to. Users can access the logs via the </span><br><span class=\"line\">      Application Timeline Server.</span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log-aggregation-enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    URL for log aggregation server</span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log.server.url<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>http://hadoop100:19888/jobhistory/logs<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>How long to keep aggregation logs before deleting them.  -1 disables.</span><br><span class=\"line\">    Be careful set this too small and you will spam the name node.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>604800<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span> </span><br><span class=\"line\">mapred --daemon stop historyserver</span><br><span class=\"line\"></span><br><span class=\"line\">stop-yarn.sh</span><br><span class=\"line\"></span><br><span class=\"line\">start-yarn.sh </span><br><span class=\"line\"></span><br><span class=\"line\">mapred --daemon start historyserver </span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222046081.png\" alt=\"image-20240804222046081\"></p>\n<p>启新任务观察：</p>\n<p>对于单个组件重启</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(1)分别启动/停止HDFS组件e</span><br><span class=\"line\">hdfs --daemon start/stop namenode/datanode/secoondarynamenode</span><br><span class=\"line\">(2)启动/停止YARN</span><br><span class=\"line\">yarn --daemon start/stop resourcemanager/nodemanager</span><br><span class=\"line\">#! /bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">ssh hadoop100 &quot;/root/hadoop-3.3.6/sbin/start-dfs.sh</span><br><span class=\"line\">ssh hadoop101 &quot;/root/hadoop-3.3.6/sbin/start-yarn.sh</span><br><span class=\"line\">ssh hadoop100 &quot;mapred --daemon start historyserver&quot;</span><br></pre></td></tr></table></figure>\n<h4 id=\"常用端口号\"><a class=\"markdownIt-Anchor\" href=\"#常用端口号\">#</a> 常用端口号</h4>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222100104.png\" alt=\"image-20240804222100104\"></p>\n<p>常用端口号</p>\n<p>hadoop3.x</p>\n<p>HDFS NameNode 内部通常端口：8020/9000/9820</p>\n<p>HDFS NameNode 对用户的查询端口：9870</p>\n<p>Yarn 查看任务运行情况的：8088</p>\n<p>历史服务器：19888</p>\n<p>hadoop2.x</p>\n<p>HDFS NameNode 内部通常端口：8020/9000</p>\n<p>HDFS NameNode 对用户的查询端口：50070</p>\n<p>Yarn 查看任务运行情况的：8088</p>\n<p>历史服务器：19888</p>\n<p>常用的配置文件</p>\n<p>3.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml  workers</p>\n<p>2.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml  slaves</p>\n<h2 id=\"hdfs\"><a class=\"markdownIt-Anchor\" href=\"#hdfs\">#</a> hdfs</h2>\n<p>HDFS (Hadoop Distributed File System), 它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起起来实现其功能，集群中的服务器有各自的角色。</p>\n<p>HDFS 的使用场景：适合一次写入，多次读出的场景。一个个文件经过创建、写入和关闭之后就不能改变。</p>\n<p>高容错性</p>\n<p>数据自动保存多个副本。它通过增加副本的形式，提高容错性。</p>\n<p>适合处理大数据</p>\n<p>数据规模：能够处理数据规模达到 GB、TB、甚至 PB 级别的数据</p>\n<p>文件规模：能够处理百万规模以上的文件数量，数量相当之大。</p>\n<p>可构建在廉价机器上，通过多副本机制，提高可靠性。</p>\n<p>不适合低延时数据访问，比如毫秒级的存储数据，是做不到到的</p>\n<p>无法高效的对大量小文件进行存储。</p>\n<p>存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的；</p>\n<p>小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。</p>\n<p>不支持并发写入、文件随机修改。</p>\n<p>一个文件只能有一个写，不允许多个线程同时写；</p>\n<p>仅支持数据 append (追加), 不支持文件的随机修改。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222115457.png\" alt=\"image-20240804222115457\"></p>\n<p>1) NameNode (nn): 就是 Master, 它是一个主管、管理者。</p>\n<p>(1) 管理 HDFS 的名称空间；</p>\n<p>(2) 配置副本策略；</p>\n<p>(3) 管理数据块 (Block) 映射信息；</p>\n<p>(4) 处理客户端读写请求。</p>\n<ol>\n<li>DataNode: 就是 Slave。NameNode 下达命令，DataNode 执行实际的操作。</li>\n</ol>\n<p>(1) 存储实际的数据块；</p>\n<p>(2) 执行数据块的读 / 写操作。</p>\n<p>3) Client: 就是客户端。</p>\n<p>(1) 文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block, 然后进行上传；</p>\n<p>(2) 与 NameNode 交互，获取文件的位置信息；</p>\n<p>(3) 与 DataNode 交互，读取或者写入数据；</p>\n<p>(4) Client 提供一些命令来管理 HDFS, 比如 NameNode 格式化；</p>\n<p>(5) Client 可以通过一些命令来访问 HDFS, 比如对 HDFS 增删查改操作；</p>\n<p>4) Secondary NameNode: 并非 NameNode 的热备。当 NameNode 挂掉的时候，它并不</p>\n<p>能马上替换 NameNode 并提供服务。</p>\n<p>(1) 辅助 NameNode, 分担其工作量，比如定期合并 Fsimage 和 bEdits, 并推送给 NameNode;</p>\n<p>(2) 在紧急情况下，可辅助恢复 NameNode。</p>\n<p>HDFS 中的文件在物理上是分块存储 (Block), 块的大小可以通过配置参数 (dfs.blocksize) 来规定，默认大小在 Hadoop2.x/3.x 版本中是 128M,1.x 版本中是 64M。</p>\n<p>磁盘速率更快的情况下，可以设置为 256M</p>\n<p>(1) HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</p>\n<p>(2) 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>\n<p>HDFS 块的大小设置主要取决于磁盘传输速率。</p>\n<h3 id=\"hdfs-shell\"><a class=\"markdownIt-Anchor\" href=\"#hdfs-shell\">#</a> hdfs shell</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hadoop fs -mkdir /haha</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 上传，剪切本地文件</span><br><span class=\"line\"> hadoop fs -moveFromLocal ./README.txt /haha</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 复制本地文件</span><br><span class=\"line\">  hadoop fs -copyFromLocal ./README.txt /haha</span><br><span class=\"line\">  </span><br><span class=\"line\">  # 复制本地文件</span><br><span class=\"line\"> hadoop fs -put ./README.txt /haha</span><br><span class=\"line\"> </span><br><span class=\"line\"> # 追加文件到另一个文件末尾</span><br><span class=\"line\">hadoop fs -appendToFile ./README.txt /haha/www.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 下载</span><br><span class=\"line\">hadoop fs -copyToLocal /haha/1.txt ./1111.txt</span><br><span class=\"line\"># 下载</span><br><span class=\"line\">hadoop fs -get /haha/1.txt ./1111.txt</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># </span><br><span class=\"line\">hadoop fs -ls /</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -cat /haha/111.txt</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -chown aaa:aaa /haha/1.txt</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -cp</span><br><span class=\"line\"></span><br><span class=\"line\">hadoop fs -mv /haha/1.txt /xixix/2.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 显示一个文件的末尾1kb数据</span><br><span class=\"line\">hadoop fs -tail /haha/1.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 删除文件或文件夹</span><br><span class=\"line\">hadoop fs -rm</span><br><span class=\"line\">hadoop fs -rm -r</span><br><span class=\"line\"></span><br><span class=\"line\"># 查看文件夹下总大小</span><br><span class=\"line\">hadoop fs -du -s -h /haha</span><br><span class=\"line\"># 查看文件下每个文件大小</span><br><span class=\"line\">hadoop fs -du -h /haha</span><br><span class=\"line\"></span><br><span class=\"line\"># 修改副本大小,副本数大于机器数量时，只会创建对应的副本数。后续集群内新增机器，会增加到对应的数量。但是永远保持每个机器上至多一个副本</span><br><span class=\"line\">hadoop fs -setrep 10 /haha/1.txt</span><br></pre></td></tr></table></figure>\n<h3 id=\"hadoop-api\"><a class=\"markdownIt-Anchor\" href=\"#hadoop-api\">#</a> hadoop api</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 在windows下安装hadoop+winutils</span><br><span class=\"line\">https://blog.csdn.net/shulianghan/article/details/132045605</span><br><span class=\"line\"></span><br><span class=\"line\"># 配置maven</span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400168</span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400995</span><br><span class=\"line\"></span><br><span class=\"line\">hdfs-default &lt; hdfs-site &lt; resource &lt; 代码里修改configuraion.set</span><br><span class=\"line\"></span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400168</span><br><span class=\"line\"></span><br><span class=\"line\">https://blog.csdn.net/weixin_50956145/article/details/130511265</span><br><span class=\"line\">https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.30</span><br><span class=\"line\">https://www.bilibili.com/read/cv28415867/?jump_opus=1</span><br><span class=\"line\">https://www.cnblogs.com/linshengqian/p/15657694.html</span><br><span class=\"line\">https://www.runoob.com/maven/maven-build-life-cycle.html</span><br><span class=\"line\">https://www.cnblogs.com/xfeiyun/p/16740262.html</span><br><span class=\"line\">https://blog.csdn.net/m0_46413065/article/details/116400995</span><br></pre></td></tr></table></figure>\n<p>mvn 处理依赖</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeGxkL2FydGljbGUvZGV0YWlscy84MjI4NDI2OQ==\">https://blog.csdn.net/lixld/article/details/82284269</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0ODg2MjEzL2FydGljbGUvZGV0YWlscy8xMjM0NjE1MjI=\">https://blog.csdn.net/qq_44886213/article/details/123461522</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tdm5yZXBvc2l0b3J5LmNvbS9hcnRpZmFjdC9vcmcuc2xmNGovc2xmNGotcmVsb2FkNGovMi4xLjAtYWxwaGEx\">https://mvnrepository.com/artifact/org.slf4j/slf4j-reload4j/2.1.0-alpha1</span></p>\n<h3 id=\"hdfs-写流程\"><a class=\"markdownIt-Anchor\" href=\"#hdfs-写流程\">#</a> hdfs 写流程</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222138786.png\" alt=\"image-20240804222138786\"></p>\n<blockquote>\n<p>（1）客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p>\n<p>（2）NameNode 返回是否可以上传。</p>\n<p>（3）客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p>\n<p>（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。</p>\n<p>（5）客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>\n<p>（6）dn1、dn2、dn3 逐级应答客户端。</p>\n<p>（7）客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet 会放入一个应答队列等待应答。</p>\n<p>（8）当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行 3-7 步）。</p>\n</blockquote>\n<h3 id=\"网络拓扑-节点距离计算\"><a class=\"markdownIt-Anchor\" href=\"#网络拓扑-节点距离计算\">#</a> 网络拓扑 - 节点距离计算</h3>\n<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。那么这个最近距离怎么计算呢？</p>\n<p>节点距离：两个节点到达最近的共同祖先的距离总和</p>\n<p>第一个副本在 Client 所处的节点上。如果客户端在集群外，随机选一个</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222158187.png\" alt=\"image-20240804222158187\"></p>\n<h3 id=\"hdfs-读流程\"><a class=\"markdownIt-Anchor\" href=\"#hdfs-读流程\">#</a> hdfs 读流程</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222217631.png\" alt=\"image-20240804222217631\"></p>\n<p>判断权限，文件是否存在</p>\n<p>选择原则：节点距离，节点负载</p>\n<p>串行读</p>\n<blockquote>\n<p>（1）客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</p>\n<p>（2）挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</p>\n<p>（3）DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。</p>\n<p>（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</p>\n</blockquote>\n<h3 id=\"nn-2nn\"><a class=\"markdownIt-Anchor\" href=\"#nn-2nn\">#</a> nn &amp; 2nn</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222236021.png\" alt=\"image-20240804222236021\"></p>\n<p>(1) Fsimage 文件：HDFS 文件系统元数据的一个永久性的检查点其中包含 HDFS 文件系统的所有目录和文件 inode 的序列化信息。</p>\n<p>Edits 文件：存放 HDFS 文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到 Edits 文件中。</p>\n<p>(3) seen txid 文件保存的是一个数字，就是最后一个 edits 的数字</p>\n<p>(4) 每次 NameNode 启动的时候都会将 Fsimage 文件读入内存，加载 Edits 里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成 NameNode 启动的时候就将 Fsimage 和 Edits 文件进行了合并。</p>\n<p>查看镜像文件</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs oiv -p <span class=\"variable constant_\">XML</span> -i fsimage_0000000000000000517 -o /root/image.<span class=\"property\">xml</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">479</span> <span class=\"variable constant_\">INFO</span> offlineImageViewer.<span class=\"property\">FSImageHandler</span>: <span class=\"title class_\">Loading</span> <span class=\"number\">5</span> strings</span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">GLOBAL</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">29</span> maxEntries=<span class=\"number\">536870911</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">USER</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">24</span> maxEntries=<span class=\"number\">16777215</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">GROUP</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">24</span> maxEntries=<span class=\"number\">16777215</span></span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">03</span> <span class=\"number\">13</span>:<span class=\"number\">21</span>:<span class=\"number\">15</span>,<span class=\"number\">494</span> <span class=\"variable constant_\">INFO</span> namenode.<span class=\"property\">FSDirectory</span>: <span class=\"variable constant_\">XATTR</span> serial <span class=\"attr\">map</span>: bits=<span class=\"number\">24</span> maxEntries=<span class=\"number\">16777215</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span><br><span class=\"line\">&lt;fsimage&gt;<span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">layoutVersion</span>&gt;</span>-66<span class=\"tag\">&lt;/<span class=\"name\">layoutVersion</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">onDiskVersion</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">onDiskVersion</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">oivRevision</span>&gt;</span>1be78238728da9266a4f88195058f08fd012bf9c<span class=\"tag\">&lt;/<span class=\"name\">oivRevision</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">NameSection</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">namespaceId</span>&gt;</span>345030828<span class=\"tag\">&lt;/<span class=\"name\">namespaceId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">genstampV1</span>&gt;</span>1000<span class=\"tag\">&lt;/<span class=\"name\">genstampV1</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">genstampV2</span>&gt;</span>1040<span class=\"tag\">&lt;/<span class=\"name\">genstampV2</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">genstampV1Limit</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">genstampV1Limit</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">lastAllocatedBlockId</span>&gt;</span>1073741864<span class=\"tag\">&lt;/<span class=\"name\">lastAllocatedBlockId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">txid</span>&gt;</span>517<span class=\"tag\">&lt;/<span class=\"name\">txid</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">NameSection</span>&gt;</span></span></span><br><span class=\"line\">&lt;<span class=\"title class_\">ErasureCodingSection</span>&gt;</span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">policyId</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">policyId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyName</span>&gt;</span>RS-6-3-1024k<span class=\"tag\">&lt;/<span class=\"name\">policyName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">cellSize</span>&gt;</span>1048576<span class=\"tag\">&lt;/<span class=\"name\">cellSize</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyState</span>&gt;</span>DISABLED<span class=\"tag\">&lt;/<span class=\"name\">policyState</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">codecName</span>&gt;</span>rs<span class=\"tag\">&lt;/<span class=\"name\">codecName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">dataUnits</span>&gt;</span>6<span class=\"tag\">&lt;/<span class=\"name\">dataUnits</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">parityUnits</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">parityUnits</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">policyId</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">policyId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyName</span>&gt;</span>RS-3-2-1024k<span class=\"tag\">&lt;/<span class=\"name\">policyName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">cellSize</span>&gt;</span>1048576<span class=\"tag\">&lt;/<span class=\"name\">cellSize</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyState</span>&gt;</span>DISABLED<span class=\"tag\">&lt;/<span class=\"name\">policyState</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">codecName</span>&gt;</span>rs<span class=\"tag\">&lt;/<span class=\"name\">codecName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">dataUnits</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">dataUnits</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">parityUnits</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">parityUnits</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">policyId</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">policyId</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyName</span>&gt;</span>RS-LEGACY-6-3-1024k<span class=\"tag\">&lt;/<span class=\"name\">policyName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">cellSize</span>&gt;</span>1048576<span class=\"tag\">&lt;/<span class=\"name\">cellSize</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">policyState</span>&gt;</span>DISABLED<span class=\"tag\">&lt;/<span class=\"name\">policyState</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">codecName</span>&gt;</span>rs-legacy<span class=\"tag\">&lt;/<span class=\"name\">codecName</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">dataUnits</span>&gt;</span>6<span class=\"tag\">&lt;/<span class=\"name\">dataUnits</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">parityUnits</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">parityUnits</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">ecSchema</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\"><span class=\"tag\">&lt;/<span class=\"name\">erasureCodingPolicy</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>datanode 主动向 namenode 上报文件快信息</p>\n<h3 id=\"查看edits日志\"><a class=\"markdownIt-Anchor\" href=\"#查看edits日志\">#</a> 查看 edits 日志</h3>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs oev -p <span class=\"variable constant_\">XML</span> -i /data/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000520  -o /root/image.<span class=\"property\">xml</span></span><br><span class=\"line\"></span><br><span class=\"line\">记录操作步骤，仅进行追加</span><br><span class=\"line\"></span><br><span class=\"line\">每进行一小时进行合并</span><br><span class=\"line\"></span><br><span class=\"line\">2nn没有edits_progres</span><br><span class=\"line\"></span><br><span class=\"line\">namenode 会把edit_Progress 后的合并</span><br><span class=\"line\">比如后面合并<span class=\"number\">356</span>以后的</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222255010.png\" alt=\"image-20240804222255010\"></p>\n<h3 id=\"checkpoint\"><a class=\"markdownIt-Anchor\" href=\"#checkpoint\">#</a> checkpoint</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3600<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    The number of seconds between two periodic checkpoints. </span><br><span class=\"line\">    Support multiple time unit suffix(case insensitive), as described</span><br><span class=\"line\">    in dfs.heartbeat.interval.If no time unit is specified then seconds</span><br><span class=\"line\">    is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1000000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The Secondary NameNode or CheckpointNode will create a checkpoint</span><br><span class=\"line\">  of the namespace every &#x27;dfs.namenode.checkpoint.txns&#x27; transactions, regardless</span><br><span class=\"line\">  of whether &#x27;dfs.namenode.checkpoint.period&#x27; has expired.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>60<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The SecondaryNameNode and CheckpointNode will poll the NameNode</span><br><span class=\"line\">  every &#x27;dfs.namenode.checkpoint.check.period&#x27; seconds to query the number</span><br><span class=\"line\">  of uncheckpointed transactions. Support multiple time unit suffix(case insensitive),</span><br><span class=\"line\">  as described in dfs.heartbeat.interval.If no time unit is specified then</span><br><span class=\"line\">  seconds is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">3600s每隔一小时执行一次</span><br><span class=\"line\">或者每100w次执行一次，每隔60s看下有没有到100w</span><br></pre></td></tr></table></figure>\n<h3 id=\"datanode\"><a class=\"markdownIt-Anchor\" href=\"#datanode\">#</a> datanode</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222303971.png\" alt=\"image-20240804222303971\"></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>21600000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.directoryscan.interval<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>21600<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Interval in seconds for Datanode to scan data directories and</span><br><span class=\"line\">  reconcile the difference between blocks in memory and on the disk.</span><br><span class=\"line\">  Support multiple time unit suffix(case insensitive), as described</span><br><span class=\"line\">  in dfs.heartbeat.interval.If no time unit is specified then seconds</span><br><span class=\"line\">  is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<h3 id=\"数据完整性\"><a class=\"markdownIt-Anchor\" href=\"#数据完整性\">#</a> 数据完整性</h3>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如下是DataNode节点保证数据完整性的方法。</span><br><span class=\"line\">(1)当DataNode读取Block的时候,它会计算CheckSum。</span><br><span class=\"line\">(2)如果计算后的CheckSum,与Block创建时值不一样,说明Block已经损坏。</span><br><span class=\"line\">(3)Client读取其他DataNode上的Block。</span><br><span class=\"line\">(4)常见的校验算法crc(32),md5(128),shal(160) </span><br><span class=\"line\">(5)DataNode在其文件创建后周期验证CheckSum。</span><br></pre></td></tr></table></figure>\n<h3 id=\"掉线参数\"><a class=\"markdownIt-Anchor\" href=\"#掉线参数\">#</a> 掉线参数</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222317915.png\" alt=\"image-20240804222317915\"></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.heartbeat.interval<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    Determines datanode heartbeat interval in seconds.</span><br><span class=\"line\">    Can use the following suffix (case insensitive):</span><br><span class=\"line\">    ms(millis), s(sec), m(min), h(hour), d(day)</span><br><span class=\"line\">    to specify the time (such as 2s, 2m, 1h, etc.).</span><br><span class=\"line\">    Or provide complete number in seconds (such as 30 for 30 seconds).</span><br><span class=\"line\">    If no time unit is specified then seconds is assumed.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>300000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    This time decides the interval to check for expired datanodes.</span><br><span class=\"line\">    With this value and dfs.heartbeat.interval, the interval of</span><br><span class=\"line\">    deciding the datanode is stale or not is also calculated.</span><br><span class=\"line\">    The unit of this configuration is millisecond.</span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">hdfs块大小 读写速度越快，块可以配置越大。一般是128M或者256M</span><br><span class=\"line\">shell操作</span><br><span class=\"line\">读些流程</span><br></pre></td></tr></table></figure>\n<h2 id=\"mapreduce\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce\">#</a> mapreduce</h2>\n<p>MapReduce 是一个分布式运算程序的编程框架，是用户开发 &quot;基于 Hadoop 的数据分析应用&quot; 的核心框架。</p>\n<p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>\n<p>MapReduce: 自己处理业务相关代码 + 自身的默认代码</p>\n<p>优点:</p>\n<p>1、易于编程。用户只关心，业务逻辑。实现框架的接口。</p>\n<p>2、良好扩展性：可以动态增加服务器，解决计算资源不够问题</p>\n<p>3、高容错性。任何一台机器挂擦，可以将任务转移到其他节点。</p>\n<p>4、适合海量数据计算 (TB/PB) 几千台服务器共同计算。</p>\n<p>重阳市公安局</p>\n<p>トラスメット吉祥如意</p>\n<p>半夏散文章</p>\n<p>Праведая праведая我姓张却长不出你爱的模样</p>\n<p>天下之忧而忧无虑</p>\n<p>小麦小兜到了解那样的很美</p>\n<p>多少年重阳节快乐成长庚子大吉</p>\n<p>缺点:</p>\n<p>1、不擅长实时计算。Mysql</p>\n<p>2、不擅长流式计算。Sparkstreaming flink</p>\n<p>3、不擅长 DAG 有向无环图计算。spark</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222335906.png\" alt=\"image-20240804222335906\"></p>\n<p>1) MapReduce 运算程序一般需要分成 2 个阶段：Map 阶段和 Reduce 阶段</p>\n<p>2) Map 阶段的并发 MapTask, 完全并行运行，互不相干</p>\n<p>3) Reduce 阶段的并发 ReduceTask, 完全互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出</p>\n<p>4) MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行</p>\n<p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程:</p>\n<p>(1) MrAppMaster: 负责整个程序的过程调度及状态协调。</p>\n<p>(2) MapTask: 负责 Map 阶段的整个数据处理流程。</p>\n<p>(3) ReduceTask: 负责 Reduce 阶段的整个数据处理流程。</p>\n<p>数据序列化类型</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222347314.png\" alt=\"image-20240804222347314\"></p>\n<h3 id=\"mapreduce编程规范\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce编程规范\">#</a> mapreduce 编程规范</h3>\n<p>1.Mapper 阶段</p>\n<p>(1) 用户自定义的 Mapper 要继承自己的父类</p>\n<p>(2) Mapper 的输入数据是 KV 对的形式 (KV 的类型可自定义)</p>\n<p>(3) Mapper 中的业务逻辑写在 map () 方法中</p>\n<p>(4) Mapper 的输出数据是 KV 对的形式 (KV 的类型可自定义)</p>\n<p>(5) map () 方法 (MapTask 进程) 对每一个 &lt; K,V &gt; 调用一次</p>\n<p>2.Reducer 阶段</p>\n<p>(1) 用户自定义的 Reducer 要继承自己的父类</p>\n<p>(2) Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV</p>\n<p>(3) Reducer 的业务逻辑写在 reduce () 方法中</p>\n<p>ReduceTask 进程对每一组相同 k 的 &lt;k,v&gt; 组调用一次 reduce () 方法法</p>\n<p>3.Driver 阶段</p>\n<p>相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群，提交的是</p>\n<p>封装了 MapReduce 程序相关运行参数的 job 对象</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222405781.png\" alt=\"image-20240804222405781\"></p>\n<h3 id=\"wordcount\"><a class=\"markdownIt-Anchor\" href=\"#wordcount\">#</a> wordcount</h3>\n<p>1.mapper</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.wordcount2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountMapper</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Mapper</span>&lt;LongWritable , Text, Text, IntWritable&gt;&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">Text</span> <span class=\"variable\">outk</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Text</span>();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outv</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">map</span><span class=\"params\">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"comment\">// get a line ,text to string</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">line</span> <span class=\"operator\">=</span> value.toString();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 切割</span></span><br><span class=\"line\">        String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// loop write out</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (String word:words)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            outk.set(word);</span><br><span class=\"line\">            context.write(outk,outv);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>reducer</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.wordcount2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountReducer</span> <span class=\"keyword\">extends</span>  <span class=\"title class_\">Reducer</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outV</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">reduce</span><span class=\"params\">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">sum</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (IntWritable value : values) &#123;</span><br><span class=\"line\">            sum += value.get();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        outV.set(sum);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 写出</span></span><br><span class=\"line\">        context.write(key,outV);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>driver</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.example.wordcount2;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountDriver</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 1 获取job</span></span><br><span class=\"line\">        <span class=\"type\">Configuration</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">        <span class=\"type\">Job</span> <span class=\"variable\">job</span> <span class=\"operator\">=</span> Job.getInstance(conf);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 2 设置jar包路径</span></span><br><span class=\"line\">        job.setJarByClass(WordCountDriver.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 3 关联mapper和reducer</span></span><br><span class=\"line\">        job.setMapperClass(WordCountMapper.class);</span><br><span class=\"line\">        job.setReducerClass(WordCountReducer.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 4 设置map输出的kv类型</span></span><br><span class=\"line\">        job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 5 设置最终输出的kV类型</span></span><br><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 6 设置输入路径和输出路径</span></span><br><span class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">0</span>]));</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">1</span>]));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 7 提交job</span></span><br><span class=\"line\">        <span class=\"type\">boolean</span> <span class=\"variable\">result</span> <span class=\"operator\">=</span> job.waitForCompletion(<span class=\"literal\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        System.exit(result ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>package，将没有依赖的包放到 hadoop 集群执行</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop jar MapReduceDemo-<span class=\"number\">1.0</span>-SNAPSHOT.jar org.example.wordcount2.WordCountDriver /input /uuu</span><br></pre></td></tr></table></figure>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2NDEzMDY1L2FydGljbGUvZGV0YWlscy8xMTY0MTkzMjY=\">https://blog.csdn.net/m0_46413065/article/details/116419326</span></p>\n<h3 id=\"序列化\"><a class=\"markdownIt-Anchor\" href=\"#序列化\">#</a> 序列化</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222417067.png\" alt=\"image-20240804222417067\"></p>\n<p>java 自带 serializable 很重，带了各种校验头信息。</p>\n<p>hadoop 序列化：</p>\n<p>紧凑：存储空间少</p>\n<p>快速：传输速度快</p>\n<p>互操作性:</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222431605.png\" alt=\"image-20240804222431605\"></p>\n<h3 id=\"mapreduce-框架原理\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce-框架原理\">#</a> mapreduce 框架原理</h3>\n<h4 id=\"inputformat数据输入\"><a class=\"markdownIt-Anchor\" href=\"#inputformat数据输入\">#</a> inputformat 数据输入</h4>\n<p>MapTask 并行度决定机制</p>\n<p>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</p>\n<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222447121.png\" alt=\"image-20240804222447121\"></p>\n<h4 id=\"fileinputformat-切片机制\"><a class=\"markdownIt-Anchor\" href=\"#fileinputformat-切片机制\">#</a> fileinputformat 切片机制</h4>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222459009.png\" alt=\"image-20240804222459009\"></p>\n<p>(1) 源码中计算切片大小的公式</p>\n<p>Math.max(minSize, Math.min(maxSize, blockSize));</p>\n<p>mapreduce.input.fileinputformat.split.minsize=1 默认值为 1</p>\n<p>mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值 Long.MAXValue</p>\n<p>因此，默认情况下，切片大小 = blocksize。</p>\n<p>(2) 切片大小设置</p>\n<p>maxsize (切片最大值): 参数如果调得比 blockSize 小，则会让切片变小，而且就等于配置的这个参数的值。</p>\n<p>minsize (切片最小值): 参数调的比 blockSize 大，则可以让切片变得比 blockSize 还大。</p>\n<p>(3) 获取切片信息 API</p>\n<p>// 获取切片的文件名称</p>\n<p>String name = inputSplit.getPath().getNamie ();</p>\n<p>// 根据文件类型获取切片信息</p>\n<p>FileSplit inputSplit = (FileSplit) context.getInputsplit()</p>\n<p>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValuue TextInputFormat 、NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</p>\n<h4 id=\"textinputformat切片机制\"><a class=\"markdownIt-Anchor\" href=\"#textinputformat切片机制\">#</a> textinputformat 切片机制</h4>\n<p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，LongWritable 类型。值是这行的内容，不包括任何行终止符 (换行符和回车符),Text 类型。</p>\n<h4 id=\"combinetextinputformat\"><a class=\"markdownIt-Anchor\" href=\"#combinetextinputformat\">#</a> combinetextinputformat</h4>\n<p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask, 这样如果有大量小文件，就会产生大量的 MapTask, 处理效率极其低下。</p>\n<p>1) 应用场景:</p>\n<p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个 / 小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。T</p>\n<p>2) 虚拟存储切片最大值设置 (</p>\n<p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</p>\n<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>\n<p>3) 切片机制～</p>\n<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>\n<p>切片过程</p>\n<p>(a) 判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。</p>\n<p>(b) 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 5 设置最终输出的kV类型</span></span><br><span class=\"line\">job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class=\"line\">CombineTextInputFormat.setMaxInputSplitSize(job,<span class=\"number\">4194034</span>);</span><br><span class=\"line\"><span class=\"comment\">// 6 设置输入路径和输出路径</span></span><br><span class=\"line\">FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">0</span>]));</span><br><span class=\"line\">FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">1</span>]));</span><br></pre></td></tr></table></figure>\n<h3 id=\"mapreduce工作流程\"><a class=\"markdownIt-Anchor\" href=\"#mapreduce工作流程\">#</a> mapreduce 工作流程</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222514569.png\" alt=\"image-20240804222514569\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222529350.png\" alt=\"image-20240804222529350\"></p>\n<h3 id=\"shuffle\"><a class=\"markdownIt-Anchor\" href=\"#shuffle\">#</a> shuffle</h3>\n<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</p>\n<p>可以进行排序压缩等操作</p>\n<p>maptask 阶段：</p>\n<p>对 key 的索引按照字典序快排</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222546927.png\" alt=\"image-20240804222546927\"></p>\n<h3 id=\"partition分区\"><a class=\"markdownIt-Anchor\" href=\"#partition分区\">#</a> partition 分区</h3>\n<p>要求将统计结果按照条件输出到不同文件中 (分区)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class=\"line\">        </span><br><span class=\"line\">        job.setNumReduceTasks(<span class=\"number\">2</span>);</span><br><span class=\"line\">        <span class=\"comment\">// 6 设置输入路径和输出路径</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HashPartitioner</span>&lt;K, V&gt; extendss Partitioner&lt;K, V&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getPartition</span><span class=\"params\">(K key, V value, <span class=\"type\">int</span> 1humReduceTasks)</span></span><br><span class=\"line\">         <span class=\"keyword\">return</span> (key.hashCode() &amp; Integer.MAX_VALUE)៖ numReduceTasks;</span><br></pre></td></tr></table></figure>\n<p>如果分区 &gt; 1 才有 hash</p>\n<p>否则直接 partition-1 = 0 只有 0 号分区</p>\n<p>默认分区是根据 key 的 hashCode 对 ReduceTasks 个数取模得到的。用户没法控制哪个 key 存储到哪个分区。</p>\n<p>自定义 partitoner</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222606892.png\" alt=\"image-20240804222606892\"></p>\n<h3 id=\"排序\"><a class=\"markdownIt-Anchor\" href=\"#排序\">#</a> 排序</h3>\n<p>对于 MapTask, 它会将处理的结果暂时放到环形缓冲区中，当环不形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有有文件进行归并排序。</p>\n<p>对于 ReduceTask, 它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内字中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask 统一对内存和磁盘上的所有数据进行一次师日并排序</p>\n<h2 id=\"yarn\"><a class=\"markdownIt-Anchor\" href=\"#yarn\">#</a> yarn</h2>\n<p>Yam 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。</p>\n<p>YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件构成。</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222630987.png\" alt=\"image-20240804222630987\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222643801.png\" alt=\"image-20240804222643801\"></p>\n<h3 id=\"yarn调度器\"><a class=\"markdownIt-Anchor\" href=\"#yarn调度器\">#</a> yarn 调度器</h3>\n<p>目前，Hadoop 作业调度器主要有三种：FIFO、容量 (CapacityScheduler) 和公平 (Fair Scheduler)。Apache Hadoop3.1.3 默认的资源调度器是 Capacity SScheduler</p>\n<h4 id=\"fifo调度器\"><a class=\"markdownIt-Anchor\" href=\"#fifo调度器\">#</a> FIFO 调度器</h4>\n<p>(FirstInFirstOut) 单队列，根据提交作业的先后顺序，先来先服务</p>\n<h4 id=\"容量调度器\"><a class=\"markdownIt-Anchor\" href=\"#容量调度器\">#</a> 容量调度器</h4>\n<p>1、多队列：每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略。</p>\n<p>2、容量保证：管理员可为每个队列设置资源最低保证和资源使用上限</p>\n<p>3、灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。</p>\n<p>4、多租户：支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会又对同一用户提交的作业所占资源量进行限定。</p>\n<p>1) 队列资源分配</p>\n<p>从 root 开始，使用深度优先算法，优先选择资源占用率最低的队列分配资源。</p>\n<p>2) 作业资源分配</p>\n<p>默认按照提交作业的优先级和提交时间顺序分配资源。</p>\n<p>3) 容器资源分配</p>\n<p>按照容器的优先级分配资源；如果优先级相同，按照数据本地性原则:</p>\n<p>(1) 任务和数据在同一节点</p>\n<p>(2) 任务和数据在同一机架</p>\n<p>(3) 任务和数据不在同一节点也不在同一机架</p>\n<h4 id=\"公平调度器\"><a class=\"markdownIt-Anchor\" href=\"#公平调度器\">#</a> 公平调度器</h4>\n<p>同队列所有任务共享资源，在时间尺度上获得公平的资源</p>\n<p>1) 与容量调度器相同点</p>\n<p>(1) 多队列：支持多队列多作业</p>\n<p>(2) 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线</p>\n<p>(3) 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。</p>\n<p>(4) 多租户：支持多用户共享集群和多应用程序同时运行；为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。</p>\n<p>2) 与容量调度器不同点</p>\n<p>(1) 核心调度策略不同</p>\n<p>容量调度器：优先选择资源利用率低的队列</p>\n<p>公平调度器：优先选择对资源的缺额比例大的</p>\n<p>(2) 每个队列可以单独设置资源分配方式</p>\n<p>容量调度器：FIFO，DRF</p>\n<p>公平调度器：FIFO，DRF，FAIR</p>\n<p>公平调度器设计目标是：在时间尺度上，所有作业获得公平的的资源。某一时刻一个作业应获资源和实际获取资源的差距叫 &quot;缺额&quot;</p>\n<p>调度器会优先为缺额大的作业分配资源</p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222659518.png\" alt=\"image-20240804222659518\"></p>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222717084.png\" alt=\"image-20240804222717084\"></p>\n<h4 id=\"drf\"><a class=\"markdownIt-Anchor\" href=\"#drf\">#</a> DRF</h4>\n<p>DRF (DominantResource Fairness), 我们之前说的资源，都那是单一标准，例如只考虑内存 (也是 Yam 默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU, 网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。</p>\n<p>那么在 YARN 中，我们用 DRF 来决定如何调度:</p>\n<p>假设集群一共有 100CPU 和 10T 内存，而应用 A 需要 (2CPU,300GB), 应用 B 需要 (6CPU,100GB)。则两个应用分别需要 A (2% CPU,3% 内存) 和 B (6% CPU,1% 为存) 的资源，这就意味着 A 是内存主导的，B 是 CPU 主导的，针对这种情况，我们可以选择 DRF 策略对不同应用进行不同资源 (CPU 和内存) 的一个不同比例的限制。</p>\n<h3 id=\"yarn-shell\"><a class=\"markdownIt-Anchor\" href=\"#yarn-shell\">#</a> yarn shell</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn application -list </span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">11</span> <span class=\"number\">06</span>:<span class=\"number\">34</span>:<span class=\"number\">10</span>,<span class=\"number\">406</span> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop101/<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.191</span>:<span class=\"number\">8032</span></span><br><span class=\"line\">Total number of <span class=\"title function_\">applications</span> <span class=\"params\">(application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: [])</span>:<span class=\"number\">1</span></span><br><span class=\"line\">                Application-Id      Application-Name        Application-Type          User           Queue                   State           Final-State             Progress                        Tracking-URL</span><br><span class=\"line\">application_1717055365946_0004            word count               MAPREDUCE          root         <span class=\"keyword\">default</span>                 RUNNING             UNDEFINED                   <span class=\"number\">0</span>%              http:<span class=\"comment\">//hadoop100:38735</span></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn application -list </span><br><span class=\"line\"><span class=\"number\">2024</span>-<span class=\"number\">06</span>-<span class=\"number\">11</span> <span class=\"number\">06</span>:<span class=\"number\">34</span>:<span class=\"number\">10</span>,<span class=\"number\">406</span> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop101/<span class=\"number\">192.168</span><span class=\"number\">.13</span><span class=\"number\">.191</span>:<span class=\"number\">8032</span></span><br><span class=\"line\">Total number of <span class=\"title function_\">applications</span> <span class=\"params\">(application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: [])</span>:<span class=\"number\">1</span></span><br><span class=\"line\">                Application-Id      Application-Name        Application-Type          User           Queue                   State           Final-State             Progress                        Tracking-URL</span><br><span class=\"line\">application_1717055365946_0004            word count               MAPREDUCE          root         <span class=\"keyword\">default</span>                 RUNNING             UNDEFINED                   <span class=\"number\">0</span>%              http:<span class=\"comment\">//hadoop100:38735</span></span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn application -kill application_1717055365946_0004  </span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn logs -applicationId application_1717055365946_0004  </span><br><span class=\"line\">root<span class=\"meta\">@hadoop100</span>:~# yarn logs -applicationId application_1717055365946_0004  -containerId </span><br><span class=\"line\"># 查看尝试运行的任务</span><br><span class=\"line\">yarn applicationattempt -list application_1717055365946_0004 </span><br><span class=\"line\"># </span><br><span class=\"line\">yarn applicationattempt -status application_1717055365946_0004 </span><br><span class=\"line\"># yarn 容器状态，只有在任务运行中可以查看</span><br><span class=\"line\">yarn container -list </span><br><span class=\"line\">yarn container -status</span><br><span class=\"line\">yarn node -list -all</span><br><span class=\"line\"># 加载队列配置</span><br><span class=\"line\">yarn rmadmin -refreshQueues</span><br><span class=\"line\"># 查看队列</span><br><span class=\"line\">yarn queue -status <span class=\"keyword\">default</span> </span><br></pre></td></tr></table></figure>\n<h3 id=\"yarn生产环境参数\"><a class=\"markdownIt-Anchor\" href=\"#yarn生产环境参数\">#</a> yarn 生产环境参数</h3>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222733648.png\" alt=\"image-20240804222733648\"></p>\n<h3 id=\"多队列\"><a class=\"markdownIt-Anchor\" href=\"#多队列\">#</a> 多队列</h3>\n<p>1) 在生产环境怎么创建队列？</p>\n<p>(1) 调度器默认就 1 个 default 队列，不能满足生产要求。</p>\n<p>(2) 按照框架:hive/spark/flink 每个框架的任务放入指定的队列 (企业用的不是特别多)</p>\n<p>(3) 按照业务模块：登录注册、购物车、下单、业务部门 1、业务部门 2</p>\n<p>2) 创建多队列的好处</p>\n<p>(1) 因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。</p>\n<p>(2) 实现任务的降级使用，特殊时期保证重要的任务队列资原充足。</p>\n<p>业务部门 1 (重要)=》业务部门 2 (比较重要)=》下单 (一般)=》购物车 (一般)=》登录注册 (次要)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  vim capacity-scheduler.xml </span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"keyword\">default</span>,hive&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The queues at the <span class=\"built_in\">this</span> <span class=\"title function_\">level</span> <span class=\"params\">(root is the root queue)</span>.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.capacity&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">40</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">60</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.user-limit-factor&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">0.7</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      Default queue user limit a percentage from <span class=\"number\">0.0</span> to <span class=\"number\">1.0</span>.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">1</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      Default queue user limit a percentage from <span class=\"number\">0.0</span> to <span class=\"number\">1.0</span>.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.maximum-capacity&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">60</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The maximum capacity of the <span class=\"keyword\">default</span> queue.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.state&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The state of the <span class=\"keyword\">default</span> queue. State can be one of RUNNING or STOPPED.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.acl_submit_applications&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The ACL of who can submit jobs to the <span class=\"keyword\">default</span> queue.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.<span class=\"keyword\">default</span>.acl_administer_queue&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      The ACL of who can administer jobs on the <span class=\"keyword\">default</span> queue.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetimeTimeout</span><br><span class=\"line\">参考资料：https:<span class=\"comment\">//blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 </span><br><span class=\"line\">--&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-application-lifetime&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;-<span class=\"number\">1</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;!-- 如果application没指定超时时间，则用<span class=\"keyword\">default</span>-application-lifetime作为默认值 --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.capacity.root.hive.<span class=\"keyword\">default</span>-application-lifetime&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;-<span class=\"number\">1</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"># 提交任务到不同队列</span><br><span class=\"line\">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-example-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar wordcount -D mapreduce.job.queuename=hive /input /outputttt</span><br><span class=\"line\"></span><br><span class=\"line\"># 或者在driver中</span><br><span class=\"line\"><span class=\"type\">Configure</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configure</span>();</span><br><span class=\"line\">conf.set(<span class=\"string\">&quot;mapreduce.job.queuename&quot;</span>,<span class=\"string\">&quot;hive&quot;</span>);</span><br></pre></td></tr></table></figure>\n<h3 id=\"任务优先级\"><a class=\"markdownIt-Anchor\" href=\"#任务优先级\">#</a> 任务优先级</h3>\n<p>容量调度器，支持任务优先级的配置，在资源紧张时，优先吸高的任务将优先获取资源。默认情况，Yarn 将所有任务的优先级限制为 0，若想使用任务的优先级功能，须开放该限制。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"number\">5</span>&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\">hadoop jar hadoop-mapreduce-examples-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar pi -D mapreduce.job.priorit=<span class=\"number\">5</span> <span class=\"number\">5</span> <span class=\"number\">20000</span></span><br><span class=\"line\"></span><br><span class=\"line\"># 动态设置优先级</span><br><span class=\"line\">yarn application -appId xxx -upgradePriority <span class=\"number\">6</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置多队列公平调度器\"><a class=\"markdownIt-Anchor\" href=\"#配置多队列公平调度器\">#</a> 配置多队列公平调度器</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim yarn-site.xml</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;配置使用公平调度器&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;/opt/<span class=\"keyword\">module</span>/hadoop-<span class=\"number\">3.1</span><span class=\"number\">.3</span>/etc/hadoop/fair-scheduler.xml&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;指明公平调度器队列分配配置文件&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;<span class=\"literal\">false</span>&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;禁止队列间资源抢占&lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\">vim fair-scheduler.xml</span><br><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span><br><span class=\"line\">&lt;allocations&gt;</span><br><span class=\"line\">  &lt;!-- 单个队列中Application Master占用资源的最大比例,取值<span class=\"number\">0</span>-<span class=\"number\">1</span> ，企业一般配置<span class=\"number\">0.1</span> --&gt;</span><br><span class=\"line\">  &lt;queueMaxAMShareDefault&gt;<span class=\"number\">0.5</span>&lt;/queueMaxAMShareDefault&gt;</span><br><span class=\"line\">  &lt;!-- 单个队列最大资源的默认值 test atguigu <span class=\"keyword\">default</span> --&gt;</span><br><span class=\"line\">  &lt;queueMaxResourcesDefault&gt;4096mb,4vcores&lt;/queueMaxResourcesDefault&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">  &lt;!-- 增加一个队列test --&gt;</span><br><span class=\"line\">  &lt;queue name=<span class=\"string\">&quot;test&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 队列最小资源 --&gt;</span><br><span class=\"line\">    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列最大资源 --&gt;</span><br><span class=\"line\">    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列中最多同时运行的应用数，默认<span class=\"number\">50</span>，根据线程数配置 --&gt;</span><br><span class=\"line\">    &lt;maxRunningApps&gt;<span class=\"number\">4</span>&lt;/maxRunningApps&gt;</span><br><span class=\"line\">    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span><br><span class=\"line\">    &lt;!-- &lt;maxAMShare&gt;<span class=\"number\">0.5</span>&lt;/maxAMShare&gt;--&gt;</span><br><span class=\"line\">    &lt;!-- 该队列资源权重,默认值为<span class=\"number\">1.0</span> --&gt;</span><br><span class=\"line\">    &lt;weight&gt;<span class=\"number\">1.0</span>&lt;/weight&gt;</span><br><span class=\"line\">    &lt;!-- 队列内部的资源分配策略 --&gt;</span><br><span class=\"line\">    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;</span><br><span class=\"line\">  &lt;/queue&gt;</span><br><span class=\"line\">  &lt;!-- 增加一个队列atguigu --&gt;</span><br><span class=\"line\">  &lt;queue name=<span class=\"string\">&quot;atguigu&quot;</span> type=<span class=\"string\">&quot;parent&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 队列最小资源 --&gt;</span><br><span class=\"line\">    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列最大资源 --&gt;</span><br><span class=\"line\">    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;</span><br><span class=\"line\">    &lt;!-- 队列中最多同时运行的应用数，默认<span class=\"number\">50</span>，根据线程数配置 --&gt;</span><br><span class=\"line\">    &lt;maxRunningApps&gt;<span class=\"number\">4</span>&lt;/maxRunningApps&gt;</span><br><span class=\"line\">    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span><br><span class=\"line\">    &lt;!-- &lt;maxAMShare&gt;<span class=\"number\">0.5</span>&lt;/maxAMShare&gt;--&gt;</span><br><span class=\"line\">    &lt;!-- 该队列资源权重,默认值为<span class=\"number\">1.0</span> --&gt;</span><br><span class=\"line\">    &lt;weight&gt;<span class=\"number\">1.0</span>&lt;/weight&gt;</span><br><span class=\"line\">    &lt;!-- 队列内部的资源分配策略 --&gt;</span><br><span class=\"line\">    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;</span><br><span class=\"line\">  &lt;/queue&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\">  &lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;</span><br><span class=\"line\">  &lt;queuePlacementPolicy&gt;</span><br><span class=\"line\">    &lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; <span class=\"literal\">false</span>表示：如果指定队列不存在,不允许自动创建--&gt;</span><br><span class=\"line\">    &lt;rule name=<span class=\"string\">&quot;specified&quot;</span> create=<span class=\"string\">&quot;false&quot;</span>/&gt;</span><br><span class=\"line\">    &lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;</span><br><span class=\"line\">    &lt;rule name=<span class=\"string\">&quot;nestedUserQueue&quot;</span> create=<span class=\"string\">&quot;true&quot;</span>&gt;</span><br><span class=\"line\">        &lt;rule name=<span class=\"string\">&quot;primaryGroup&quot;</span> create=<span class=\"string\">&quot;false&quot;</span>/&gt;</span><br><span class=\"line\">    &lt;/rule&gt;</span><br><span class=\"line\">    &lt;!-- 最后一个规则必须为reject或者<span class=\"keyword\">default</span>。Reject表示拒绝创建提交失败，<span class=\"keyword\">default</span>表示把任务提交到<span class=\"keyword\">default</span>队列 --&gt;</span><br><span class=\"line\">    &lt;rule name=<span class=\"string\">&quot;reject&quot;</span> /&gt;</span><br><span class=\"line\">  &lt;/queuePlacementPolicy&gt;</span><br><span class=\"line\">&lt;/allocations&gt;</span><br><span class=\"line\"># 指定用户和不指定用户</span><br><span class=\"line\">hadoop jar hadoop-mapreduce-examples-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar pi -D mapreduce.job.queuename=root.test <span class=\"number\">1</span> <span class=\"number\">1</span> </span><br><span class=\"line\">hadoop jar hadoop-mapreduce-examples-<span class=\"number\">3.1</span><span class=\"number\">.3</span>.jar pi <span class=\"number\">1</span> <span class=\"number\">1</span> </span><br></pre></td></tr></table></figure>\n<h3 id=\"tool接口\"><a class=\"markdownIt-Anchor\" href=\"#tool接口\">#</a> tool 接口</h3>\n<p>期望可以动态传参，结果报错，误认为是第一个输入参数。</p>\n<p>[@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1</p>\n<p>1）需求：自己写的程序也可以动态修改参数。编写 Yarn 的 Tool 接口。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">创建类WordCount并实现Tool接口：</span><br><span class=\"line\"><span class=\"keyword\">package</span> com.atguigu.yarn;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCount</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Tool</span> &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">private</span> Configuration conf;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"type\">Job</span> <span class=\"variable\">job</span> <span class=\"operator\">=</span> Job.getInstance(conf);</span><br><span class=\"line\"> </span><br><span class=\"line\">        job.setJarByClass(WordCountDriver.class);</span><br><span class=\"line\"> </span><br><span class=\"line\">        job.setMapperClass(WordCountMapper.class);</span><br><span class=\"line\">        job.setReducerClass(WordCountReducer.class);</span><br><span class=\"line\"> </span><br><span class=\"line\">        job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\"> </span><br><span class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">0</span>]));</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> <span class=\"title class_\">Path</span>(args[<span class=\"number\">1</span>]));</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">return</span> job.waitForCompletion(<span class=\"literal\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setConf</span><span class=\"params\">(Configuration conf)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.conf = conf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Configuration <span class=\"title function_\">getConf</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> conf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountMapper</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"type\">Text</span> <span class=\"variable\">outK</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Text</span>();</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outV</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">map</span><span class=\"params\">(LongWritable key, Text value, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">line</span> <span class=\"operator\">=</span> value.toString();</span><br><span class=\"line\">            String[] words = line.split(<span class=\"string\">&quot; &quot;</span>);</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (String word : words) &#123;</span><br><span class=\"line\">                outK.set(word);</span><br><span class=\"line\"> </span><br><span class=\"line\">                context.write(outK, outV);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">class</span> <span class=\"title class_\">WordCountReducer</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"type\">IntWritable</span> <span class=\"variable\">outV</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IntWritable</span>();</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">reduce</span><span class=\"params\">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">sum</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"> </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (IntWritable value : values) &#123;</span><br><span class=\"line\">                sum += value.get();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            outV.set(sum);</span><br><span class=\"line\"> </span><br><span class=\"line\">            context.write(key, outV);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222803108.png\" alt=\"image-20240804222803108\"></p>\n",
            "tags": [
                "大数据"
            ]
        }
    ]
}