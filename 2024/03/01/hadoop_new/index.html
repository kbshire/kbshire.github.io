



<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="Hexo" href="http://example.com/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="Hexo" href="http://example.com/atom.xml" />
<link rel="alternate" type="application/json" title="Hexo" href="http://example.com/feed.json" />

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  
  <meta name="keywords" content="大数据" />


<link rel="canonical" href="http://example.com/2024/03/01/hadoop_new/">



  <title>
Hadoop - 大数据 |
Yume Shoka = Hexo</title>
<meta name="generator" content="Hexo 5.4.2"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">Hadoop
  </h1>
  
<div class="meta">
  <span class="item" title="Created: 2024-03-01 13:38:45">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">Posted on</span>
    <time itemprop="dateCreated datePublished" datetime="2024-03-01T13:38:45+08:00">2024-03-01</time>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="Toggle navigation bar">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">Yume Shoka</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/84ffd76b69056962d5e97a5fa075a9f4.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/8cf1f497bbd805e019dde1603f0fb84d.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/601b4c1b1f50f3ceb9e8080e1957a890.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/f99aff0036302f62ec222c046d3e953b.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/31105e606c74614c7f8f7b1d08e1619a.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/49bee8cf2d93684cad9a1acc8bc17d8a.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">Home</a></span><i class="ic i-angle-right"></i>
<span  class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="item" rel="index" title="In 大数据"><span itemprop="name">大数据</span></a>
<meta itemprop="position" content="1" /></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="en">
  <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/01/hadoop_new/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/avatar.jpg">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content=", ">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <h2 id="hadoop"><a class="markdownIt-Anchor" href="#hadoop">#</a> hadoop</h2>
<p>Hadoop 是 Apache 软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构。</p>
<ul>
<li>Hadoop 是基于 Java 语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中；</li>
<li>Hadoop 的核心是<strong>分布式文件系统 HDFS（Hadoop Distributed File System）和 MapReduce；</strong></li>
<li>Hadoop 被公认为行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力；</li>
</ul>
<p>解决海量数据存储和分析计算问题</p>
<p>优势： 维护多副本</p>
<p>在集群间分配任务数据，方便扩展</p>
<p>并行工作</p>
<p>自动将失败任务重新分配</p>
<h4 id="hadoop-版本演进"><a class="markdownIt-Anchor" href="#hadoop-版本演进">#</a> hadoop 版本演进</h4>
<p>Apache Hadoop 版本分为两代：第一代 Hadoop 称为 Hadoop 1.0，第二代 Hadoop 称为 Hadoop 2.0。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221529201.png" alt="image-20240804221529201"></p>
<p><strong>第一代 Hadoop 包含三个大版本，分别是 0.20.x、0.21.x、0.22.x</strong>。</p>
<ul>
<li>0.20.x 最后演化成 1.0.x，变成了稳定版。</li>
<li>0.21.x 和 0.22.x 则增加了 NameNode HA 等新的重大特性。</li>
</ul>
<p><strong>第二代 Hadoop 包含两个大版本，分别是 0.23.x、2.x</strong>。</p>
<ul>
<li>它们完全不同于 Hadoop 1.0，是一套全新的架构，均包含 HDFS Federation 和 YARN 两个系统。</li>
<li>相比于 0.23.x，2.x 增加了 NameNode HA 和 Wire-compatibility 两个重大特性。</li>
</ul>
<h4 id="组件"><a class="markdownIt-Anchor" href="#组件">#</a> 组件</h4>
<p>HDFS</p>
<p>NameNode：</p>
<ul>
<li>NameNode 是 HDFS 的主节点，负责管理文件系统的命名空间和元数据信息。</li>
<li>它维护了整个文件系统的目录树结构以及文件和数据块的映射关系。</li>
<li>NameNode 还负责处理客户端的读写请求，包括打开、关闭、重命名和删除文件等操作。</li>
</ul>
<p>DataNode：</p>
<ul>
<li>DataNode 是 HDFS 的数据节点，负责存储实际的数据块。</li>
<li>它接收来自客户端或其他 DataNode 的数据写入请求，并将数据块存储在本地磁盘上。</li>
<li>DataNode 还负责处理客户端的数据读取请求，将数据块传输给客户端。</li>
</ul>
<p>Standby Namenode(2NN)：</p>
<ul>
<li>辅助 namenode。作为备用的 NameNode。当活动的 NameNode 失效时，Standby NameNode 可以接管其工作，从而提高了系统的可用性。</li>
</ul>
<p>1) NameNode (nn): 存储文件的元数据，如文件名，文件目录结构，文件属性 (生成时间、副本数、文件权限), 以及每个文件的块列表和块所在 DataNode 等。</p>
<p>2) DataNode (dn): 在本地文件系统存储文件块数据，以及块数据的校验和。</p>
<p>3）Secondary NameNode (2nn): 每隔一段时间对 NameNode 元数据备份</p>
<p><strong>YARN 组件</strong></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221615769.png" alt=""></p>
<p>1) ResourceManager (RM): 整个集群资源 (内存、CPU 等) 的老大</p>
<p>2) NodeManager (NM): 单个节点服务器资源老大</p>
<p>3) ApplicationMaster (AM): 单个任务运行的老大</p>
<p>4) Container: 容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。</p>
<p>说明 1: 客户端可以有多个</p>
<p>说明 2: 集群上可以运行多个 ApplicationMaster</p>
<p>说明 3: 每个 NodeManager 上可以有多个 Container</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221919615.png" alt="image-20240804221919615"></p>
<h4 id="部署hadoop"><a class="markdownIt-Anchor" href="#部署hadoop">#</a> 部署 hadoop</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">adduser hadoop</span><br><span class="line"></span><br><span class="line">vim /etc/sudoers</span><br><span class="line"><span class="comment"># Allow members of group sudo to execute any command</span></span><br><span class="line">%sudo   ALL=(ALL:ALL) ALL</span><br><span class="line">hadoop  ALL=(ALL:ALL) NOPASSWD:ALL</span><br><span class="line"></span><br><span class="line">hostnamectl</span><br><span class="line"></span><br><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line">apt update</span><br><span class="line"></span><br><span class="line">apt install vim net-tools lrzsz bash-com* -y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">apt install openjdk-8-jre-headless</span><br><span class="line"></span><br><span class="line">wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</span><br><span class="line">tar xzvf hadoop-3.3.6.tar.gz</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/root/hadoop-3.3.6</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<p>部署模式：</p>
<p>local: 数据存储在本地</p>
<p>pseudo-distributed：数据存储在 hdfs</p>
<p>fully-distributed：数据存储在 hdfs，多台服务器工作</p>
<p>本地运行 example wordcount</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> input</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;aa bb cc cc dd dd dd ee ee&quot;</span> &gt; input/111.txt</span><br><span class="line"></span><br><span class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount input/ output/</span><br></pre></td></tr></table></figure>
<h4 id="完全分布式部署"><a class="markdownIt-Anchor" href="#完全分布式部署">#</a> 完全分布式部署：</h4>
<p>NameNode 和 SecondaryNameNode 不要安装在同一台服务器</p>
<p>ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台机器上</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221900758.png" alt="image-20240804221900758"></p>
<p>Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认。配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<p>默认配置文件</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221943124.png" alt="image-20240804221943124"></p>
<p>自定义配置文件</p>
<p>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个配置文件存放在</p>
<p>SHADOOP_HOME/etc/kadop, 这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<p>在集群上所有节点配置</p>
<p>core-site.xml</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop100:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">description</span>&gt;</span>The name of the default file system.  A URI whose</span></span><br><span class="line"><span class="language-xml">      scheme and authority determine the FileSystem implementation.  The</span></span><br><span class="line"><span class="language-xml">      uri&#x27;s scheme determines the config property (fs.SCHEME.impl) naming</span></span><br><span class="line"><span class="language-xml">      the FileSystem implementation class.  The uri&#x27;s authority is used to</span></span><br><span class="line"><span class="language-xml">      determine the host, port, etc. for a filesystem.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      </span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hadoop-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      </span></span><br><span class="line"><span class="language-xml">    <span class="comment">&lt;!-- Static Web User Filter properties. --&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        The user name to filter as, on static web filters</span></span><br><span class="line"><span class="language-xml">        while rendering content. An example use is the HDFS</span></span><br><span class="line"><span class="language-xml">        web UI (user to be used for browsing files).</span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>vim hdfs-site.xml</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        The address and the base port where the dfs namenode web ui will listen on.</span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        The secondary namenode http server address and port.</span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span> </span><br></pre></td></tr></table></figure>
<p>vim yarn-site.xml</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">  &lt;property&gt;</span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span></span><br><span class="line"><span class="language-xml">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line">    &lt;!--<span class="language-xml"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span>--&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span>   </span><br><span class="line">  </span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Environment variables that containers may override rather than use NodeManager&#x27;s default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRER_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>vim mapred-site.xml</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span> </span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">description</span>&gt;</span>The runtime framework for executing MapReduce jobs.</span></span><br><span class="line"><span class="language-xml">  Can be one of local, classic or yarn.</span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>vim worker</p>
<p>不能有空格</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop100</span><br><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>
<p>初始化 namenode</p>
<p>只有集群第一次启动需要初始化，此时会创建数据目录</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format </span><br></pre></td></tr></table></figure>
<p>启动集群</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="variable constant_">HDFS_NAMENODE_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">HDFS_DATANODE_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">HDFS_SECONDARYNAMENODE_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">YARN_RESOURCEMANAGER_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">YARN_NODEMANAGER_USER</span>=root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 只有在这里的java_home 他启动的时候才会认，他不认/etc/profile 和 ~/.<span class="property">bashrc</span></span><br><span class="line">vim etc/hadoop/hadoop-env.<span class="property">sh</span></span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">JAVA_HOME</span>=<span class="regexp">/root/</span>jdk8u352-b08</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sbin/start-dfs.<span class="property">sh</span></span><br><span class="line"></span><br><span class="line">jps确认 </span><br><span class="line"></span><br><span class="line">curl <span class="attr">http</span>:<span class="comment">//192.168.13.190:9870/dfshealth.html#tab-overview</span></span><br><span class="line">启动resourcemanager </span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">HDFS_NAMENODE_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">HDFS_DATANODE_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">HDFS_SECONDARYNAMENODE_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">YARN_RESOURCEMANAGER_USER</span>=root</span><br><span class="line"><span class="keyword">export</span> <span class="variable constant_">YARN_NODEMANAGER_USER</span>=root</span><br><span class="line"></span><br><span class="line">设置主机互信</span><br><span class="line"></span><br><span class="line">sbin/start-yarn.<span class="property">sh</span></span><br><span class="line"></span><br><span class="line">测试</span><br><span class="line">hadoop fs -mkdir /input</span><br><span class="line">hadoop fs -put /root/hadoop-<span class="number">3.3</span><span class="number">.6</span>/input/<span class="number">1.</span>txt /input </span><br><span class="line">存储位置</span><br><span class="line">/data/hadoop-root/dfs/data/current/<span class="variable constant_">BP</span>-<span class="number">897483346</span>-<span class="number">192.168</span><span class="number">.13</span><span class="number">.190</span>-<span class="number">1716822540714</span>/current/finalized/subdir0/subdir0</span><br><span class="line"></span><br><span class="line">测试：</span><br><span class="line">hadoop jar /root/hadoop-<span class="number">3.3</span><span class="number">.6</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-<span class="number">3.3</span><span class="number">.6</span>.<span class="property">jar</span> wordcount /input /output</span><br><span class="line"></span><br><span class="line">如果有报错，是需要往mapred-site.<span class="property">xml</span>中加东西的，那么添加如下字段，在重新执行</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        <span class="language-xml"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line">        <span class="language-xml"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804221955345.png" alt="image-20240804221955345"></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbWlyYWNsZS1sdW5hL3AvMTc3ODUzMTAuaHRtbA==">https://www.cnblogs.com/miracle-luna/p/17785310.html</span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZmFucWlzb2Z0L3AvMTc4NTkwODYuaHRtbA==">https://www.cnblogs.com/fanqisoft/p/17859086.html</span></p>
<h4 id="故障处理"><a class="markdownIt-Anchor" href="#故障处理">#</a> 故障处理：</h4>
<p>停进程，删除数据目录，格式化</p>
<h4 id="记录历史运行情况"><a class="markdownIt-Anchor" href="#记录历史运行情况">#</a> 记录历史运行情况</h4>
<p>启动 history server</p>
<p>mapred-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop100:10020&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop100:19888&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;MapReduce JobHistory Server Web UI host:port&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>集群内同步该配置文件</p>
<p>重启 yarn</p>
<p>启动历史服务器，在 namenode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>
<p>测试历史功能：</p>
<p>hadoop jar /root/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222012877.png" alt="image-20240804222012877"></p>
<h4 id="日志汇聚"><a class="markdownIt-Anchor" href="#日志汇聚">#</a> 日志汇聚</h4>
<p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222029398.png" alt="image-20240804222029398"></p>
<p>namenode:</p>
<p>yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container&#x27;s logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      &quot;yarn.nodemanager.remote-app-log-dir&quot; and</span><br><span class="line">      &quot;yarn.nodemanager.remote-app-log-dir-suffix&quot; properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the </span><br><span class="line">      Application Timeline Server.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    URL for log aggregation server</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop100:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>How long to keep aggregation logs before deleting them.  -1 disables.</span><br><span class="line">    Be careful set this too small and you will spam the name node.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">mapred --daemon stop historyserver</span><br><span class="line"></span><br><span class="line">stop-yarn.sh</span><br><span class="line"></span><br><span class="line">start-yarn.sh </span><br><span class="line"></span><br><span class="line">mapred --daemon start historyserver </span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222046081.png" alt="image-20240804222046081"></p>
<p>启新任务观察：</p>
<p>对于单个组件重启</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(1)分别启动/停止HDFS组件e</span><br><span class="line">hdfs --daemon start/stop namenode/datanode/secoondarynamenode</span><br><span class="line">(2)启动/停止YARN</span><br><span class="line">yarn --daemon start/stop resourcemanager/nodemanager</span><br><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">ssh hadoop100 &quot;/root/hadoop-3.3.6/sbin/start-dfs.sh</span><br><span class="line">ssh hadoop101 &quot;/root/hadoop-3.3.6/sbin/start-yarn.sh</span><br><span class="line">ssh hadoop100 &quot;mapred --daemon start historyserver&quot;</span><br></pre></td></tr></table></figure>
<h4 id="常用端口号"><a class="markdownIt-Anchor" href="#常用端口号">#</a> 常用端口号</h4>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222100104.png" alt="image-20240804222100104"></p>
<p>常用端口号</p>
<p>hadoop3.x</p>
<p>HDFS NameNode 内部通常端口：8020/9000/9820</p>
<p>HDFS NameNode 对用户的查询端口：9870</p>
<p>Yarn 查看任务运行情况的：8088</p>
<p>历史服务器：19888</p>
<p>hadoop2.x</p>
<p>HDFS NameNode 内部通常端口：8020/9000</p>
<p>HDFS NameNode 对用户的查询端口：50070</p>
<p>Yarn 查看任务运行情况的：8088</p>
<p>历史服务器：19888</p>
<p>常用的配置文件</p>
<p>3.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml  workers</p>
<p>2.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml  slaves</p>
<h2 id="hdfs"><a class="markdownIt-Anchor" href="#hdfs">#</a> hdfs</h2>
<p>HDFS (Hadoop Distributed File System), 它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS 的使用场景：适合一次写入，多次读出的场景。一个个文件经过创建、写入和关闭之后就不能改变。</p>
<p>高容错性</p>
<p>数据自动保存多个副本。它通过增加副本的形式，提高容错性。</p>
<p>适合处理大数据</p>
<p>数据规模：能够处理数据规模达到 GB、TB、甚至 PB 级别的数据</p>
<p>文件规模：能够处理百万规模以上的文件数量，数量相当之大。</p>
<p>可构建在廉价机器上，通过多副本机制，提高可靠性。</p>
<p>不适合低延时数据访问，比如毫秒级的存储数据，是做不到到的</p>
<p>无法高效的对大量小文件进行存储。</p>
<p>存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的；</p>
<p>小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。</p>
<p>不支持并发写入、文件随机修改。</p>
<p>一个文件只能有一个写，不允许多个线程同时写；</p>
<p>仅支持数据 append (追加), 不支持文件的随机修改。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222115457.png" alt="image-20240804222115457"></p>
<p>1) NameNode (nn): 就是 Master, 它是一个主管、管理者。</p>
<p>(1) 管理 HDFS 的名称空间；</p>
<p>(2) 配置副本策略；</p>
<p>(3) 管理数据块 (Block) 映射信息；</p>
<p>(4) 处理客户端读写请求。</p>
<ol>
<li>DataNode: 就是 Slave。NameNode 下达命令，DataNode 执行实际的操作。</li>
</ol>
<p>(1) 存储实际的数据块；</p>
<p>(2) 执行数据块的读 / 写操作。</p>
<p>3) Client: 就是客户端。</p>
<p>(1) 文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block, 然后进行上传；</p>
<p>(2) 与 NameNode 交互，获取文件的位置信息；</p>
<p>(3) 与 DataNode 交互，读取或者写入数据；</p>
<p>(4) Client 提供一些命令来管理 HDFS, 比如 NameNode 格式化；</p>
<p>(5) Client 可以通过一些命令来访问 HDFS, 比如对 HDFS 增删查改操作；</p>
<p>4) Secondary NameNode: 并非 NameNode 的热备。当 NameNode 挂掉的时候，它并不</p>
<p>能马上替换 NameNode 并提供服务。</p>
<p>(1) 辅助 NameNode, 分担其工作量，比如定期合并 Fsimage 和 bEdits, 并推送给 NameNode;</p>
<p>(2) 在紧急情况下，可辅助恢复 NameNode。</p>
<p>HDFS 中的文件在物理上是分块存储 (Block), 块的大小可以通过配置参数 (dfs.blocksize) 来规定，默认大小在 Hadoop2.x/3.x 版本中是 128M,1.x 版本中是 64M。</p>
<p>磁盘速率更快的情况下，可以设置为 256M</p>
<p>(1) HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</p>
<p>(2) 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>
<p>HDFS 块的大小设置主要取决于磁盘传输速率。</p>
<h3 id="hdfs-shell"><a class="markdownIt-Anchor" href="#hdfs-shell">#</a> hdfs shell</h3>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"> hadoop fs -mkdir /haha</span><br><span class="line"> </span><br><span class="line"> # 上传，剪切本地文件</span><br><span class="line"> hadoop fs -moveFromLocal ./README.txt /haha</span><br><span class="line"> </span><br><span class="line"> # 复制本地文件</span><br><span class="line">  hadoop fs -copyFromLocal ./README.txt /haha</span><br><span class="line">  </span><br><span class="line">  # 复制本地文件</span><br><span class="line"> hadoop fs -put ./README.txt /haha</span><br><span class="line"> </span><br><span class="line"> # 追加文件到另一个文件末尾</span><br><span class="line">hadoop fs -appendToFile ./README.txt /haha/www.txt</span><br><span class="line"></span><br><span class="line"># 下载</span><br><span class="line">hadoop fs -copyToLocal /haha/1.txt ./1111.txt</span><br><span class="line"># 下载</span><br><span class="line">hadoop fs -get /haha/1.txt ./1111.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># </span><br><span class="line">hadoop fs -ls /</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /haha/111.txt</span><br><span class="line"></span><br><span class="line">hadoop fs -chown aaa:aaa /haha/1.txt</span><br><span class="line"></span><br><span class="line">hadoop fs -cp</span><br><span class="line"></span><br><span class="line">hadoop fs -mv /haha/1.txt /xixix/2.txt</span><br><span class="line"></span><br><span class="line"># 显示一个文件的末尾1kb数据</span><br><span class="line">hadoop fs -tail /haha/1.txt</span><br><span class="line"></span><br><span class="line"># 删除文件或文件夹</span><br><span class="line">hadoop fs -rm</span><br><span class="line">hadoop fs -rm -r</span><br><span class="line"></span><br><span class="line"># 查看文件夹下总大小</span><br><span class="line">hadoop fs -du -s -h /haha</span><br><span class="line"># 查看文件下每个文件大小</span><br><span class="line">hadoop fs -du -h /haha</span><br><span class="line"></span><br><span class="line"># 修改副本大小,副本数大于机器数量时，只会创建对应的副本数。后续集群内新增机器，会增加到对应的数量。但是永远保持每个机器上至多一个副本</span><br><span class="line">hadoop fs -setrep 10 /haha/1.txt</span><br></pre></td></tr></table></figure>
<h3 id="hadoop-api"><a class="markdownIt-Anchor" href="#hadoop-api">#</a> hadoop api</h3>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 在windows下安装hadoop+winutils</span><br><span class="line">https://blog.csdn.net/shulianghan/article/details/132045605</span><br><span class="line"></span><br><span class="line"># 配置maven</span><br><span class="line">https://blog.csdn.net/m0_46413065/article/details/116400168</span><br><span class="line">https://blog.csdn.net/m0_46413065/article/details/116400995</span><br><span class="line"></span><br><span class="line">hdfs-default &lt; hdfs-site &lt; resource &lt; 代码里修改configuraion.set</span><br><span class="line"></span><br><span class="line">https://blog.csdn.net/m0_46413065/article/details/116400168</span><br><span class="line"></span><br><span class="line">https://blog.csdn.net/weixin_50956145/article/details/130511265</span><br><span class="line">https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.30</span><br><span class="line">https://www.bilibili.com/read/cv28415867/?jump_opus=1</span><br><span class="line">https://www.cnblogs.com/linshengqian/p/15657694.html</span><br><span class="line">https://www.runoob.com/maven/maven-build-life-cycle.html</span><br><span class="line">https://www.cnblogs.com/xfeiyun/p/16740262.html</span><br><span class="line">https://blog.csdn.net/m0_46413065/article/details/116400995</span><br></pre></td></tr></table></figure>
<p>mvn 处理依赖</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeGxkL2FydGljbGUvZGV0YWlscy84MjI4NDI2OQ==">https://blog.csdn.net/lixld/article/details/82284269</span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0ODg2MjEzL2FydGljbGUvZGV0YWlscy8xMjM0NjE1MjI=">https://blog.csdn.net/qq_44886213/article/details/123461522</span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9tdm5yZXBvc2l0b3J5LmNvbS9hcnRpZmFjdC9vcmcuc2xmNGovc2xmNGotcmVsb2FkNGovMi4xLjAtYWxwaGEx">https://mvnrepository.com/artifact/org.slf4j/slf4j-reload4j/2.1.0-alpha1</span></p>
<h3 id="hdfs-写流程"><a class="markdownIt-Anchor" href="#hdfs-写流程">#</a> hdfs 写流程</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222138786.png" alt="image-20240804222138786"></p>
<blockquote>
<p>（1）客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p>
<p>（2）NameNode 返回是否可以上传。</p>
<p>（3）客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p>
<p>（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。</p>
<p>（5）客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>
<p>（6）dn1、dn2、dn3 逐级应答客户端。</p>
<p>（7）客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet 会放入一个应答队列等待应答。</p>
<p>（8）当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行 3-7 步）。</p>
</blockquote>
<h3 id="网络拓扑-节点距离计算"><a class="markdownIt-Anchor" href="#网络拓扑-节点距离计算">#</a> 网络拓扑 - 节点距离计算</h3>
<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。那么这个最近距离怎么计算呢？</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和</p>
<p>第一个副本在 Client 所处的节点上。如果客户端在集群外，随机选一个</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222158187.png" alt="image-20240804222158187"></p>
<h3 id="hdfs-读流程"><a class="markdownIt-Anchor" href="#hdfs-读流程">#</a> hdfs 读流程</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222217631.png" alt="image-20240804222217631"></p>
<p>判断权限，文件是否存在</p>
<p>选择原则：节点距离，节点负载</p>
<p>串行读</p>
<blockquote>
<p>（1）客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</p>
<p>（2）挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</p>
<p>（3）DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。</p>
<p>（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</p>
</blockquote>
<h3 id="nn-2nn"><a class="markdownIt-Anchor" href="#nn-2nn">#</a> nn &amp; 2nn</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222236021.png" alt="image-20240804222236021"></p>
<p>(1) Fsimage 文件：HDFS 文件系统元数据的一个永久性的检查点其中包含 HDFS 文件系统的所有目录和文件 inode 的序列化信息。</p>
<p>Edits 文件：存放 HDFS 文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到 Edits 文件中。</p>
<p>(3) seen txid 文件保存的是一个数字，就是最后一个 edits 的数字</p>
<p>(4) 每次 NameNode 启动的时候都会将 Fsimage 文件读入内存，加载 Edits 里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成 NameNode 启动的时候就将 Fsimage 和 Edits 文件进行了合并。</p>
<p>查看镜像文件</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -p <span class="variable constant_">XML</span> -i fsimage_0000000000000000517 -o /root/image.<span class="property">xml</span></span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">03</span> <span class="number">13</span>:<span class="number">21</span>:<span class="number">15</span>,<span class="number">479</span> <span class="variable constant_">INFO</span> offlineImageViewer.<span class="property">FSImageHandler</span>: <span class="title class_">Loading</span> <span class="number">5</span> strings</span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">03</span> <span class="number">13</span>:<span class="number">21</span>:<span class="number">15</span>,<span class="number">494</span> <span class="variable constant_">INFO</span> namenode.<span class="property">FSDirectory</span>: <span class="variable constant_">GLOBAL</span> serial <span class="attr">map</span>: bits=<span class="number">29</span> maxEntries=<span class="number">536870911</span></span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">03</span> <span class="number">13</span>:<span class="number">21</span>:<span class="number">15</span>,<span class="number">494</span> <span class="variable constant_">INFO</span> namenode.<span class="property">FSDirectory</span>: <span class="variable constant_">USER</span> serial <span class="attr">map</span>: bits=<span class="number">24</span> maxEntries=<span class="number">16777215</span></span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">03</span> <span class="number">13</span>:<span class="number">21</span>:<span class="number">15</span>,<span class="number">494</span> <span class="variable constant_">INFO</span> namenode.<span class="property">FSDirectory</span>: <span class="variable constant_">GROUP</span> serial <span class="attr">map</span>: bits=<span class="number">24</span> maxEntries=<span class="number">16777215</span></span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">03</span> <span class="number">13</span>:<span class="number">21</span>:<span class="number">15</span>,<span class="number">494</span> <span class="variable constant_">INFO</span> namenode.<span class="property">FSDirectory</span>: <span class="variable constant_">XATTR</span> serial <span class="attr">map</span>: bits=<span class="number">24</span> maxEntries=<span class="number">16777215</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span><br><span class="line">&lt;fsimage&gt;<span class="language-xml"><span class="tag">&lt;<span class="name">version</span>&gt;</span><span class="tag">&lt;<span class="name">layoutVersion</span>&gt;</span>-66<span class="tag">&lt;/<span class="name">layoutVersion</span>&gt;</span><span class="tag">&lt;<span class="name">onDiskVersion</span>&gt;</span>1<span class="tag">&lt;/<span class="name">onDiskVersion</span>&gt;</span><span class="tag">&lt;<span class="name">oivRevision</span>&gt;</span>1be78238728da9266a4f88195058f08fd012bf9c<span class="tag">&lt;/<span class="name">oivRevision</span>&gt;</span><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">NameSection</span>&gt;</span><span class="tag">&lt;<span class="name">namespaceId</span>&gt;</span>345030828<span class="tag">&lt;/<span class="name">namespaceId</span>&gt;</span><span class="tag">&lt;<span class="name">genstampV1</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV1</span>&gt;</span><span class="tag">&lt;<span class="name">genstampV2</span>&gt;</span>1040<span class="tag">&lt;/<span class="name">genstampV2</span>&gt;</span><span class="tag">&lt;<span class="name">genstampV1Limit</span>&gt;</span>0<span class="tag">&lt;/<span class="name">genstampV1Limit</span>&gt;</span><span class="tag">&lt;<span class="name">lastAllocatedBlockId</span>&gt;</span>1073741864<span class="tag">&lt;/<span class="name">lastAllocatedBlockId</span>&gt;</span><span class="tag">&lt;<span class="name">txid</span>&gt;</span>517<span class="tag">&lt;/<span class="name">txid</span>&gt;</span><span class="tag">&lt;/<span class="name">NameSection</span>&gt;</span></span></span><br><span class="line">&lt;<span class="title class_">ErasureCodingSection</span>&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">erasureCodingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">policyId</span>&gt;</span>1<span class="tag">&lt;/<span class="name">policyId</span>&gt;</span><span class="tag">&lt;<span class="name">policyName</span>&gt;</span>RS-6-3-1024k<span class="tag">&lt;/<span class="name">policyName</span>&gt;</span><span class="tag">&lt;<span class="name">cellSize</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">cellSize</span>&gt;</span><span class="tag">&lt;<span class="name">policyState</span>&gt;</span>DISABLED<span class="tag">&lt;/<span class="name">policyState</span>&gt;</span><span class="tag">&lt;<span class="name">ecSchema</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">codecName</span>&gt;</span>rs<span class="tag">&lt;/<span class="name">codecName</span>&gt;</span><span class="tag">&lt;<span class="name">dataUnits</span>&gt;</span>6<span class="tag">&lt;/<span class="name">dataUnits</span>&gt;</span><span class="tag">&lt;<span class="name">parityUnits</span>&gt;</span>3<span class="tag">&lt;/<span class="name">parityUnits</span>&gt;</span><span class="tag">&lt;/<span class="name">ecSchema</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">erasureCodingPolicy</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">erasureCodingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">policyId</span>&gt;</span>2<span class="tag">&lt;/<span class="name">policyId</span>&gt;</span><span class="tag">&lt;<span class="name">policyName</span>&gt;</span>RS-3-2-1024k<span class="tag">&lt;/<span class="name">policyName</span>&gt;</span><span class="tag">&lt;<span class="name">cellSize</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">cellSize</span>&gt;</span><span class="tag">&lt;<span class="name">policyState</span>&gt;</span>DISABLED<span class="tag">&lt;/<span class="name">policyState</span>&gt;</span><span class="tag">&lt;<span class="name">ecSchema</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">codecName</span>&gt;</span>rs<span class="tag">&lt;/<span class="name">codecName</span>&gt;</span><span class="tag">&lt;<span class="name">dataUnits</span>&gt;</span>3<span class="tag">&lt;/<span class="name">dataUnits</span>&gt;</span><span class="tag">&lt;<span class="name">parityUnits</span>&gt;</span>2<span class="tag">&lt;/<span class="name">parityUnits</span>&gt;</span><span class="tag">&lt;/<span class="name">ecSchema</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">erasureCodingPolicy</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">erasureCodingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">policyId</span>&gt;</span>3<span class="tag">&lt;/<span class="name">policyId</span>&gt;</span><span class="tag">&lt;<span class="name">policyName</span>&gt;</span>RS-LEGACY-6-3-1024k<span class="tag">&lt;/<span class="name">policyName</span>&gt;</span><span class="tag">&lt;<span class="name">cellSize</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">cellSize</span>&gt;</span><span class="tag">&lt;<span class="name">policyState</span>&gt;</span>DISABLED<span class="tag">&lt;/<span class="name">policyState</span>&gt;</span><span class="tag">&lt;<span class="name">ecSchema</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">codecName</span>&gt;</span>rs-legacy<span class="tag">&lt;/<span class="name">codecName</span>&gt;</span><span class="tag">&lt;<span class="name">dataUnits</span>&gt;</span>6<span class="tag">&lt;/<span class="name">dataUnits</span>&gt;</span><span class="tag">&lt;<span class="name">parityUnits</span>&gt;</span>3<span class="tag">&lt;/<span class="name">parityUnits</span>&gt;</span><span class="tag">&lt;/<span class="name">ecSchema</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">erasureCodingPolicy</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>datanode 主动向 namenode 上报文件快信息</p>
<h3 id="查看edits日志"><a class="markdownIt-Anchor" href="#查看edits日志">#</a> 查看 edits 日志</h3>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -p <span class="variable constant_">XML</span> -i /data/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000520  -o /root/image.<span class="property">xml</span></span><br><span class="line"></span><br><span class="line">记录操作步骤，仅进行追加</span><br><span class="line"></span><br><span class="line">每进行一小时进行合并</span><br><span class="line"></span><br><span class="line">2nn没有edits_progres</span><br><span class="line"></span><br><span class="line">namenode 会把edit_Progress 后的合并</span><br><span class="line">比如后面合并<span class="number">356</span>以后的</span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222255010.png" alt="image-20240804222255010"></p>
<h3 id="checkpoint"><a class="markdownIt-Anchor" href="#checkpoint">#</a> checkpoint</h3>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The number of seconds between two periodic checkpoints. </span><br><span class="line">    Support multiple time unit suffix(case insensitive), as described</span><br><span class="line">    in dfs.heartbeat.interval.If no time unit is specified then seconds</span><br><span class="line">    is assumed.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The Secondary NameNode or CheckpointNode will create a checkpoint</span><br><span class="line">  of the namespace every &#x27;dfs.namenode.checkpoint.txns&#x27; transactions, regardless</span><br><span class="line">  of whether &#x27;dfs.namenode.checkpoint.period&#x27; has expired.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The SecondaryNameNode and CheckpointNode will poll the NameNode</span><br><span class="line">  every &#x27;dfs.namenode.checkpoint.check.period&#x27; seconds to query the number</span><br><span class="line">  of uncheckpointed transactions. Support multiple time unit suffix(case insensitive),</span><br><span class="line">  as described in dfs.heartbeat.interval.If no time unit is specified then</span><br><span class="line">  seconds is assumed.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">3600s每隔一小时执行一次</span><br><span class="line">或者每100w次执行一次，每隔60s看下有没有到100w</span><br></pre></td></tr></table></figure>
<h3 id="datanode"><a class="markdownIt-Anchor" href="#datanode">#</a> datanode</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222303971.png" alt="image-20240804222303971"></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>21600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.directoryscan.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>21600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Interval in seconds for Datanode to scan data directories and</span><br><span class="line">  reconcile the difference between blocks in memory and on the disk.</span><br><span class="line">  Support multiple time unit suffix(case insensitive), as described</span><br><span class="line">  in dfs.heartbeat.interval.If no time unit is specified then seconds</span><br><span class="line">  is assumed.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h3 id="数据完整性"><a class="markdownIt-Anchor" href="#数据完整性">#</a> 数据完整性</h3>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如下是DataNode节点保证数据完整性的方法。</span><br><span class="line">(1)当DataNode读取Block的时候,它会计算CheckSum。</span><br><span class="line">(2)如果计算后的CheckSum,与Block创建时值不一样,说明Block已经损坏。</span><br><span class="line">(3)Client读取其他DataNode上的Block。</span><br><span class="line">(4)常见的校验算法crc(32),md5(128),shal(160) </span><br><span class="line">(5)DataNode在其文件创建后周期验证CheckSum。</span><br></pre></td></tr></table></figure>
<h3 id="掉线参数"><a class="markdownIt-Anchor" href="#掉线参数">#</a> 掉线参数</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222317915.png" alt="image-20240804222317915"></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Determines datanode heartbeat interval in seconds.</span><br><span class="line">    Can use the following suffix (case insensitive):</span><br><span class="line">    ms(millis), s(sec), m(min), h(hour), d(day)</span><br><span class="line">    to specify the time (such as 2s, 2m, 1h, etc.).</span><br><span class="line">    Or provide complete number in seconds (such as 30 for 30 seconds).</span><br><span class="line">    If no time unit is specified then seconds is assumed.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    This time decides the interval to check for expired datanodes.</span><br><span class="line">    With this value and dfs.heartbeat.interval, the interval of</span><br><span class="line">    deciding the datanode is stale or not is also calculated.</span><br><span class="line">    The unit of this configuration is millisecond.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">hdfs块大小 读写速度越快，块可以配置越大。一般是128M或者256M</span><br><span class="line">shell操作</span><br><span class="line">读些流程</span><br></pre></td></tr></table></figure>
<h2 id="mapreduce"><a class="markdownIt-Anchor" href="#mapreduce">#</a> mapreduce</h2>
<p>MapReduce 是一个分布式运算程序的编程框架，是用户开发 &quot;基于 Hadoop 的数据分析应用&quot; 的核心框架。</p>
<p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>
<p>MapReduce: 自己处理业务相关代码 + 自身的默认代码</p>
<p>优点:</p>
<p>1、易于编程。用户只关心，业务逻辑。实现框架的接口。</p>
<p>2、良好扩展性：可以动态增加服务器，解决计算资源不够问题</p>
<p>3、高容错性。任何一台机器挂擦，可以将任务转移到其他节点。</p>
<p>4、适合海量数据计算 (TB/PB) 几千台服务器共同计算。</p>
<p>重阳市公安局</p>
<p>トラスメット吉祥如意</p>
<p>半夏散文章</p>
<p>Праведая праведая我姓张却长不出你爱的模样</p>
<p>天下之忧而忧无虑</p>
<p>小麦小兜到了解那样的很美</p>
<p>多少年重阳节快乐成长庚子大吉</p>
<p>缺点:</p>
<p>1、不擅长实时计算。Mysql</p>
<p>2、不擅长流式计算。Sparkstreaming flink</p>
<p>3、不擅长 DAG 有向无环图计算。spark</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222335906.png" alt="image-20240804222335906"></p>
<p>1) MapReduce 运算程序一般需要分成 2 个阶段：Map 阶段和 Reduce 阶段</p>
<p>2) Map 阶段的并发 MapTask, 完全并行运行，互不相干</p>
<p>3) Reduce 阶段的并发 ReduceTask, 完全互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出</p>
<p>4) MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行</p>
<p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程:</p>
<p>(1) MrAppMaster: 负责整个程序的过程调度及状态协调。</p>
<p>(2) MapTask: 负责 Map 阶段的整个数据处理流程。</p>
<p>(3) ReduceTask: 负责 Reduce 阶段的整个数据处理流程。</p>
<p>数据序列化类型</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222347314.png" alt="image-20240804222347314"></p>
<h3 id="mapreduce编程规范"><a class="markdownIt-Anchor" href="#mapreduce编程规范">#</a> mapreduce 编程规范</h3>
<p>1.Mapper 阶段</p>
<p>(1) 用户自定义的 Mapper 要继承自己的父类</p>
<p>(2) Mapper 的输入数据是 KV 对的形式 (KV 的类型可自定义)</p>
<p>(3) Mapper 中的业务逻辑写在 map () 方法中</p>
<p>(4) Mapper 的输出数据是 KV 对的形式 (KV 的类型可自定义)</p>
<p>(5) map () 方法 (MapTask 进程) 对每一个 &lt; K,V &gt; 调用一次</p>
<p>2.Reducer 阶段</p>
<p>(1) 用户自定义的 Reducer 要继承自己的父类</p>
<p>(2) Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV</p>
<p>(3) Reducer 的业务逻辑写在 reduce () 方法中</p>
<p>ReduceTask 进程对每一组相同 k 的 &lt;k,v&gt; 组调用一次 reduce () 方法法</p>
<p>3.Driver 阶段</p>
<p>相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群，提交的是</p>
<p>封装了 MapReduce 程序相关运行参数的 job 对象</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222405781.png" alt="image-20240804222405781"></p>
<h3 id="wordcount"><a class="markdownIt-Anchor" href="#wordcount">#</a> wordcount</h3>
<p>1.mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.wordcount2;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable , Text, Text, IntWritable&gt;&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outk</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outv</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// get a line ,text to string</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// loop write out</span></span><br><span class="line">        <span class="keyword">for</span> (String word:words)&#123;</span><br><span class="line"></span><br><span class="line">            outk.set(word);</span><br><span class="line">            context.write(outk,outv);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.wordcount2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span>  <span class="title class_">Reducer</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.wordcount2;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1 获取job</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar包路径</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联mapper和reducer</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置map输出的kv类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终输出的kV类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>package，将没有依赖的包放到 hadoop 集群执行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar MapReduceDemo-<span class="number">1.0</span>-SNAPSHOT.jar org.example.wordcount2.WordCountDriver /input /uuu</span><br></pre></td></tr></table></figure>
<p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2NDEzMDY1L2FydGljbGUvZGV0YWlscy8xMTY0MTkzMjY=">https://blog.csdn.net/m0_46413065/article/details/116419326</span></p>
<h3 id="序列化"><a class="markdownIt-Anchor" href="#序列化">#</a> 序列化</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222417067.png" alt="image-20240804222417067"></p>
<p>java 自带 serializable 很重，带了各种校验头信息。</p>
<p>hadoop 序列化：</p>
<p>紧凑：存储空间少</p>
<p>快速：传输速度快</p>
<p>互操作性:</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222431605.png" alt="image-20240804222431605"></p>
<h3 id="mapreduce-框架原理"><a class="markdownIt-Anchor" href="#mapreduce-框架原理">#</a> mapreduce 框架原理</h3>
<h4 id="inputformat数据输入"><a class="markdownIt-Anchor" href="#inputformat数据输入">#</a> inputformat 数据输入</h4>
<p>MapTask 并行度决定机制</p>
<p>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</p>
<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222447121.png" alt="image-20240804222447121"></p>
<h4 id="fileinputformat-切片机制"><a class="markdownIt-Anchor" href="#fileinputformat-切片机制">#</a> fileinputformat 切片机制</h4>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222459009.png" alt="image-20240804222459009"></p>
<p>(1) 源码中计算切片大小的公式</p>
<p>Math.max(minSize, Math.min(maxSize, blockSize));</p>
<p>mapreduce.input.fileinputformat.split.minsize=1 默认值为 1</p>
<p>mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值 Long.MAXValue</p>
<p>因此，默认情况下，切片大小 = blocksize。</p>
<p>(2) 切片大小设置</p>
<p>maxsize (切片最大值): 参数如果调得比 blockSize 小，则会让切片变小，而且就等于配置的这个参数的值。</p>
<p>minsize (切片最小值): 参数调的比 blockSize 大，则可以让切片变得比 blockSize 还大。</p>
<p>(3) 获取切片信息 API</p>
<p>// 获取切片的文件名称</p>
<p>String name = inputSplit.getPath().getNamie ();</p>
<p>// 根据文件类型获取切片信息</p>
<p>FileSplit inputSplit = (FileSplit) context.getInputsplit()</p>
<p>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValuue TextInputFormat 、NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</p>
<h4 id="textinputformat切片机制"><a class="markdownIt-Anchor" href="#textinputformat切片机制">#</a> textinputformat 切片机制</h4>
<p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，LongWritable 类型。值是这行的内容，不包括任何行终止符 (换行符和回车符),Text 类型。</p>
<h4 id="combinetextinputformat"><a class="markdownIt-Anchor" href="#combinetextinputformat">#</a> combinetextinputformat</h4>
<p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask, 这样如果有大量小文件，就会产生大量的 MapTask, 处理效率极其低下。</p>
<p>1) 应用场景:</p>
<p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个 / 小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。T</p>
<p>2) 虚拟存储切片最大值设置 (</p>
<p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</p>
<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p>3) 切片机制～</p>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p>切片过程</p>
<p>(a) 判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。</p>
<p>(b) 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 5 设置最终输出的kV类型</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job,<span class="number">4194034</span>);</span><br><span class="line"><span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br></pre></td></tr></table></figure>
<h3 id="mapreduce工作流程"><a class="markdownIt-Anchor" href="#mapreduce工作流程">#</a> mapreduce 工作流程</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222514569.png" alt="image-20240804222514569"></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222529350.png" alt="image-20240804222529350"></p>
<h3 id="shuffle"><a class="markdownIt-Anchor" href="#shuffle">#</a> shuffle</h3>
<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</p>
<p>可以进行排序压缩等操作</p>
<p>maptask 阶段：</p>
<p>对 key 的索引按照字典序快排</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222546927.png" alt="image-20240804222546927"></p>
<h3 id="partition分区"><a class="markdownIt-Anchor" href="#partition分区">#</a> partition 分区</h3>
<p>要求将统计结果按照条件输出到不同文件中 (分区)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        </span><br><span class="line">        job.setNumReduceTasks(<span class="number">2</span>);</span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; extendss Partitioner&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> 1humReduceTasks)</span></span><br><span class="line">         <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE)៖ numReduceTasks;</span><br></pre></td></tr></table></figure>
<p>如果分区 &gt; 1 才有 hash</p>
<p>否则直接 partition-1 = 0 只有 0 号分区</p>
<p>默认分区是根据 key 的 hashCode 对 ReduceTasks 个数取模得到的。用户没法控制哪个 key 存储到哪个分区。</p>
<p>自定义 partitoner</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222606892.png" alt="image-20240804222606892"></p>
<h3 id="排序"><a class="markdownIt-Anchor" href="#排序">#</a> 排序</h3>
<p>对于 MapTask, 它会将处理的结果暂时放到环形缓冲区中，当环不形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有有文件进行归并排序。</p>
<p>对于 ReduceTask, 它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内字中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask 统一对内存和磁盘上的所有数据进行一次师日并排序</p>
<h2 id="yarn"><a class="markdownIt-Anchor" href="#yarn">#</a> yarn</h2>
<p>Yam 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。</p>
<p>YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件构成。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222630987.png" alt="image-20240804222630987"></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222643801.png" alt="image-20240804222643801"></p>
<h3 id="yarn调度器"><a class="markdownIt-Anchor" href="#yarn调度器">#</a> yarn 调度器</h3>
<p>目前，Hadoop 作业调度器主要有三种：FIFO、容量 (CapacityScheduler) 和公平 (Fair Scheduler)。Apache Hadoop3.1.3 默认的资源调度器是 Capacity SScheduler</p>
<h4 id="fifo调度器"><a class="markdownIt-Anchor" href="#fifo调度器">#</a> FIFO 调度器</h4>
<p>(FirstInFirstOut) 单队列，根据提交作业的先后顺序，先来先服务</p>
<h4 id="容量调度器"><a class="markdownIt-Anchor" href="#容量调度器">#</a> 容量调度器</h4>
<p>1、多队列：每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略。</p>
<p>2、容量保证：管理员可为每个队列设置资源最低保证和资源使用上限</p>
<p>3、灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。</p>
<p>4、多租户：支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会又对同一用户提交的作业所占资源量进行限定。</p>
<p>1) 队列资源分配</p>
<p>从 root 开始，使用深度优先算法，优先选择资源占用率最低的队列分配资源。</p>
<p>2) 作业资源分配</p>
<p>默认按照提交作业的优先级和提交时间顺序分配资源。</p>
<p>3) 容器资源分配</p>
<p>按照容器的优先级分配资源；如果优先级相同，按照数据本地性原则:</p>
<p>(1) 任务和数据在同一节点</p>
<p>(2) 任务和数据在同一机架</p>
<p>(3) 任务和数据不在同一节点也不在同一机架</p>
<h4 id="公平调度器"><a class="markdownIt-Anchor" href="#公平调度器">#</a> 公平调度器</h4>
<p>同队列所有任务共享资源，在时间尺度上获得公平的资源</p>
<p>1) 与容量调度器相同点</p>
<p>(1) 多队列：支持多队列多作业</p>
<p>(2) 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线</p>
<p>(3) 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。</p>
<p>(4) 多租户：支持多用户共享集群和多应用程序同时运行；为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。</p>
<p>2) 与容量调度器不同点</p>
<p>(1) 核心调度策略不同</p>
<p>容量调度器：优先选择资源利用率低的队列</p>
<p>公平调度器：优先选择对资源的缺额比例大的</p>
<p>(2) 每个队列可以单独设置资源分配方式</p>
<p>容量调度器：FIFO，DRF</p>
<p>公平调度器：FIFO，DRF，FAIR</p>
<p>公平调度器设计目标是：在时间尺度上，所有作业获得公平的的资源。某一时刻一个作业应获资源和实际获取资源的差距叫 &quot;缺额&quot;</p>
<p>调度器会优先为缺额大的作业分配资源</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222659518.png" alt="image-20240804222659518"></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222717084.png" alt="image-20240804222717084"></p>
<h4 id="drf"><a class="markdownIt-Anchor" href="#drf">#</a> DRF</h4>
<p>DRF (DominantResource Fairness), 我们之前说的资源，都那是单一标准，例如只考虑内存 (也是 Yam 默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU, 网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。</p>
<p>那么在 YARN 中，我们用 DRF 来决定如何调度:</p>
<p>假设集群一共有 100CPU 和 10T 内存，而应用 A 需要 (2CPU,300GB), 应用 B 需要 (6CPU,100GB)。则两个应用分别需要 A (2% CPU,3% 内存) 和 B (6% CPU,1% 为存) 的资源，这就意味着 A 是内存主导的，B 是 CPU 主导的，针对这种情况，我们可以选择 DRF 策略对不同应用进行不同资源 (CPU 和内存) 的一个不同比例的限制。</p>
<h3 id="yarn-shell"><a class="markdownIt-Anchor" href="#yarn-shell">#</a> yarn shell</h3>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">root<span class="meta">@hadoop100</span>:~# yarn application -list </span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">11</span> <span class="number">06</span>:<span class="number">34</span>:<span class="number">10</span>,<span class="number">406</span> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop101/<span class="number">192.168</span><span class="number">.13</span><span class="number">.191</span>:<span class="number">8032</span></span><br><span class="line">Total number of <span class="title function_">applications</span> <span class="params">(application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: [])</span>:<span class="number">1</span></span><br><span class="line">                Application-Id      Application-Name        Application-Type          User           Queue                   State           Final-State             Progress                        Tracking-URL</span><br><span class="line">application_1717055365946_0004            word count               MAPREDUCE          root         <span class="keyword">default</span>                 RUNNING             UNDEFINED                   <span class="number">0</span>%              http:<span class="comment">//hadoop100:38735</span></span><br><span class="line">root<span class="meta">@hadoop100</span>:~# yarn application -list </span><br><span class="line"><span class="number">2024</span>-<span class="number">06</span>-<span class="number">11</span> <span class="number">06</span>:<span class="number">34</span>:<span class="number">10</span>,<span class="number">406</span> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop101/<span class="number">192.168</span><span class="number">.13</span><span class="number">.191</span>:<span class="number">8032</span></span><br><span class="line">Total number of <span class="title function_">applications</span> <span class="params">(application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: [])</span>:<span class="number">1</span></span><br><span class="line">                Application-Id      Application-Name        Application-Type          User           Queue                   State           Final-State             Progress                        Tracking-URL</span><br><span class="line">application_1717055365946_0004            word count               MAPREDUCE          root         <span class="keyword">default</span>                 RUNNING             UNDEFINED                   <span class="number">0</span>%              http:<span class="comment">//hadoop100:38735</span></span><br><span class="line">root<span class="meta">@hadoop100</span>:~# yarn application -kill application_1717055365946_0004  </span><br><span class="line">root<span class="meta">@hadoop100</span>:~# yarn logs -applicationId application_1717055365946_0004  </span><br><span class="line">root<span class="meta">@hadoop100</span>:~# yarn logs -applicationId application_1717055365946_0004  -containerId </span><br><span class="line"># 查看尝试运行的任务</span><br><span class="line">yarn applicationattempt -list application_1717055365946_0004 </span><br><span class="line"># </span><br><span class="line">yarn applicationattempt -status application_1717055365946_0004 </span><br><span class="line"># yarn 容器状态，只有在任务运行中可以查看</span><br><span class="line">yarn container -list </span><br><span class="line">yarn container -status</span><br><span class="line">yarn node -list -all</span><br><span class="line"># 加载队列配置</span><br><span class="line">yarn rmadmin -refreshQueues</span><br><span class="line"># 查看队列</span><br><span class="line">yarn queue -status <span class="keyword">default</span> </span><br></pre></td></tr></table></figure>
<h3 id="yarn生产环境参数"><a class="markdownIt-Anchor" href="#yarn生产环境参数">#</a> yarn 生产环境参数</h3>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222733648.png" alt="image-20240804222733648"></p>
<h3 id="多队列"><a class="markdownIt-Anchor" href="#多队列">#</a> 多队列</h3>
<p>1) 在生产环境怎么创建队列？</p>
<p>(1) 调度器默认就 1 个 default 队列，不能满足生产要求。</p>
<p>(2) 按照框架:hive/spark/flink 每个框架的任务放入指定的队列 (企业用的不是特别多)</p>
<p>(3) 按照业务模块：登录注册、购物车、下单、业务部门 1、业务部门 2</p>
<p>2) 创建多队列的好处</p>
<p>(1) 因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。</p>
<p>(2) 实现任务的降级使用，特殊时期保证重要的任务队列资原充足。</p>
<p>业务部门 1 (重要)=》业务部门 2 (比较重要)=》下单 (一般)=》购物车 (一般)=》登录注册 (次要)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">  vim capacity-scheduler.xml </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="keyword">default</span>,hive&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The queues at the <span class="built_in">this</span> <span class="title function_">level</span> <span class="params">(root is the root queue)</span>.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">40</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">60</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Default queue target capacity.&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.user-limit-factor&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">0.7</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      Default queue user limit a percentage from <span class="number">0.0</span> to <span class="number">1.0</span>.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">1</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      Default queue user limit a percentage from <span class="number">0.0</span> to <span class="number">1.0</span>.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.maximum-capacity&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">60</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The maximum capacity of the <span class="keyword">default</span> queue.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.state&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The state of the <span class="keyword">default</span> queue. State can be one of RUNNING or STOPPED.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.acl_submit_applications&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The ACL of who can submit jobs to the <span class="keyword">default</span> queue.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.<span class="keyword">default</span>.acl_administer_queue&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The ACL of who can administer jobs on the <span class="keyword">default</span> queue.</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetimeTimeout</span><br><span class="line">参考资料：https:<span class="comment">//blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;</span></span><br><span class="line"> </span><br><span class="line">&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 </span><br><span class="line">--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-application-lifetime&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-<span class="number">1</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;!-- 如果application没指定超时时间，则用<span class="keyword">default</span>-application-lifetime作为默认值 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.hive.<span class="keyword">default</span>-application-lifetime&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-<span class="number">1</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"># 提交任务到不同队列</span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-example-<span class="number">3.1</span><span class="number">.3</span>.jar wordcount -D mapreduce.job.queuename=hive /input /outputttt</span><br><span class="line"></span><br><span class="line"># 或者在driver中</span><br><span class="line"><span class="type">Configure</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configure</span>();</span><br><span class="line">conf.set(<span class="string">&quot;mapreduce.job.queuename&quot;</span>,<span class="string">&quot;hive&quot;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="任务优先级"><a class="markdownIt-Anchor" href="#任务优先级">#</a> 任务优先级</h3>
<p>容量调度器，支持任务优先级的配置，在资源紧张时，优先吸高的任务将优先获取资源。默认情况，Yarn 将所有任务的优先级限制为 0，若想使用任务的优先级功能，须开放该限制。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">5</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">3.1</span><span class="number">.3</span>.jar pi -D mapreduce.job.priorit=<span class="number">5</span> <span class="number">5</span> <span class="number">20000</span></span><br><span class="line"></span><br><span class="line"># 动态设置优先级</span><br><span class="line">yarn application -appId xxx -upgradePriority <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h3 id="配置多队列公平调度器"><a class="markdownIt-Anchor" href="#配置多队列公平调度器">#</a> 配置多队列公平调度器</h3>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;配置使用公平调度器&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/<span class="keyword">module</span>/hadoop-<span class="number">3.1</span><span class="number">.3</span>/etc/hadoop/fair-scheduler.xml&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;指明公平调度器队列分配配置文件&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;禁止队列间资源抢占&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">vim fair-scheduler.xml</span><br><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span><br><span class="line">&lt;allocations&gt;</span><br><span class="line">  &lt;!-- 单个队列中Application Master占用资源的最大比例,取值<span class="number">0</span>-<span class="number">1</span> ，企业一般配置<span class="number">0.1</span> --&gt;</span><br><span class="line">  &lt;queueMaxAMShareDefault&gt;<span class="number">0.5</span>&lt;/queueMaxAMShareDefault&gt;</span><br><span class="line">  &lt;!-- 单个队列最大资源的默认值 test atguigu <span class="keyword">default</span> --&gt;</span><br><span class="line">  &lt;queueMaxResourcesDefault&gt;4096mb,4vcores&lt;/queueMaxResourcesDefault&gt;</span><br><span class="line"> </span><br><span class="line">  &lt;!-- 增加一个队列test --&gt;</span><br><span class="line">  &lt;queue name=<span class="string">&quot;test&quot;</span>&gt;</span><br><span class="line">    &lt;!-- 队列最小资源 --&gt;</span><br><span class="line">    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;</span><br><span class="line">    &lt;!-- 队列最大资源 --&gt;</span><br><span class="line">    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;</span><br><span class="line">    &lt;!-- 队列中最多同时运行的应用数，默认<span class="number">50</span>，根据线程数配置 --&gt;</span><br><span class="line">    &lt;maxRunningApps&gt;<span class="number">4</span>&lt;/maxRunningApps&gt;</span><br><span class="line">    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span><br><span class="line">    &lt;!-- &lt;maxAMShare&gt;<span class="number">0.5</span>&lt;/maxAMShare&gt;--&gt;</span><br><span class="line">    &lt;!-- 该队列资源权重,默认值为<span class="number">1.0</span> --&gt;</span><br><span class="line">    &lt;weight&gt;<span class="number">1.0</span>&lt;/weight&gt;</span><br><span class="line">    &lt;!-- 队列内部的资源分配策略 --&gt;</span><br><span class="line">    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;</span><br><span class="line">  &lt;/queue&gt;</span><br><span class="line">  &lt;!-- 增加一个队列atguigu --&gt;</span><br><span class="line">  &lt;queue name=<span class="string">&quot;atguigu&quot;</span> type=<span class="string">&quot;parent&quot;</span>&gt;</span><br><span class="line">    &lt;!-- 队列最小资源 --&gt;</span><br><span class="line">    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;</span><br><span class="line">    &lt;!-- 队列最大资源 --&gt;</span><br><span class="line">    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;</span><br><span class="line">    &lt;!-- 队列中最多同时运行的应用数，默认<span class="number">50</span>，根据线程数配置 --&gt;</span><br><span class="line">    &lt;maxRunningApps&gt;<span class="number">4</span>&lt;/maxRunningApps&gt;</span><br><span class="line">    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span><br><span class="line">    &lt;!-- &lt;maxAMShare&gt;<span class="number">0.5</span>&lt;/maxAMShare&gt;--&gt;</span><br><span class="line">    &lt;!-- 该队列资源权重,默认值为<span class="number">1.0</span> --&gt;</span><br><span class="line">    &lt;weight&gt;<span class="number">1.0</span>&lt;/weight&gt;</span><br><span class="line">    &lt;!-- 队列内部的资源分配策略 --&gt;</span><br><span class="line">    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;</span><br><span class="line">  &lt;/queue&gt;</span><br><span class="line"> </span><br><span class="line">  &lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;</span><br><span class="line">  &lt;queuePlacementPolicy&gt;</span><br><span class="line">    &lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; <span class="literal">false</span>表示：如果指定队列不存在,不允许自动创建--&gt;</span><br><span class="line">    &lt;rule name=<span class="string">&quot;specified&quot;</span> create=<span class="string">&quot;false&quot;</span>/&gt;</span><br><span class="line">    &lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;</span><br><span class="line">    &lt;rule name=<span class="string">&quot;nestedUserQueue&quot;</span> create=<span class="string">&quot;true&quot;</span>&gt;</span><br><span class="line">        &lt;rule name=<span class="string">&quot;primaryGroup&quot;</span> create=<span class="string">&quot;false&quot;</span>/&gt;</span><br><span class="line">    &lt;/rule&gt;</span><br><span class="line">    &lt;!-- 最后一个规则必须为reject或者<span class="keyword">default</span>。Reject表示拒绝创建提交失败，<span class="keyword">default</span>表示把任务提交到<span class="keyword">default</span>队列 --&gt;</span><br><span class="line">    &lt;rule name=<span class="string">&quot;reject&quot;</span> /&gt;</span><br><span class="line">  &lt;/queuePlacementPolicy&gt;</span><br><span class="line">&lt;/allocations&gt;</span><br><span class="line"># 指定用户和不指定用户</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">3.1</span><span class="number">.3</span>.jar pi -D mapreduce.job.queuename=root.test <span class="number">1</span> <span class="number">1</span> </span><br><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">3.1</span><span class="number">.3</span>.jar pi <span class="number">1</span> <span class="number">1</span> </span><br></pre></td></tr></table></figure>
<h3 id="tool接口"><a class="markdownIt-Anchor" href="#tool接口">#</a> tool 接口</h3>
<p>期望可以动态传参，结果报错，误认为是第一个输入参数。</p>
<p>[@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1</p>
<p>1）需求：自己写的程序也可以动态修改参数。编写 Yarn 的 Tool 接口。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">创建类WordCount并实现Tool接口：</span><br><span class="line"><span class="keyword">package</span> com.atguigu.yarn;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> <span class="keyword">implements</span> <span class="title class_">Tool</span> &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> Configuration conf;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"> </span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"> </span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"> </span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"> </span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"> </span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setConf</span><span class="params">(Configuration conf)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.conf = conf;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Configuration <span class="title function_">getConf</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> conf;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"> </span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">            String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                outK.set(word);</span><br><span class="line"> </span><br><span class="line">                context.write(outK, outV);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"> </span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">                sum += value.get();</span><br><span class="line">            &#125;</span><br><span class="line">            outV.set(sum);</span><br><span class="line"> </span><br><span class="line">            context.write(key, outV);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804222803108.png" alt="image-20240804222803108"></p>

      <div class="tags">
          <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag"><i class="ic i-tag"></i> 大数据</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">Edited on</span>
    <time title="Modified: 2024-08-04 23:28:53" itemprop="dateModified" datetime="2024-08-04T23:28:53+08:00">2024-08-04</time>
  </span>
</div>

      
<div class="reward">
  <button><i class="ic i-heartbeat"></i> Donate</button>
  <p>Give me a cup of [coffee]~(￣▽￣)~*</p>
  <div id="qr">
      
      <div>
        <img data-src="/images/wechatpay.png" alt="John Doe WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div>
        <img data-src="/images/alipay.png" alt="John Doe Alipay">
        <p>Alipay</p>
      </div>
      
      <div>
        <img data-src="/images/paypal.png" alt="John Doe PayPal">
        <p>PayPal</p>
      </div>
  </div>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>Post author:  </strong>John Doe <i class="ic i-at"><em>@</em></i>Hexo
  </li>
  <li class="link">
    <strong>Post link: </strong>
    <a href="http://example.com/2024/03/01/hadoop_new/" title="Hadoop">http://example.com/2024/03/01/hadoop_new/</a>
  </li>
  <li class="license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/2024/02/15/Elastic%20Stack/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;img.timelessq.com&#x2F;images&#x2F;2022&#x2F;07&#x2F;26&#x2F;767d892afd9db5bda9fd978ea0a7cb0b.jpg" title="Elastic stack">
  <span class="type">Previous Post</span>
  <span class="category"><i class="ic i-flag"></i> 运维</span>
  <h3>Elastic stack</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="/2024/04/01/spark/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;img.timelessq.com&#x2F;images&#x2F;2022&#x2F;07&#x2F;26&#x2F;8cf4932745804d12b330608b12a53aca.jpg" title="spark">
  <span class="type">Next Post</span>
  <span class="category"><i class="ic i-flag"></i> 大数据</span>
  <h3>spark</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="Contents">
          <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop"><span class="toc-number">1.</span> <span class="toc-text"> hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#hadoop-%E7%89%88%E6%9C%AC%E6%BC%94%E8%BF%9B"><span class="toc-number">1.0.1.</span> <span class="toc-text"> hadoop 版本演进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%84%E4%BB%B6"><span class="toc-number">1.0.2.</span> <span class="toc-text"> 组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2hadoop"><span class="toc-number">1.0.3.</span> <span class="toc-text"> 部署 hadoop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-number">1.0.4.</span> <span class="toc-text"> 完全分布式部署：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-number">1.0.5.</span> <span class="toc-text"> 故障处理：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%B0%E5%BD%95%E5%8E%86%E5%8F%B2%E8%BF%90%E8%A1%8C%E6%83%85%E5%86%B5"><span class="toc-number">1.0.6.</span> <span class="toc-text"> 记录历史运行情况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E6%B1%87%E8%81%9A"><span class="toc-number">1.0.7.</span> <span class="toc-text"> 日志汇聚</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="toc-number">1.0.8.</span> <span class="toc-text"> 常用端口号</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hdfs"><span class="toc-number">2.</span> <span class="toc-text"> hdfs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-shell"><span class="toc-number">2.1.</span> <span class="toc-text"> hdfs shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop-api"><span class="toc-number">2.2.</span> <span class="toc-text"> hadoop api</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-%E5%86%99%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text"> hdfs 写流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">2.4.</span> <span class="toc-text"> 网络拓扑 - 节点距离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-%E8%AF%BB%E6%B5%81%E7%A8%8B"><span class="toc-number">2.5.</span> <span class="toc-text"> hdfs 读流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-2nn"><span class="toc-number">2.6.</span> <span class="toc-text"> nn &amp; 2nn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8Bedits%E6%97%A5%E5%BF%97"><span class="toc-number">2.7.</span> <span class="toc-text"> 查看 edits 日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checkpoint"><span class="toc-number">2.8.</span> <span class="toc-text"> checkpoint</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#datanode"><span class="toc-number">2.9.</span> <span class="toc-text"> datanode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="toc-number">2.10.</span> <span class="toc-text"> 数据完整性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%89%E7%BA%BF%E5%8F%82%E6%95%B0"><span class="toc-number">2.11.</span> <span class="toc-text"> 掉线参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mapreduce"><span class="toc-number">3.</span> <span class="toc-text"> mapreduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#mapreduce%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83"><span class="toc-number">3.1.</span> <span class="toc-text"> mapreduce 编程规范</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wordcount"><span class="toc-number">3.2.</span> <span class="toc-text"> wordcount</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text"> 序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapreduce-%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86"><span class="toc-number">3.4.</span> <span class="toc-text"> mapreduce 框架原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#inputformat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5"><span class="toc-number">3.4.1.</span> <span class="toc-text"> inputformat 数据输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#fileinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-number">3.4.2.</span> <span class="toc-text"> fileinputformat 切片机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#textinputformat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-number">3.4.3.</span> <span class="toc-text"> textinputformat 切片机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#combinetextinputformat"><span class="toc-number">3.4.4.</span> <span class="toc-text"> combinetextinputformat</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapreduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.5.</span> <span class="toc-text"> mapreduce 工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shuffle"><span class="toc-number">3.6.</span> <span class="toc-text"> shuffle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#partition%E5%88%86%E5%8C%BA"><span class="toc-number">3.7.</span> <span class="toc-text"> partition 分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F"><span class="toc-number">3.8.</span> <span class="toc-text"> 排序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yarn"><span class="toc-number">4.</span> <span class="toc-text"> yarn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">4.1.</span> <span class="toc-text"> yarn 调度器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#fifo%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">4.1.1.</span> <span class="toc-text"> FIFO 调度器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">4.1.2.</span> <span class="toc-text"> 容量调度器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">4.1.3.</span> <span class="toc-text"> 公平调度器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#drf"><span class="toc-number">4.1.4.</span> <span class="toc-text"> DRF</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn-shell"><span class="toc-number">4.2.</span> <span class="toc-text"> yarn shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%8F%82%E6%95%B0"><span class="toc-number">4.3.</span> <span class="toc-text"> yarn 生产环境参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%98%9F%E5%88%97"><span class="toc-number">4.4.</span> <span class="toc-text"> 多队列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BC%98%E5%85%88%E7%BA%A7"><span class="toc-number">4.5.</span> <span class="toc-text"> 任务优先级</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%A4%9A%E9%98%9F%E5%88%97%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">4.6.</span> <span class="toc-text"> 配置多队列公平调度器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tool%E6%8E%A5%E5%8F%A3"><span class="toc-number">4.7.</span> <span class="toc-text"> tool 接口</span></a></li></ol></li></ol>
      </div>
      <div class="related panel pjax" data-title="Related">
        <ul>
          <li class="active"><a href="/2024/03/01/hadoop_new/" rel="bookmark" title="Hadoop">Hadoop</a></li><li><a href="/2024/04/01/spark/" rel="bookmark" title="spark">spark</a></li><li><a href="/2024/05/01/hive_flink/" rel="bookmark" title="hive_flink">hive_flink</a></li><li><a href="/2024/10/01/clickhouse/" rel="bookmark" title="Clickhouse">Clickhouse</a></li><li><a href="/2024/11/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E6%A6%82%E8%AE%BA/" rel="bookmark" title="大数据生态概论">大数据生态概论</a></li>
        </ul>
      </div>
      <div class="overview panel" data-title="Overview">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="John Doe"
      data-src="/images/avatar.jpg">
  <p class="name" itemprop="name">John Doe</p>
  <div class="description" itemprop="description"></div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">54</span>
        <span class="name">posts</span>
      </a>
    </div>
    <div class="item categories">
      <a href="/categories/">
        <span class="count">7</span>
        <span class="name">categories</span>
      </a>
    </div>
    <div class="item tags">
      <a href="/tags/">
        <span class="count">7</span>
        <span class="name">tags</span>
      </a>
    </div>
</nav>

<div class="social">
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>Home</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="/2024/02/15/Elastic%20Stack/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="/2024/04/01/spark/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>Random Posts</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E8%BF%90%E7%BB%B4/" title="In 运维">运维</a>
</div>

    <span><a href="/2024/02/15/Elastic%20Stack/" title="Elastic stack">Elastic stack</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E9%80%86%E5%90%91/" title="In 逆向">逆向</a>
</div>

    <span><a href="/2023/08/15/%E7%A0%B4%E8%A7%A3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E8%AE%A4%E8%AF%86%E5%A3%B3%E4%B8%8E%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%89%B9%E5%BE%81/" title="破解基础知识之认识壳与程序的特征">破解基础知识之认识壳与程序的特征</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/web%E5%AE%89%E5%85%A8/" title="In web安全">web安全</a>
</div>

    <span><a href="/2022/05/17/include/" title="file include">file include</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E8%BF%90%E7%BB%B4/" title="In 运维">运维</a>
</div>

    <span><a href="/2023/05/20/Kubernetes/" title="Kubernetes">Kubernetes</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2023/01/01/%E5%8F%91%E7%8E%B0%E5%B0%8F%E5%8F%AF%E7%88%B1%EF%BC%81/" title="about me">about me</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E9%80%86%E5%90%91/" title="In 逆向">逆向</a>
</div>

    <span><a href="/2023/08/22/%E5%8E%BB%E5%BC%B9%E7%AA%97%E7%BB%BF%E5%8C%96/" title="破解相关">破解相关</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E8%BF%90%E7%BB%B4/" title="In 运维">运维</a>
</div>

    <span><a href="/2024/07/01/GPU%E5%85%A5%E9%97%A8%E6%A6%82%E8%AE%BA/" title="GPU入门概论">GPU入门概论</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/web%E5%AE%89%E5%85%A8/" title="In web安全">web安全</a>
</div>

    <span><a href="/2022/11/17/JWT%20%E5%9F%BA%E7%A1%80/" title="JWT">JWT</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" title="In 大数据">大数据</a>
</div>

    <span><a href="/2024/05/01/hive_flink/" title="hive_flink">hive_flink</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2023/08/02/Capabilities/" title="docker底层原理+capability">docker底层原理+capability</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>Recent Comments</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">John Doe @ Yume Shoka</span>
  </div>
  <div class="powered-by">
    Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2024/03/01/hadoop_new/',
    favicon: {
      show: "（●´3｀●）Goooood",
      hide: "(´Д｀)Booooom"
    },
    search : {
      placeholder: "Search for Posts",
      empty: "We didn't find any results for the search: ${query}",
      stats: "${hits} results found in ${time} ms"
    },
    valine: true,fancybox: true,
    copyright: 'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




</body>
</html>
