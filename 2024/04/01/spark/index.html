



<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="Hexo" href="http://example.com/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="Hexo" href="http://example.com/atom.xml" />
<link rel="alternate" type="application/json" title="Hexo" href="http://example.com/feed.json" />

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  
  <meta name="keywords" content="大数据" />


<link rel="canonical" href="http://example.com/2024/04/01/spark/">



  <title>
spark - 大数据 |
Yume Shoka = Hexo</title>
<meta name="generator" content="Hexo 5.4.2"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">spark
  </h1>
  
<div class="meta">
  <span class="item" title="Created: 2024-04-01 13:38:45">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">Posted on</span>
    <time itemprop="dateCreated datePublished" datetime="2024-04-01T13:38:45+08:00">2024-04-01</time>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="Toggle navigation bar">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">Yume Shoka</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/31105e606c74614c7f8f7b1d08e1619a.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/2e0802e6e5cfd05c700bc3f911f9351c.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/536315e4defca264dab1b39dc99bb2dc.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/d4df076eb51491f02d9c9754567256fb.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/a1f3404a5032323ea4857ac5a6354d2f.jpg"></li>
          <li class="item" data-background-image="https://img.timelessq.com/images/2022/07/26/560306a4274d1fc26eb475dd52237387.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">Home</a></span><i class="ic i-angle-right"></i>
<span  class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="item" rel="index" title="In 大数据"><span itemprop="name">大数据</span></a>
<meta itemprop="position" content="1" /></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="en">
  <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/01/spark/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/avatar.jpg">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content=", ">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <h2 id="spark"><a class="markdownIt-Anchor" href="#spark">#</a> spark</h2>
<p>分布式计算引擎框架，基于 mapreduce 开发</p>
<p>单机：单进程，单节点</p>
<p>伪分布式：多进程，单节点</p>
<p>分布式：多进程，多节点</p>
<p>分布式计算核心：切分数据，减少数据规模</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223237028.png" alt="image-20240804223237028"></p>
<p>spark 分布式集群采用集群中心化</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223253379.png" alt="image-20240804223253379"></p>
<p>框架：不完整的计算机程序 (核心功能已经开发完毕，但是是和业务相关的代码未开发)(MR，spark)</p>
<p>系统：完整的计算机程序 (HDFS,Kafka)</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223306758.png" alt="image-20240804223306758"></p>
<p>引擎：核心功能</p>
<p>spark 基于 mr 开发，两者区别</p>
<p>1. 开发语言：mr：java，不适合进行大量数据处理。spark：scala，适合大量数据处理，封装大量功能</p>
<p>2. 处理方式：hadoop 出现的早，只考虑单一的计算操作</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223319497.png" alt="image-20240804223319497"></p>
<p>spark 优化了计算过程</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223331386.png" alt="image-20240804223331386"></p>
<p>回顾：Hadoop 主要解决，海量数据的存储和海量数据的分析计算。</p>
<p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223347611.png" alt="image-20240804223347611"></p>
<p>spark 内置模块</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223358385.png" alt="image-20240804223358385"></p>
<h3 id="部署spark集群"><a class="markdownIt-Anchor" href="#部署spark集群">#</a> 部署 spark 集群</h3>
<p>部署 Spark 其实指的就是 Spark 的程序逻辑在什么资源中执行</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223413816.png" alt="image-20240804223413816"></p>
<p>如果资源是当前单节点提供的，那么就称之为单机模式</p>
<p>如果资源是当前多节点提供的，那么就称之为分布式模式</p>
<p>如果资源是由 Yarn 提供的，那么就称之为 Yarn 部署环境</p>
<p>如果资源是由 Spark 提供的，那么就称之为 Spark 部署环境 (Standalone</p>
<p>生产环境中主要采用：yarn+spark  也称之为（spark on yarna）</p>
<p>(1) Local 模式：在本地部署单个 Spark 服务</p>
<p>(2) Standalone 模式：Spark 自带的任务调度模式。(国内不常用)</p>
<p>(3) YARN 模式：Spark 使用 Hadoop 的 YARN 组件进行资源与任务调度。(国内最常用)</p>
<p>(4) Mesos 模式：Spark 使用 Mesos 平台进行资源与任务的调度。(国内很少用)</p>
<h4 id="部署local"><a class="markdownIt-Anchor" href="#部署local">#</a> 部署 local</h4>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz</span></span><br><span class="line">tar xzvf spark-<span class="number">3.4</span><span class="number">.3</span>-bin-hadoop3.tgz</span><br><span class="line"></span><br><span class="line">bin/spark-submit --<span class="keyword">class</span> <span class="title class_">org</span>.apache.spark.examples.SparkPi --master local[<span class="number">2</span>] ./examples/jars/spark-examples_2<span class="number">.12</span>-<span class="number">3.4</span><span class="number">.3</span>.jar <span class="number">10</span></span><br><span class="line"></span><br><span class="line">--master 指定资源提供者</span><br><span class="line">local 单线程</span><br><span class="line">local[<span class="number">2</span>] 两个线程执行</span><br><span class="line">local[*] 使用全部核</span><br><span class="line"></span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO DAGScheduler: Job <span class="number">0</span> finished: reduce at SparkPi.scala:<span class="number">38</span>, took <span class="number">1.055199</span> s</span><br><span class="line">Pi is roughly <span class="number">3.1425071142507113</span></span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO SparkContext: SparkContext is stopping with exitCode <span class="number">0.</span></span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO SparkUI: Stopped Spark web UI at http:<span class="comment">//hadoop100:4040</span></span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO MemoryStore: MemoryStore cleared</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO BlockManager: BlockManager stopped</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">12</span> 08:09:<span class="number">06</span> INFO ShutdownHookManager: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223435814.png" alt="image-20240804223435814"></p>
<p>Spark 在运行时，会启动进程，申请资源，执行计算，但是一旦计算完毕，那么进程会停止，资源会释放掉</p>
<p>Stopped Spark web UI at <span class="exturl" data-url="aHR0cDovL2hhZG9vcDEwMDo0MDQw">http://hadoop100:4040</span></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223450585.png" alt="image-20240804223450585"></p>
<h4 id="yarn模式"><a class="markdownIt-Anchor" href="#yarn模式">#</a> yarn 模式</h4>
<p>编辑启动关闭脚本</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo <span class="string">&quot;========hadoop100==========&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop100</span> <span class="string">&quot;/root/jdk8u352-b08/bin/jps&quot;</span></span><br><span class="line">echo <span class="string">&quot;========hadoop101==========&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop101</span> <span class="string">&quot;jps&quot;</span></span><br><span class="line">echo <span class="string">&quot;========hadoop102==========&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop102</span> <span class="string">&quot;jps&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">ssh root<span class="meta">@hadoop100</span> <span class="string">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/start-dfs.sh&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop101</span> <span class="string">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/start-yarn.sh&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop100</span> <span class="string">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/bin/mapred --daemon start historyserver&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">ssh root<span class="meta">@hadoop100</span> <span class="string">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/bin/mapred --daemon stop historyserver&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop101</span> <span class="string">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/stop-yarn.sh&quot;</span></span><br><span class="line">ssh root<span class="meta">@hadoop100</span> <span class="string">&quot;source /root/admin-openrc;/root/hadoop-3.3.6/sbin/stop-dfs.sh&quot;</span></span><br><span class="line">vim spark-env.sh</span><br><span class="line">YARN_CONF_DIR=/root/hadoop-<span class="number">3.3</span><span class="number">.6</span>/etc/hadoop/</span><br><span class="line"></span><br><span class="line">hadoop-start.sh</span><br><span class="line"></span><br><span class="line">bin/spark-submit --<span class="keyword">class</span> <span class="title class_">org</span>.apache.spark.examples.SparkPi --master yarn ./examples/jars/spark-examples_2<span class="number">.12</span>-<span class="number">3.4</span><span class="number">.3</span>.jar <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root<span class="meta">@hadoop100</span>:~# jpsall </span><br><span class="line">========hadoop100==========</span><br><span class="line"><span class="number">42388</span> DataNode</span><br><span class="line"><span class="number">42683</span> NodeManager</span><br><span class="line"><span class="number">43820</span> Jps</span><br><span class="line"><span class="number">42894</span> JobHistoryServer</span><br><span class="line"><span class="number">43583</span> SparkSubmit</span><br><span class="line"><span class="number">42191</span> NameNode</span><br><span class="line">========hadoop101==========</span><br><span class="line"><span class="number">37904</span> Jps</span><br><span class="line"><span class="number">37169</span> NodeManager</span><br><span class="line"><span class="number">36818</span> ResourceManager</span><br><span class="line"><span class="number">36610</span> DataNode</span><br><span class="line"><span class="number">37843</span> YarnCoarseGrainedExecutorBackend</span><br><span class="line"><span class="number">37721</span> ExecutorLauncher</span><br><span class="line">========hadoop102==========</span><br><span class="line"><span class="number">37362</span> DataNode</span><br><span class="line"><span class="number">37509</span> SecondaryNameNode</span><br><span class="line"><span class="number">37626</span> NodeManager</span><br><span class="line"><span class="number">38093</span> Jps</span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223505654.png" alt="image-20240804223505654"></p>
<p>配置历史服务</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir hdfs:<span class="comment">//hadoop100:8020/directory</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line">export SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=18080 -Dspark.history.fs.logDirectory=hdfs://hadoop100:8020/directory -Dspark.history.retainedApplications=30&quot;</span></span><br><span class="line"></span><br><span class="line">vim spark-defaults.conf</span><br><span class="line">spark.yarn.historyServer.address=hadoop100:<span class="number">18080</span></span><br><span class="line">spark.history.ui.port=<span class="number">18080</span></span><br></pre></td></tr></table></figure>
<p>hdfs 中创建 /directory</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223520765.png" alt="image-20240804223520765"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">bin/spark-submit --<span class="keyword">class</span> <span class="title class_">org</span>.apache.spark.examples.SparkPi --master yarn ./examples/jars/spark-examples_2<span class="number">.12</span>-<span class="number">3.4</span><span class="number">.3</span>.jar <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">将spark历史记录保存到了hadoop history中</span><br></pre></td></tr></table></figure>
<p>运行时，会将 yarn 需要用到的 lib 和 conf 上传到 hdfs 中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">14</span> <span class="number">06</span>:<span class="number">32</span>:<span class="number">57</span> INFO Client: Uploading resource file:/tmp/spark-4dbf287c-<span class="number">9986</span>-44f0-89c1-7ede052800c0/__spark_libs__5882056198812431005.zip -&gt; hdfs:<span class="comment">//hadoop100:8020/user/root/.sparkStaging/application_1718182778487_0005/__spark_libs__5882056198812431005.zip</span></span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">14</span> <span class="number">06</span>:<span class="number">32</span>:<span class="number">58</span> INFO Client: Uploading resource file:/tmp/spark-4dbf287c-<span class="number">9986</span>-44f0-89c1-7ede052800c0/__spark_conf__3646583648306474590.zip -&gt; hdfs:<span class="comment">//hadoop100:8020/user/root/.sparkStaging/application_1718182778487_0005/__spark_conf__.zip</span></span><br></pre></td></tr></table></figure>
<p>运行结束会删除文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">14</span> <span class="number">06</span>:<span class="number">33</span>:<span class="number">07</span> INFO ShutdownHookManager: Deleting directory /tmp/spark-f3d8debb-<span class="number">7304</span>-<span class="number">4251</span>-b80b-11b1ab91f45c</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">14</span> <span class="number">06</span>:<span class="number">33</span>:<span class="number">07</span> INFO ShutdownHookManager: Deleting directory /tmp/spark-4dbf287c-<span class="number">9986</span>-44f0-89c1-7ede052800c0</span><br></pre></td></tr></table></figure>
<p>yarn 模式中有 client 和 cluster 模式，主要区别在于：Driver 程序的运行节点。</p>
<p>yarn-client:Driver 程序运行在客户端，适用于交互、调试，希望立即看到 Japp 的输出。</p>
<p>yarn-cluster:Driver 程序运行在由 ResourceManager 启动的 APPIMaster, 适用于生产环境。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223534005.png" alt="image-20240804223534005"></p>
<p>默认使用的客户端模式</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 使用cluster模式</span><br><span class="line">bin/spark-submit --<span class="keyword">class</span> <span class="title class_">org</span>.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2<span class="number">.12</span>-<span class="number">3.4</span><span class="number">.3</span>.jar <span class="number">10</span></span><br><span class="line"></span><br><span class="line">========hadoop100==========</span><br><span class="line"><span class="number">42388</span> DataNode</span><br><span class="line"><span class="number">51081</span> Jps</span><br><span class="line"><span class="number">42683</span> NodeManager</span><br><span class="line"><span class="number">44508</span> HistoryServer</span><br><span class="line"><span class="number">42894</span> JobHistoryServer</span><br><span class="line"><span class="number">50959</span> SparkSubmit</span><br><span class="line"><span class="number">42191</span> NameNode</span><br><span class="line">========hadoop101==========</span><br><span class="line"><span class="number">37169</span> NodeManager</span><br><span class="line"><span class="number">36818</span> ResourceManager</span><br><span class="line"><span class="number">36610</span> DataNode</span><br><span class="line"><span class="number">40596</span> Jps</span><br><span class="line">========hadoop102==========</span><br><span class="line"><span class="number">37362</span> DataNode</span><br><span class="line"><span class="number">37509</span> SecondaryNameNode</span><br><span class="line"><span class="number">42121</span> Jps</span><br><span class="line"><span class="number">37626</span> NodeManager</span><br><span class="line"><span class="number">41995</span> ApplicationMaster</span><br></pre></td></tr></table></figure>
<h4 id="standalone模式"><a class="markdownIt-Anchor" href="#standalone模式">#</a> standalone 模式</h4>
<p>Standalone 模式是 Spark 自带的资源调度引擎，构建一个由 Master+VVorker 构成的 Spark 集群，Spark 运行在集群中。</p>
<p>这个要和 Hadoop 中的 Standalone 区别开来。这里的 Standalone 是指只用 Spark 来搭建一个集群，不需要借助 Hadoop 的 Yarn 和 Mesos 等其他框架。</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223542959.png" alt="image-20240804223542959"></p>
<h4 id="mesos模式"><a class="markdownIt-Anchor" href="#mesos模式">#</a> mesos 模式</h4>
<p>Spark 客户端直接连接 Mesos; 不需要额外构建 Spark 集群。国内应用比较少，更多的是运用 Yarn 调度。</p>
<h4 id="模式对比"><a class="markdownIt-Anchor" href="#模式对比">#</a> 模式对比</h4>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223557692.png" alt="image-20240804223557692"></p>
<h4 id="端口号"><a class="markdownIt-Anchor" href="#端口号">#</a> 端口号</h4>
<p>1) Spark 查看当前 Spark-shell 运行任务情况端口号：4040</p>
<p>2) Spark 历史服务器端口号：18080 (类比于 Hadoop 历史服务器端口号：19888)</p>
<h3 id="rdd"><a class="markdownIt-Anchor" href="#rdd">#</a> rdd</h3>
<p>RDD: 分布式计算模型</p>
<p>1. 一定是一个对象</p>
<p>2. 一定封装了大量方法和属性</p>
<p>3. 一定需要适合进行分布式处理 (减小数据规模，并行计算算)</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223609055.png" alt="image-20240804223609055"></p>
<h4 id="rdd编程"><a class="markdownIt-Anchor" href="#rdd编程">#</a> RDD 编程</h4>
<p>在 Spark 中创建 RDD 的创建方式可以分为三种：从集合中创建] RDD、从外部存储创建 RDD、从其他 RDD 创建。</p>
<p>RDD 的处理方式和 JavalO 流完全一样，也采用装饰者设计式来实现功能的</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2<span class="number">.12</span>&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">3.3</span><span class="number">.1</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Hello world!&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建spark配置对象</span></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建spark运行环境</span></span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建spark运行环境</span></span><br><span class="line">        <span class="comment">//JavaSparkContext javaSparkContext = new JavaSparkContext(&quot;local&quot;,&quot;spark&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 释放资源</span></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="对接内存数据构建rdd对象"><a class="markdownIt-Anchor" href="#对接内存数据构建rdd对象">#</a> 对接内存数据构建 RDD 对象</h4>
<p>parallelize 方法可以传递参数：集合</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">package org.<span class="property">example</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">api</span>.<span class="property">java</span>.<span class="property">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">api</span>.<span class="property">java</span>.<span class="property">JavaSparkContext</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.<span class="property">util</span>.<span class="property">Arrays</span>;</span><br><span class="line"><span class="keyword">import</span> java.<span class="property">util</span>.<span class="property">List</span>;</span><br><span class="line"></span><br><span class="line">public <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    public <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span>(<span class="params"><span class="built_in">String</span>[] args</span>) &#123;</span><br><span class="line">        <span class="title class_">System</span>.<span class="property">out</span>.<span class="title function_">println</span>(<span class="string">&quot;Hello world!&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建spark配置对象</span></span><br><span class="line">        <span class="title class_">SparkConf</span> sparkConf = <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.<span class="title function_">setMaster</span>(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.<span class="title function_">setAppName</span>(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建spark运行环境</span></span><br><span class="line">        <span class="title class_">JavaSparkContext</span> javaSparkContext = <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建spark运行环境</span></span><br><span class="line">        <span class="comment">//JavaSparkContext javaSparkContext = new JavaSparkContext(&quot;local&quot;,&quot;spark&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对接数据源</span></span><br><span class="line">        <span class="title class_">List</span>&lt;<span class="title class_">String</span>&gt; names = <span class="title class_">Arrays</span>.<span class="title function_">asList</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;lisi&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="title class_">JavaRDD</span>&lt;<span class="title class_">String</span>&gt; rdd = javaSparkContext.<span class="title function_">parallelize</span>(names);</span><br><span class="line"></span><br><span class="line">        <span class="title class_">List</span>&lt;<span class="title class_">String</span>&gt; collect = rdd.<span class="title function_">collect</span>();</span><br><span class="line"></span><br><span class="line">        collect.<span class="title function_">forEach</span>(<span class="title class_">System</span>.<span class="property">out</span>::println);</span><br><span class="line">        <span class="comment">// 释放资源</span></span><br><span class="line">        javaSparkContext.<span class="title function_">close</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="对接磁盘数据"><a class="markdownIt-Anchor" href="#对接磁盘数据">#</a> 对接磁盘数据</h4>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">class</span> <span class="title class_">Main1</span> &#123;</span><br><span class="line">    public <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span>(<span class="params"><span class="built_in">String</span>[] args</span>) &#123;</span><br><span class="line">        <span class="title class_">System</span>.<span class="property">out</span>.<span class="title function_">println</span>(<span class="string">&quot;Hello world!&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建spark配置对象</span></span><br><span class="line">        <span class="title class_">SparkConf</span> sparkConf = <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.<span class="title function_">setMaster</span>(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.<span class="title function_">setAppName</span>(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建spark运行环境</span></span><br><span class="line">        <span class="title class_">JavaSparkContext</span> javaSparkContext = <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line">        <span class="title class_">JavaRDD</span>&lt;<span class="title class_">String</span>&gt; stringJavaRDD = javaSparkContext.<span class="title function_">textFile</span>(<span class="string">&quot;C:\\Users\\Administrator\\IdeaProjects\\MapReduceDemo\\data\\text&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="title class_">List</span>&lt;<span class="title class_">String</span>&gt; collect = stringJavaRDD.<span class="title function_">collect</span>();</span><br><span class="line">        collect.<span class="title function_">forEach</span>(<span class="title class_">System</span>.<span class="property">out</span>::println);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 释放资源</span></span><br><span class="line">        javaSparkContext.<span class="title function_">close</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="磁盘数据分区"><a class="markdownIt-Anchor" href="#磁盘数据分区">#</a> 磁盘数据分区</h4>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">partition</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Hello world!&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建spark配置对象</span></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建spark运行环境</span></span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// idea开发时相对路径默认以项目根路径为基准</span></span><br><span class="line">        <span class="comment">//JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(&quot;C:\\Users\\Administrator\\IdeaProjects\\MapReduceDemo\\data\\text&quot;);</span></span><br><span class="line">        <span class="comment">// textfile 第二个参数最小分区数，不传递的时候使用默认值</span></span><br><span class="line">        <span class="comment">// 1.textFile可以传递第二个参数:minPartitions(最小分区数)</span></span><br><span class="line">        <span class="comment">//参数可以不需要传递的,那么Spark会采用默认值</span></span><br><span class="line">        <span class="comment">//minPartitions = math.min(defaultParallelism,2)</span></span><br><span class="line">        <span class="comment">//2.使用配置参数:spark.default.parallelism=&gt;4=&gt; 4=&gt; math.min(参数,2)</span></span><br><span class="line">        <span class="comment">//3.采用环境默认总核值=&gt;math.min(总核数,2)</span></span><br><span class="line">        </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class="string">&quot;data\\text&quot;</span>);</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        stringJavaRDD.saveAsTextFile(<span class="string">&quot;output1222&quot;</span>);</span><br><span class="line">        <span class="comment">// 释放资源</span></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="内存数据源分区数据分配"><a class="markdownIt-Anchor" href="#内存数据源分区数据分配">#</a> 内存数据源，分区数据分配</h4>
<p>​      数据分配方式       (i*length)/numSlices,(((i + 1) * 1) * length) /numSlices)</p>
<h4 id="磁盘数据源"><a class="markdownIt-Anchor" href="#磁盘数据源">#</a> 磁盘数据源</h4>
<p>Spark 不支持文件操作的。文件操作都是由 Hadoop 完成的</p>
<p>Hadoop 进行文件切片数量的计算和文件数据存储计算规则不样</p>
<p>1. 分区数量计算的时候，考虑的是尽可能的平均：按字节来计算</p>
<p>2. 分区数据的存储是考虑业务数据的完整性：按照行来读取</p>
<p>读取数据时，还需要考虑数据偏移量，偏移量从 0 开始的。</p>
<p>读取数据时，相同的偏移量不能重复读取。</p>
<p>使用 spark 时，数据不能全放一行，会造成数据倾斜</p>
<h4 id="transformation-转换算子"><a class="markdownIt-Anchor" href="#transformation-转换算子">#</a> transformation 转换算子</h4>
<h5 id="map"><a class="markdownIt-Anchor" href="#map">#</a> map</h5>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223623531.png" alt="image-20240804223623531"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Scala</span>语言中可以将无关的数据封装在一起,形成一个整体,称之为元素的组合,简称为【元组】</span><br><span class="line">如果想要访问元组中的数据,必须采用顺序号</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> kv1 = (<span class="string">&quot;haha&quot;</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        <span class="type">JDK1</span><span class="number">.8</span>以后也存在元组,采用特殊的类:<span class="type">TupleX</span></span><br><span class="line"></span><br><span class="line">        <span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">String</span>&gt; tuple2 = <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(<span class="string">&quot;abc&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="type">System</span>.out.println(tuple2._1);</span><br><span class="line">        <span class="type">System</span>.out.println(tuple2._2());</span><br><span class="line">        tuple中最大容量为<span class="number">22</span></span><br><span class="line">        使用时可以 ._1 也可以 ._1()</span><br></pre></td></tr></table></figure>
<h6 id="函数式编程"><a class="markdownIt-Anchor" href="#函数式编程">#</a> 函数式编程</h6>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.rdd;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">operator3</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line">        javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">2</span>).map(NumberTest::mul2).collect().forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NumberTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span>  <span class="type">int</span> <span class="title function_">mul2</span><span class="params">(Integer num2)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> num2 *= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223648424.png" alt="image-20240804223648424"></p>
<p>RDD 不会保存数据，不会等每一个 rdd 执行完再执行下一个</p>
<h5 id="filter"><a class="markdownIt-Anchor" href="#filter">#</a> filter</h5>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">filter</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO filter 过滤 对数据源中数据进行筛选  满足保留，不满足丢弃</span></span><br><span class="line">        <span class="comment">// 返回结果为true满足 ，返回false不满足</span></span><br><span class="line"><span class="comment">//        JavaRDD&lt;Integer&gt; filter = parallelize111.filter(new Function&lt;Integer, Boolean&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public Boolean call(Integer num) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                return true;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;);</span></span><br><span class="line"><span class="comment">//        filter.collect().forEach(System.out::println);</span></span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Integer&gt; filterrdd = parallelize111.filter(</span><br><span class="line">                num -&gt; <span class="literal">true</span></span><br><span class="line">        );</span><br><span class="line"><span class="comment">//        JavaRDD&lt;Integer&gt; filterrdd111 = parallelize111.filter(</span></span><br><span class="line"><span class="comment">//                num -&gt; (num % 2 ==1)</span></span><br><span class="line"><span class="comment">//        );</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">/// filter 执行过程中可能会造成数据倾斜</span></span><br><span class="line">        filterrdd.collect().forEach(System.out::println);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="flatmap"><a class="markdownIt-Anchor" href="#flatmap">#</a> flatmap</h5>
<p>数据扁平化，扁平映射（整体变为个体）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">flatmap</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>), Arrays.asList(<span class="number">3</span>, <span class="number">4</span>)), <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        JavaRDD&lt;Integer&gt; integerJavaRDD = parallelizerdd.flatMap(new FlatMapFunction&lt;List&lt;Integer&gt;, Integer&gt;() &#123;</span></span><br><span class="line"><span class="comment">//                                                                     @Override</span></span><br><span class="line"><span class="comment">//                                                                     public Iterator&lt;Integer&gt; call(List&lt;Integer&gt; integers) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                                                                         return integers.iterator();</span></span><br><span class="line"><span class="comment">//                                                                     &#125;</span></span><br><span class="line"><span class="comment">//                                                                 &#125;</span></span><br><span class="line"><span class="comment">//        );</span></span><br><span class="line"></span><br><span class="line">                JavaRDD&lt;Integer&gt; integerJavaRDD = parallelizerdd.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;List&lt;Integer&gt;, Integer&gt;() &#123;</span><br><span class="line">                                                                     <span class="meta">@Override</span></span><br><span class="line">                                                                     <span class="keyword">public</span> Iterator&lt;Integer&gt; <span class="title function_">call</span><span class="params">(List&lt;Integer&gt; integers)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                                                         List&lt;Integer&gt; objectArrayList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                                                                         integers.forEach(num -&gt; objectArrayList.add(num * <span class="number">2</span>));</span><br><span class="line">                                                                         <span class="keyword">return</span> objectArrayList.iterator();</span><br><span class="line">                                                                     &#125;</span><br><span class="line">                                                                 &#125;</span><br><span class="line">        );</span><br><span class="line">        integerJavaRDD.collect().forEach(System.out::println);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">data/text：</span><br><span class="line">hadoop python</span><br><span class="line">java php golang</span><br><span class="line">B</span><br><span class="line">V</span><br><span class="line">D</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">flatmap</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>), Arrays.asList(<span class="number">3</span>, <span class="number">4</span>)), <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class="string">&quot;data/text&quot;</span>);</span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class="line">                line -&gt; Arrays.asList(line.split(<span class="string">&quot; &quot;</span>)).iterator()</span><br><span class="line">        );</span><br><span class="line">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="groupby"><a class="markdownIt-Anchor" href="#groupby">#</a> groupby</h5>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">groupby</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span> ,<span class="number">6</span>), <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照指定的规则分组数据</span></span><br><span class="line"><span class="comment">//        parallelizerdd.groupBy(new Function&lt;Integer, Object&gt;() &#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//            public Object call(Integer integers) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                // 返回值是数据对应组的名称，相同名称的数据防止在同一个组中</span></span><br><span class="line"><span class="comment">//                if (integers % 2 == 0)</span></span><br><span class="line"><span class="comment">//                &#123;return &quot;123&quot;;&#125;</span></span><br><span class="line"><span class="comment">//                else &#123;return &quot;456&quot;;&#125;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;).collect().forEach(System.out::println);</span></span><br><span class="line"><span class="comment">//(123,[2, 4, 6])</span></span><br><span class="line"><span class="comment">//(456,[1])</span></span><br><span class="line">        parallelizerdd.groupBy(num -&gt; num % <span class="number">2</span> == <span class="number">0</span>).collect().forEach(System.out::println);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="shuffle"><a class="markdownIt-Anchor" href="#shuffle">#</a> shuffle</h5>
<p>默认情况下，数据处理后，所在的分区不会发生变化，但是 groupBy 方法例外</p>
<p>Spark 在数据处理中，要求同一个组的数据必须在同一个分区中</p>
<p>所以分组操作会将数据分区打乱重新组合，在 spark 中称为 shuffle</p>
<p>一个分区可以存放多个组，，所有数据必须分组后才能继续执行操作</p>
<p>RDD 对象不能保存数据，当前 groupBy 操作会将数据保存到磁盘文件中，保证数据全部分组后执行后续操作</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223703724.png" alt="image-20240804223703724"></p>
<p>shuffle 操作一定会落盘</p>
<p>shuffle 操作有可能会导致资源浪费</p>
<p>Spark 中含有 shuffle 操作的方法都有改变分区的能力</p>
<p>RDD 的分区和 task 有关系</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">groupby2</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span> ,<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="number">3</span>);</span><br><span class="line">        parallelizerdd.groupBy(num -&gt; num % <span class="number">2</span> == <span class="number">0</span>,<span class="number">2</span>).collect().forEach(System.out::println);</span><br><span class="line">        Thread.sleep(<span class="number">100000L</span>);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>shuffle 会将完整的计算流程一分为二，其中一部分任务会写磁盘，另外一部分的任务会读磁盘</p>
<p>写磁盘的操作不完成，不允许读磁盘</p>
<h5 id="distinct"><a class="markdownIt-Anchor" href="#distinct">#</a> distinct</h5>
<p>hashset 是单点去重</p>
<p>distinct 是分布式去重，采用分组 + shuffle 方式</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">distinct</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span> ,<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">2</span>), <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        parallelizerdd.distinct().collect().forEach(System.out::println);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//parallelizerdd.groupBy(num -&gt; num % 2 == 0,2).collect().forEach(System.out::println);</span></span><br><span class="line">        <span class="comment">//Thread.sleep(100000L);</span></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="sortby"><a class="markdownIt-Anchor" href="#sortby">#</a> sortby</h5>
<p>按照指定规则排序</p>
<p>第一个参数表示排序规则</p>
<p>Spark 会为每一个数据增加一个标记，然后按照标记对数据进行排序</p>
<p>第二个参数表示排序的方式：升序 (true), 降序 (false)</p>
<p>第三个参数表示分区数量</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sortby</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(<span class="number">12</span>, <span class="number">52</span>, <span class="number">4</span> ,<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">2</span>), <span class="number">3</span>);</span><br><span class="line">        parallelizerdd.saveAsTextFile(<span class="string">&quot;sort222&quot;</span>);</span><br><span class="line"></span><br><span class="line">        parallelizerdd.sortBy(<span class="keyword">new</span> <span class="title class_">Function</span>&lt;Integer, Object&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> Object <span class="title function_">call</span><span class="params">(Integer integerssss)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="keyword">return</span> integerssss;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,<span class="literal">true</span>,<span class="number">2</span>)</span><br><span class="line">               <span class="comment">// .collect()</span></span><br><span class="line">                <span class="comment">//.forEach(System.out::println);</span></span><br><span class="line">                .saveAsTextFile(<span class="string">&quot;sort333&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//parallelizerdd.groupBy(num -&gt; num % 2 == 0,2).collect().forEach(System.out::println);</span></span><br><span class="line">        <span class="comment">//Thread.sleep(100000L);</span></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>return “”+integerssss;    // 按照字典排序</p>
<h5 id="coalesce"><a class="markdownIt-Anchor" href="#coalesce">#</a> coalesce</h5>
<p>缩减分区</p>
<p>coalesce 方法默认没有 shuffle 功能，所以数据不会被打击乱重新组合，所以如果要扩大分区是无法实现的</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filterrdd.coalesce(<span class="number">3</span>,<span class="literal">true</span>);    <span class="comment">// shuffle 为true的时候可以扩大分区</span></span><br></pre></td></tr></table></figure>
<h5 id="repartition"><a class="markdownIt-Anchor" href="#repartition">#</a> repartition</h5>
<p>重分区，就是设定 shuffle=true 的 coalesce 方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filterrdd.repartition(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h4 id="kv"><a class="markdownIt-Anchor" href="#kv">#</a> kv</h4>
<p>Spark RDD 会整体数据的处理就称之为单值类型的数据处理</p>
<p>Spark RDD 会 KV 数据个体的处理就称之为 KV 类型的数据处理：K 和 V 不作为整体使用</p>
<p>mapValues 方法只对 V 进行处理，K 不做任何操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">kv</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        <span class="comment">//JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(1, 2, 4 ,6, 7, 8, 1, 2), 3);</span></span><br><span class="line"></span><br><span class="line">        Tuple2&lt;String, Integer&gt; a = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; b = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Arrays.asList(a, b);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; parallelized = javaSparkContext.parallelizePairs(list);</span><br><span class="line">        parallelized.mapValues(num -&gt; num*<span class="number">2</span>).collect().forEach(System.out::println);</span><br><span class="line"></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">kv2</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        <span class="comment">//JavaRDD &lt;Integer&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(1, 2, 4 ,6, 7, 8, 1, 2), 3);</span></span><br><span class="line"></span><br><span class="line">        List&lt;Integer&gt; integerList = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Integer&gt; integerJavaRDD = javaSparkContext.parallelize(integerList);</span><br><span class="line"></span><br><span class="line">        integerJavaRDD.mapToPair(</span><br><span class="line">                num -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(num, num*<span class="number">2</span>)</span><br><span class="line">        ).collect().forEach(System.out::println);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        integerJavaRDD.mapToPair(</span></span><br><span class="line"><span class="comment">//                num -&gt; new Tuple2&lt;&gt;(num, num*2)</span></span><br><span class="line"><span class="comment">//        ).mapValues(num-&gt;num*2).collect().forEach(System.out::println);</span></span><br><span class="line"></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="mapvalue"><a class="markdownIt-Anchor" href="#mapvalue">#</a> mapvalue</h5>
<h5 id="groupbykey"><a class="markdownIt-Anchor" href="#groupbykey">#</a> groupbykey</h5>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">kv3_groupbykey</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; parallelized = javaSparkContext.parallelize(Arrays.asList(new Tuple2&lt;&gt;(&quot;a&quot;, 1), new Tuple2&lt;&gt;(&quot;b&quot;, 2), new Tuple2&lt;&gt;(&quot;a&quot;, 3), new Tuple2&lt;&gt;(&quot;b&quot;, 4)));</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        JavaPairRDD&lt;String, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; stringIterableJavaPairRDD = parallelized.groupBy(t -&gt; t._1);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        stringIterableJavaPairRDD.collect().forEach(System.out::println);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// (a,[(a,1), (a,3)])</span></span><br><span class="line">        <span class="comment">//(b,[(b,2), (b,4)])</span></span><br><span class="line"></span><br><span class="line">       javaSparkContext.parallelizePairs((Arrays.asList(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)))).groupByKey().collect().forEach(System.out::println);</span><br><span class="line">       javaSparkContext.close();</span><br><span class="line"></span><br><span class="line"><span class="comment">//(a,[1, 3])</span></span><br><span class="line"><span class="comment">//(b,[2, 4])</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>groupby 底层调用 greoupbykey</p>
<p>groupbykey 有 shuffle</p>
<h5 id="reducebykey"><a class="markdownIt-Anchor" href="#reducebykey">#</a> reducebykey</h5>
<p>reduceByKey 方法的作用：将 KV 类型的数据按照 K 对 V 进行 reduce (将多个值聚合成 1 个值) 操作</p>
<p>基本思想：两两计算</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String ,Integer&gt; wordcountrdd = parallelized.reduceByKey(Integer::sum);</span><br></pre></td></tr></table></figure>
<h5 id="sortbykey"><a class="markdownIt-Anchor" href="#sortbykey">#</a> sortbykey</h5>
<p>groupByKey : 按照 K 对 V 进行分组</p>
<p>reduceByKey 按照 K 对进行两两聚合</p>
<p>sortByKey 按照 K 排序</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String ,Integer&gt; sortrdd = parallelized.sortByKey();</span><br></pre></td></tr></table></figure>
<h4 id="rdd行动算子"><a class="markdownIt-Anchor" href="#rdd行动算子">#</a> RDD 行动算子</h4>
<p>RDD 的行动算子会触发作业 (Job) 的执行</p>
<p>转换算子的目的：将旧的 RDD 转换成新的 RDD, 为了组合多个 RDD 的功能</p>
<p>返回值是 rdd，是转换算子。具体值是行动算子</p>
<p>collect 将 executor 执行的结果按照分区的数据拉取回到 driver，将结果组合成集合对象</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; integerJavaRDD = parallelize111.map(num -&gt; num * <span class="number">2</span>);</span><br><span class="line"><span class="comment">// collect 是行动算子</span></span><br><span class="line">List&lt;Integer&gt; collect = integerJavaRDD.collect();</span><br><span class="line">collect.forEach(System.out::println);</span><br></pre></td></tr></table></figure>
<p>Spark 在编写代码时，调用转换算子，并不会真正执行，因为只是在 Driver 端组合功能</p>
<p>所以当前的代码其实就是在 Driver 端执行</p>
<p>所以当前 main 方法也称之为 driver 方法，当前运行 main 纟我程，也称之 Driver 线程</p>
<p>转换算子中的逻辑代码是在 Executor 端执行的。并不会在 tDriver 端调用和执行。</p>
<p>RDD 封装的逻辑其实就是转换算子中的逻辑</p>
<p>集合数据</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223718854.png" alt="image-20240804223718854"></p>
<p>文件：读取切片</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223732201.png" alt="image-20240804223732201"></p>
<p>collect 方法就是将 Executor 端执行的结果按照分区的顺序位取 (采集) 回到 Driver 端，将结果组合成集合对象</p>
<p>collect 方法可能会导致多个 Executor 的大量数据拉取到 Driiver 端，导致内存溢出，所以生成环境慎用</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>),<span class="number">2</span>);</span><br><span class="line"> JavaRDD&lt;Integer&gt; integerJavaRDD = parallelize111.map(num -&gt; num * <span class="number">2</span>);</span><br><span class="line"> <span class="comment">// collect 是行动算子,用于采集数据</span></span><br><span class="line"> List&lt;Integer&gt; collect = integerJavaRDD.collect();</span><br><span class="line"> collect.forEach(System.out::println);</span><br><span class="line"> <span class="comment">// count获取结果数量</span></span><br><span class="line"> <span class="type">long</span> <span class="variable">count</span> <span class="operator">=</span> integerJavaRDD.count();</span><br><span class="line"> <span class="comment">// 获取结果的第一个</span></span><br><span class="line"> <span class="type">Integer</span> <span class="variable">first</span> <span class="operator">=</span> integerJavaRDD.first();</span><br><span class="line"> <span class="comment">// 从结果中获取前n个</span></span><br><span class="line"> List&lt;Integer&gt; take = integerJavaRDD.take(<span class="number">3</span>);</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> <span class="comment">// countbykey 将结果按照key计算数量</span></span><br><span class="line"> <span class="comment">// &#123;4=1, 2=1, 1=1, 3=1&#125;</span></span><br><span class="line"> JavaPairRDD&lt;Integer, Integer&gt; integerIntegerJavaPairRDD = parallelize111.mapToPair(num -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(num, num));</span><br><span class="line"> Map&lt;Integer, Long&gt; integerLongMap = integerIntegerJavaPairRDD.countByKey();</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 保存</span></span><br><span class="line"> integerIntegerJavaPairRDD.saveAsTextFile(<span class="string">&quot;a&quot;</span>);</span><br><span class="line"> <span class="comment">// 保存对象</span></span><br><span class="line"> integerIntegerJavaPairRDD.saveAsObjectFile(<span class="string">&quot;bb&quot;</span>);</span><br><span class="line"> <span class="comment">// 单线循环</span></span><br><span class="line"> parallelize111.collect().forEach(System.out::println);</span><br><span class="line"> </span><br><span class="line"> <span class="comment">// 分布式循环，占内存比较小，执行效率低</span></span><br><span class="line"> parallelize111.foreach(</span><br><span class="line">         System.out::println</span><br><span class="line"> );</span><br><span class="line"> <span class="comment">// 执行效率高，依托于内存大小</span></span><br><span class="line"> parallelize111.foreachPartition(System.out::println);</span><br><span class="line"></span><br><span class="line"> javaSparkContext.close();</span><br></pre></td></tr></table></figure>
<p>main 方法也叫 driver 方法，foreach 是在 executor 执行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">action_serialize</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Integer&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Student1</span> <span class="variable">s</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Student1</span>();</span><br><span class="line"></span><br><span class="line">        parallelize111.foreach(</span><br><span class="line">                num -&gt; &#123;</span><br><span class="line">                    System.out.println(s.age + num);</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Student1</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="variable">age</span> <span class="operator">=</span> <span class="number">30</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Student 想 new ，得在 executor 上拉取</p>
<p>在 Executor 端循环遍历的时候使用到了 Driver 端对象</p>
<p>运行过程中，就需要将 Driver 端的对象通过网络传递到 Executor 端，否则无法使用</p>
<p>传输的对象必须要实现可序列化接口，否则无法传递</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">        JavaRDD&lt;String&gt; parallelize111 = javaSparkContext.parallelize(Arrays.asList(<span class="string">&quot;haha&quot;</span>,<span class="string">&quot;Haha&quot;</span>), <span class="number">2</span>);</span><br><span class="line">        <span class="type">Search</span> <span class="variable">search</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Search</span>(<span class="string">&quot;H&quot;</span>);</span><br><span class="line">        search.match(parallelize111);</span><br><span class="line"></span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Search</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> String query;            <span class="comment">// query是成员变量，需要序列化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Search</span><span class="params">(String query)</span>&#123;</span><br><span class="line">        <span class="built_in">this</span>.query = query;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span>  <span class="title function_">match</span><span class="params">(JavaRDD&lt;String&gt; rdd)</span> &#123;</span><br><span class="line">        rdd.filter(</span><br><span class="line">                s -&gt; s.startsWith(query)</span><br><span class="line">        ).collect().forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">class Search implements Serializable &#123;</span></span><br><span class="line"><span class="comment">    private String query;            // query是成员变量，需要序列化</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    public Search(String query)&#123;</span></span><br><span class="line"><span class="comment">        this.query = query;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    public void  match(JavaRDD&lt;String&gt; rdd) &#123;</span></span><br><span class="line"><span class="comment">        String q = this.query;         // q是局部变量，在栈中，和search无关，不需要序列化</span></span><br><span class="line"><span class="comment">        rdd.filter(</span></span><br><span class="line"><span class="comment">                s -&gt; s.startsWith(q)</span></span><br><span class="line"><span class="comment">        ).collect().forEach(System.out::println);</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure>
<p>rdd 算子 (方法) 的逻辑代码是在 executo 执行的，其他的是在 driver 执行的</p>
<p>collect 是行动算子，没有逻辑代码</p>
<p>filter 中的成为逻辑代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//        parallelize111.foreach(</span></span><br><span class="line"><span class="comment">//             executor端执行</span></span><br><span class="line"><span class="comment">//                num -&gt; System.out.println(num)</span></span><br><span class="line"><span class="comment">//        );</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// jdk1.8 函数式编程采用对象模拟,使用这种方式会报错，但是系统的类无法改写继承序列化</span></span><br><span class="line">        parallelize111.foreach(</span><br><span class="line">        <span class="comment">// 在driver端创建</span></span><br><span class="line">                System.out::println</span><br><span class="line">                <span class="comment">// PrintStream out = System.out;</span></span><br><span class="line">                <span class="comment">// out::println</span></span><br><span class="line">        );</span><br></pre></td></tr></table></figure>
<h4 id="kryo"><a class="markdownIt-Anchor" href="#kryo">#</a> kryo</h4>
<p>Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kryo&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">5.0</span><span class="number">.3</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.User;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test02_Kryo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 1.创建配置对象</span></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkCore&quot;</span>)</span><br><span class="line">                <span class="comment">// 替换默认的序列化机制</span></span><br><span class="line">                .set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">                <span class="comment">// 注册需要使用kryo序列化的自定义类</span></span><br><span class="line">                .registerKryoClasses(<span class="keyword">new</span> <span class="title class_">Class</span>[]&#123;Class.forName(<span class="string">&quot;com.atguigu.bean.User&quot;</span>)&#125;);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 2. 创建sparkContext</span></span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 3. 编写代码</span></span><br><span class="line">        <span class="type">User</span> <span class="variable">zhangsan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">User</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">13</span>);</span><br><span class="line">        <span class="type">User</span> <span class="variable">lisi</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">User</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">13</span>);</span><br><span class="line"> </span><br><span class="line">        JavaRDD&lt;User&gt; userJavaRDD = sc.parallelize(Arrays.asList(zhangsan, lisi), <span class="number">2</span>);</span><br><span class="line"> </span><br><span class="line">        JavaRDD&lt;User&gt; mapRDD = userJavaRDD.map(<span class="keyword">new</span> <span class="title class_">Function</span>&lt;User, User&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> User <span class="title function_">call</span><span class="params">(User v1)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">User</span>(v1.getName(), v1.getAge() + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"> </span><br><span class="line">        mapRDD. collect().forEach(System.out::println);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 4. 关闭sc</span></span><br><span class="line">        sc.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="依赖"><a class="markdownIt-Anchor" href="#依赖">#</a> 依赖</h4>
<p>RDD 转换算子 (方法):RDD 可以通过方法将旧的 RDD 转换成新的 RDD</p>
<p>RDD 依赖：Spark 中相邻的 2 个 RDD 之间存在的依赖关系</p>
<p>连续的依赖关系称为血缘关系</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223744879.png" alt="image-20240804223744879"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">flatmap</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        sparkConf.setAppName(<span class="string">&quot;spark&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">        <span class="comment">//JavaRDD&lt;List&lt;Integer&gt;&gt; parallelizerdd = javaSparkContext.parallelize(Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4)), 2);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//System.out.println(parallelizerdd.toDebugString());</span></span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class="string">&quot;data/text&quot;</span>);</span><br><span class="line">        System.out.println(stringJavaRDD.toDebugString());</span><br><span class="line">        System.out.println(<span class="string">&quot;============&quot;</span>);</span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class="line">                line -&gt; Arrays.asList(line.split(<span class="string">&quot; &quot;</span>)).iterator()</span><br><span class="line">        );</span><br><span class="line">        System.out.println(stringJavaRDD1.toDebugString());</span><br><span class="line">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(<span class="number">1</span>) data/text MapPartitionsRDD[<span class="number">1</span>] at textFile at flatmap.java:<span class="number">20</span> []</span><br><span class="line"> |  data/text HadoopRDD[<span class="number">0</span>] at textFile at flatmap.java:<span class="number">20</span> []</span><br><span class="line">============</span><br><span class="line">(<span class="number">1</span>) MapPartitionsRDD[<span class="number">2</span>] at flatMap at flatmap.java:<span class="number">23</span> []</span><br><span class="line"> |  data/text MapPartitionsRDD[<span class="number">1</span>] at textFile at flatmap.java:<span class="number">20</span> []</span><br><span class="line"> |  data/text HadoopRDD[<span class="number">0</span>] at textFile at flatmap.java:<span class="number">20</span> []</span><br><span class="line"> </span><br><span class="line"> shuffle +-  代表两段流程</span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = javaSparkContext.textFile(<span class="string">&quot;data/text&quot;</span>);</span><br><span class="line"><span class="comment">//        System.out.println(stringJavaRDD.toDebugString());</span></span><br><span class="line">        System.out.println(stringJavaRDD.rdd().dependencies());</span><br><span class="line">        System.out.println(<span class="string">&quot;============&quot;</span>);</span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD1 = stringJavaRDD.flatMap(</span><br><span class="line">                line -&gt; Arrays.asList(line.split(<span class="string">&quot; &quot;</span>)).iterator()</span><br><span class="line">        );</span><br><span class="line"><span class="comment">//        System.out.println(stringJavaRDD1.toDebugString());</span></span><br><span class="line">        System.out.println(stringJavaRDD1.rdd().dependencies());</span><br><span class="line"></span><br><span class="line">        stringJavaRDD1.groupBy(num -&gt; num);</span><br><span class="line">        System.out.println(stringJavaRDD1.rdd().dependencies());</span><br><span class="line">        stringJavaRDD1.collect().forEach(System.out::println);</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">List(org.apache.spark.OneToOneDependency@4b5a078a)</span><br><span class="line">============</span><br><span class="line">List(org.apache.spark.OneToOneDependency@33f2df51)</span><br><span class="line"><span class="number">24</span>/<span class="number">06</span>/<span class="number">25</span> <span class="number">17</span>:<span class="number">16</span>:<span class="number">52</span> INFO FileInputFormat: Total input files to process : <span class="number">1</span></span><br><span class="line">List(org.apache.spark.OneToOneDependency@33f2df51)</span><br></pre></td></tr></table></figure>
<p>onetoonedep 窄依赖</p>
<p>shuffledep  宽依赖</p>
<p>rdd 的依赖关系是 rdd 对象中分区数据的关系</p>
<p>窄依赖：如果上游 rdd 一个分区的数据被下游 rdd 的一个分区独享</p>
<p>宽依赖：如果上游 rdd 一个分区的数据被下游 rdd 的多个分区共享。会将分区数据打乱重新组合，所以此层存在 shuffle 操作</p>
<p>依赖关系和任务数量，阶段数量</p>
<p>作业 (Job): 行动算子执行时，会触发作业的执行 (ActiveJob)</p>
<p>阶段 (Stage): 一个 job 中 RDD 的计算流程，默认就一个完整的阶段，但是如果计算流程中存在 shuffle, 那么流程就会一分为二。分开的每一段就称之为 stage，前一个阶段不执行完，后一个阶段不允许执行</p>
<p>任务 (Task): 每个 Executor 执行的计算单元</p>
<p>阶段的数量和 shuffle 依赖的数量有关系：1+shuffle 依赖的数量</p>
<p>任务（分区）的数量就是每个阶段分区的数量之和，一般推荐分区数量为资源核数的 2-3 倍</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223805444.png" alt="image-20240804223805444"></p>
<p>任务 (Task): 每个 Executor 执行的计算单元</p>
<p>任务的数量其实就是每个阶段最后一个 RDD 分区的数量之和</p>
<p>移动数据不如移动计算</p>
<h4 id="持久化"><a class="markdownIt-Anchor" href="#持久化">#</a> 持久化</h4>
<p>持久化：将对象长时间的保存</p>
<p>序列化：内存中对象 =&gt;byte 序列 (byte 数组)</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223856875.png" alt="image-20240804223856875"></p>
<p>maprdd.cache();</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223912457.png" alt="image-20240804223912457"></p>
<p>maprdd.persist(Storage.MEMORY_ONLY());</p>
<p>cache 方法底层调用 persist.  maprdd.persist (Storage.MEMORY_ONLY ());  ===  cache ()</p>
<p>// 落盘持久化</p>
<p>maprdd.persist(Storage.DISK_ONLY());</p>
<p>MEMORY_ONLY 超出数据直接丢弃</p>
<p>MEMORY_AND_DISK.  内存满了放磁盘</p>
<p>MEMORY_ONLY_SER 序列化后再存内存</p>
<p>MEMORY_ONLY_SER_2. 副本 2 份</p>
<h5 id="checkpoint"><a class="markdownIt-Anchor" href="#checkpoint">#</a> checkpoint</h5>
<p>将计算结果保存到 hdfs 或者本地文件路径，实现不同进程共享</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkConf);</span><br><span class="line">javaSparkContext.setCheckpointDir(<span class="string">&quot;cp&quot;</span>);</span><br><span class="line"></span><br><span class="line">mapRDD.cache();</span><br><span class="line">mapRDD.checkPoint();</span><br></pre></td></tr></table></figure>
<p>检查点目的是 rdd 结果长时间保存，所以需要保证数据安全，会从头再跑一遍。把第二遍结果放到里面</p>
<p>性能比较低，可以在检查点之前执行 cache，将数据缓存</p>
<p>cache 方法会在血缘关系中增加依赖关系</p>
<p>checkpoint 方法改变血缘关系</p>
<p>每个 shuffle 都自动带有缓存，为了提高 shuffle 算子的性能</p>
<p>如果重复调用相同规则的 shuffle 算子，第二个 shfulle 算子不会有相同 shuffle 操作</p>
<p>使用完缓存，可以使用 unpersist 释放缓存</p>
<h4 id="分区器"><a class="markdownIt-Anchor" href="#分区器">#</a> 分区器</h4>
<p>数据分区的规则</p>
<p>计算后数据所在的分区是通过 Spark 的内部计算 (分区) 完成我。尽可能均衡一些（hash）</p>
<p>reducebykey 需要传递两个参数，第一个参数是数据分区的规则，第二个参数是数据聚合逻辑</p>
<p>第一个参数可以不用传递，使用时会使用默认分区规则。默认分区规则中使用 HashPartitioner</p>
<p>hashpartitioner 中有一个方法叫 getpartition，需要传递一个参数 key，返回一个值，表示分区编号，从 0 开始</p>
<p>得到一个分区编号，key.hashcode % partnum (hash 取余)</p>
<p>只有 kv 类型的有分区器</p>
<p>rdd.partitioner()</p>
<p>Spark 目前支持 Hash 分区、Range 分区和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区和 Reduce 的个数。</p>
<p>自定义分区器</p>
<p>1. 创建自定义类</p>
<p>2. 继承抽象类 Partitioner</p>
<p>3. 重写方法 (2)</p>
<p>4. 构建对象，在算子中使用</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Part</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 指定分区数量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">numPartitions</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 很久数据的key来获取数据存储的分区编号</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Object key)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="string">&quot;a&quot;</span>.equals(key))</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;b&quot;</span>.equals(key)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mapRDD.reduceByKey(<span class="keyword">new</span> <span class="title class_">Part</span>(),Integer::sum)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Part</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> numPart;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Part</span><span class="params">(<span class="type">int</span> num)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.numPart = num;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 指定分区数量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">numPartitions</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 很久数据的key来获取数据存储的分区编号</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Object key)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="string">&quot;a&quot;</span>.equals(key))</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;b&quot;</span>.equals(key)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> numPart;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object obj)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Part) &#123;</span><br><span class="line">            <span class="type">Part</span> <span class="variable">other</span> <span class="operator">=</span> (Part)obj;</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">this</span>.numPart == other.numPart;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>RDD 在 foreach 循环时，逻辑代码和操作全部都是在 Executor 端完成的，那么结果不会拉取回到 Driver 端</p>
<p>RDD 无法实现数据拉取操作</p>
<p>如果 Executor 端使用了 Driver 端数据，那么需要从 Driver 端将数据拉取到 Executor 端</p>
<p>数据拉取的单位是 Task (任务)</p>
<p>默认数据传输以 Task 为单位进行传输，如果想要以 Executor 为单位传输，那么需要进行包装 (封装)</p>
<p>Spark 需要采用特殊的数据模型实现数据传输：广播变量</p>
<p>jsc.broadcast(list);</p>
<p>rdd.filter(s -&gt; broadcast.value())</p>
<h4 id="sparksql"><a class="markdownIt-Anchor" href="#sparksql">#</a> sparksql</h4>
<p>Spark SQL: 结构化数据处理模块</p>
<p>SQL: 为了数据库数据访问开发的语言</p>
<p>Spark 封装模块的目的就是在结构化数据的场合，处理起来方便</p>
<p>结构化数据：特殊结构的数据 =&gt;(table,json)</p>
<p>半结构化数据:xml,html</p>
<p>非结构化数据：压缩文件，图形文件，视频，音频文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2<span class="number">.12</span>&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">3.3</span><span class="number">.1</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">envv</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//        SparkConf sparkConf = new SparkConf();</span></span><br><span class="line"><span class="comment">//        sparkConf.setMaster(&quot;local&quot;);</span></span><br><span class="line"><span class="comment">//        sparkConf.setAppName(&quot;SparkSQL&quot;);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        SparkContext sc = new SparkContext(sparkConf);</span></span><br><span class="line"><span class="comment">//        SparkSession sparkSession = new SparkSession(sc);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        sparkSession.close();</span></span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line">        <span class="comment">//rowDataset.rdd();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将数据模型转换成表</span></span><br><span class="line">        rowDataset.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用sql文的方式操作</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select avg(age) from user&quot;</span>;</span><br><span class="line">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 展示数据模型效果</span></span><br><span class="line">        rowDataset1.show();</span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="环境之间转换"><a class="markdownIt-Anchor" href="#环境之间转换">#</a> 环境之间转换</h5>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// core:sparkcontext -&gt; sql:sparksession</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">sparkSession</span>( <span class="keyword">new</span> <span class="title class_">SparkContect</span>(conf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// sql -&gt; core:sparkcontext</span></span><br><span class="line">sparksession.sparkContext().parallelize();</span><br><span class="line"></span><br><span class="line"><span class="comment">//sql -&gt; core:javasparkcontext</span></span><br><span class="line">        <span class="type">SparkContext</span> <span class="variable">sparkContext</span> <span class="operator">=</span> sparkSession.sparkContext();</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">javaSparkContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(sparkContext);</span><br><span class="line">        javaSparkContext.parallelize(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line">        <span class="comment">//rowDataset.rdd();</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//rowDataset.foreach(</span></span><br><span class="line"><span class="comment">//        row -&gt;&#123;</span></span><br><span class="line"> <span class="comment">//           System.out.println(row.getInt(2));</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据模型中的数据类型进行转换，将row转换成其他对象处理</span></span><br><span class="line">        Dataset&lt;User&gt; userDataset = rowDataset.as(Encoders.bean(User.class));</span><br><span class="line">        </span><br><span class="line">        userDataset.foreach(</span><br><span class="line">                user -&gt; &#123;</span><br><span class="line">                    System.out.println(user.getname());</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">User</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> id;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    User(<span class="type">int</span> id, <span class="type">int</span> age, String name) &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">        <span class="built_in">this</span>.age = age;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">model</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        rowDataset.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>);</span><br><span class="line"></span><br><span class="line">        sparkSession.sql(<span class="string">&quot;select * from user&quot;</span>).show();</span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">model2</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        rowDataset.select(<span class="string">&quot;*&quot;</span>).show();</span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="自定义udf"><a class="markdownIt-Anchor" href="#自定义udf">#</a> 自定义 UDF</h5>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_3</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line">        <span class="comment">//rowDataset.rdd();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将数据模型转换成表</span></span><br><span class="line">        rowDataset.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用sql文的方式操作</span></span><br><span class="line">        <span class="comment">// SparkSQL提供了一种特殊的方式,可以在SQL中增加自定义方法来实现复杂的逻辑</span></span><br><span class="line">        <span class="comment">//如果想要自定义的方法能够在SQL中使用,那么必须在SPark中进行声明和注册</span></span><br><span class="line">        <span class="comment">// register方法需要传递3个参数</span></span><br><span class="line">        <span class="comment">//第一个参数表示SQL中使用的方法名</span></span><br><span class="line">        <span class="comment">//第二个参数表示逻辑:IN=&gt;OUT</span></span><br><span class="line">        <span class="comment">//第三个参数表示返回的数据类型  ,DataType类型数据，需要使用scala语法操作</span></span><br><span class="line">        sparkSession.udf().register(<span class="string">&quot;prefixName&quot;</span>, <span class="keyword">new</span> <span class="title class_">UDF1</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;Name:&quot;</span> + s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, StringType$.MODULE$);</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select prefixName(name) from user&quot;</span>;</span><br><span class="line">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 展示数据模型效果</span></span><br><span class="line">        rowDataset1.show();</span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.types.DataTypes.StringType;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_udf</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line">        <span class="comment">//rowDataset.rdd();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将数据模型转换成表</span></span><br><span class="line">        rowDataset.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用sql文的方式操作</span></span><br><span class="line">        <span class="comment">// SparkSQL提供了一种特殊的方式,可以在SQL中增加自定义方法来实现复杂的逻辑</span></span><br><span class="line">        <span class="comment">//如果想要自定义的方法能够在SQL中使用,那么必须在SPark中进行声明和注册</span></span><br><span class="line">        <span class="comment">// register方法需要传递3个参数</span></span><br><span class="line">        <span class="comment">//第一个参数表示SQL中使用的方法名</span></span><br><span class="line">        <span class="comment">//第二个参数表示逻辑:IN=&gt;OUT</span></span><br><span class="line">        <span class="comment">//第三个参数表示返回的数据类型  ,DataType类型数据，需要使用scala语法操作</span></span><br><span class="line"><span class="comment">//        sparkSession.udf().register(&quot;prefixName&quot;, new UDF1&lt;String, String&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public String call(String s) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                return &quot;Name:&quot; + s;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;, DataTypes.StringType);</span></span><br><span class="line"></span><br><span class="line">        sparkSession.udf().register(<span class="string">&quot;prefixName&quot;</span>, <span class="keyword">new</span> <span class="title class_">UDF1</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;Name:&quot;</span> + s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, StringType);</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select prefixName(name) from user&quot;</span>;</span><br><span class="line">        Dataset&lt;Row&gt; rowDataset1 = sparkSession.sql(sql);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 展示数据模型效果</span></span><br><span class="line">        rowDataset1.show();</span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>UDF 函数是每一行数据会都用一次函数</p>
<p>UDAF 函数是所有的数据产生一个结果数据</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804223942980.png" alt="image-20240804223942980"></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224006080.png" alt="image-20240804224006080"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.sparksql;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AvgAgeBuffer</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> total;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> count;</span><br><span class="line"></span><br><span class="line">    AvgAgeBuffer() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    AvgAgeBuffer(<span class="type">long</span> total, <span class="type">long</span> count) &#123;</span><br><span class="line">        <span class="built_in">this</span>.total = total;</span><br><span class="line">        <span class="built_in">this</span>.count = count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getCount</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setCount</span><span class="params">(<span class="type">long</span> count)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.count = count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getTotal</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setTotal</span><span class="params">(<span class="type">long</span> total)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.total = total;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.example.sparksql;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义UDAF函数,实现年龄的平均值</span></span><br><span class="line"><span class="comment">//1.创建自定义的【公共】类</span></span><br><span class="line"><span class="comment">//2.继承 org.apache.spark.sql.expressions.Aggreegator</span></span><br><span class="line"><span class="comment">//3.设定泛型</span></span><br><span class="line"><span class="comment">//IN:输入数据类型</span></span><br><span class="line"><span class="comment">//BUFF:缓冲区的数据类型</span></span><br><span class="line"><span class="comment">//OUT:输出数据类型</span></span><br><span class="line"><span class="comment">//4.重写方法(4(计算)+2(状态))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyAvgAgeUDAF</span> <span class="keyword">extends</span> <span class="title class_">Aggregator</span>&lt;Long, AvgAgeBuffer, Long&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 缓冲区初始化操作</span></span><br><span class="line">    <span class="keyword">public</span> AvgAgeBuffer <span class="title function_">zero</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">AvgAgeBuffer</span>(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 把输入年龄和缓冲器数据聚合</span></span><br><span class="line">    <span class="keyword">public</span> AvgAgeBuffer <span class="title function_">reduce</span><span class="params">(AvgAgeBuffer buffer, Long age)</span> &#123;</span><br><span class="line">        buffer.setTotal(buffer.getTotal() + age);</span><br><span class="line">        buffer.setCount(buffer.getCount()+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> buffer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 合并缓冲区数据</span></span><br><span class="line">    <span class="keyword">public</span> AvgAgeBuffer <span class="title function_">merge</span><span class="params">(AvgAgeBuffer b1, AvgAgeBuffer b2)</span> &#123;</span><br><span class="line">        b1.setTotal(b1.getTotal()+b2.getTotal());</span><br><span class="line">        b1.setCount(b1.getCount()+b2.getCount());</span><br><span class="line">        <span class="keyword">return</span> b1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 计算最终结果</span></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">finish</span><span class="params">(AvgAgeBuffer reduction)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reduction.getTotal() / reduction.getCount();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Encoder&lt;AvgAgeBuffer&gt; <span class="title function_">bufferEncoder</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Encoders.bean(AvgAgeBuffer.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Encoder&lt;Long&gt; <span class="title function_">outputEncoder</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Encoders.LONG();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.example.sparksql;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_udaf</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// Spark在结构化数据的处理场景中对核心功能,环境进行了封装</span></span><br><span class="line">        <span class="comment">// 构建SparkSQL的环境对象时,一般采用构建器模式</span></span><br><span class="line">        <span class="comment">// 构建器模式:构建对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spark SQL中对数据模型也进行了封装:RDD -&gt;Dataset</span></span><br><span class="line">        <span class="comment">// 对接文件数据源时，会讲文件中一行数据封装为row对象</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line">        <span class="comment">//rowDataset.rdd();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将数据模型转换成表</span></span><br><span class="line">        rowDataset.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//SparkSQL采用特殊的方式将UDAF转换成UDF使用</span></span><br><span class="line"><span class="comment">//UDAF使用时需要创建自定义聚合对象</span></span><br><span class="line">        <span class="comment">// 两个恶参数，第一个UADF对象，第二个表示UADF对象</span></span><br><span class="line">        sparkSession.udf().register(<span class="string">&quot;avgage&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="title class_">MyAvgAgeUDAF</span>(),Encoders.LONG()));</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select avgage(age) from user&quot;</span>;</span><br><span class="line">        sparkSession.sql(sql).show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 展示数据模型效果</span></span><br><span class="line"><span class="comment">//        rowDataset1.show();</span></span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="数据加载与保存"><a class="markdownIt-Anchor" href="#数据加载与保存">#</a> 数据加载与保存</h5>
<p>SparkSQL 读取和保存的文件一般为三种，JSON 文件、CSV 文文件和列式存储的文件，同时可以通过添加参数，来识别不同的存储和压缩格式。</p>
<p>csv</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_source</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// csv 文件讲数据采用逗号分隔,可以被excel打开</span></span><br><span class="line">        <span class="comment">// csv 带有header，以,分隔.   可以以, _ \t 分隔      \t时叫做tsv</span></span><br><span class="line">        Dataset&lt;Row&gt; csv = sparkSession.read().option(<span class="string">&quot;header&quot;</span>,<span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;sep&quot;</span>,<span class="string">&quot;,&quot;</span>).csv(<span class="string">&quot;data/user.csv&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// +----+--------+---+</span></span><br><span class="line">        <span class="comment">//| _c0|     _c1|_c2|</span></span><br><span class="line">        <span class="comment">//+----+--------+---+</span></span><br><span class="line">        <span class="comment">//|1001|zhangsan| 30|</span></span><br><span class="line">        <span class="comment">//|1002|    lisi| 31|</span></span><br><span class="line">        <span class="comment">//|1003|  wangwu| 32|</span></span><br><span class="line">        <span class="comment">//+----+--------+---+</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//+----+--------+---+</span></span><br><span class="line">        <span class="comment">//|  id|    name|age|</span></span><br><span class="line">        <span class="comment">//+----+--------+---+</span></span><br><span class="line">        <span class="comment">//|1001|zhangsan| 30|</span></span><br><span class="line">        <span class="comment">//|1002|    lisi| 31|</span></span><br><span class="line">        <span class="comment">//|1003|  wangwu| 32|</span></span><br><span class="line">        <span class="comment">//+----+--------+---+</span></span><br><span class="line">        <span class="comment">//csv.show();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出目录已经存在，默认会发生异常。不希望出错，可以修改配置 保存模式</span></span><br><span class="line">        csv.write().mode(<span class="string">&quot;append&quot;</span>).option(<span class="string">&quot;header&quot;</span>,<span class="string">&quot;true&quot;</span>).csv(<span class="string">&quot;output&quot;</span>); </span><br><span class="line"></span><br><span class="line">        <span class="comment">//  DataFrameWriter var2;</span></span><br><span class="line">        <span class="comment">//        if (&quot;overwrite&quot;.equals(var4)) &#123;</span></span><br><span class="line">        <span class="comment">//            var2 = this.mode(SaveMode.Overwrite);</span></span><br><span class="line">        <span class="comment">//        &#125; else if (&quot;append&quot;.equals(var4)) &#123;</span></span><br><span class="line">        <span class="comment">//            var2 = this.mode(SaveMode.Append);</span></span><br><span class="line">        <span class="comment">//        &#125; else if (&quot;ignore&quot;.equals(var4)) &#123;</span></span><br><span class="line">        <span class="comment">//            var2 = this.mode(SaveMode.Ignore);</span></span><br><span class="line">        <span class="comment">//        &#125; else &#123;</span></span><br><span class="line">        <span class="comment">//            boolean var3;</span></span><br><span class="line">        <span class="comment">//            if (&quot;error&quot;.equals(var4)) &#123;</span></span><br><span class="line">        <span class="comment">//                var3 = true;</span></span><br><span class="line">        <span class="comment">//            &#125; else if (&quot;errorifexists&quot;.equals(var4)) &#123;</span></span><br><span class="line">        <span class="comment">//                var3 = true;</span></span><br><span class="line">        <span class="comment">//            &#125; else if (&quot;default&quot;.equals(var4)) &#123;</span></span><br><span class="line">        <span class="comment">//                var3 = true;</span></span><br><span class="line">        <span class="comment">//            &#125; else &#123;</span></span><br><span class="line">        <span class="comment">//                var3 = false;</span></span><br><span class="line">        <span class="comment">//            &#125;</span></span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>json</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_source_json</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// js object natation</span></span><br><span class="line">        <span class="comment">// 如果是对象，用&#123;&#125;  数组用[]  json文件符合json格式</span></span><br><span class="line">        <span class="comment">//SparkSQL其实就是对Spark Core RDD的封装。RDD读取文件采用用的是Hadoop,hadoop是按行读取。</span></span><br><span class="line">        <span class="comment">//SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可,无需整个文件符合JSON格式</span></span><br><span class="line">        <span class="comment">// 底层是dataset，可以读csv，写json</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().json(<span class="string">&quot;data/user.json&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//rowDataset.show();</span></span><br><span class="line"></span><br><span class="line">        rowDataset.write().json(<span class="string">&quot;output&quot;</span>);</span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>行式存储</p>
<p>存在主键，可以快速定位。查询快，统计慢</p>
<p>列式存储</p>
<p>查询快，统计快</p>
<p>mysql 行存储，hive 列存储</p>
<p>spark 列存储 parquet</p>
<p>列存储可以被压缩，snappy</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ublic <span class="keyword">class</span> <span class="title class_">sql_source_parquet</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// js object natation</span></span><br><span class="line">        <span class="comment">// 如果是对象，用&#123;&#125;  数组用[]  json文件符合json格式</span></span><br><span class="line">        <span class="comment">//SparkSQL其实就是对Spark Core RDD的封装。RDD读取文件采用用的是Hadoop,hadoop是按行读取。</span></span><br><span class="line">        <span class="comment">//SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可,无需整个文件符合JSON格式</span></span><br><span class="line">        <span class="comment">// 底层是dataset，可以读csv，写json</span></span><br><span class="line">        Dataset&lt;Row&gt; rowDataset = sparkSession.read().parquet(<span class="string">&quot;data/user&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//rowDataset.show();</span></span><br><span class="line"></span><br><span class="line">        rowDataset.write().json(<span class="string">&quot;output&quot;</span>);</span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="mysql交互"><a class="markdownIt-Anchor" href="#mysql交互">#</a> mysql 交互</h5>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">8.0</span><span class="number">.18</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_source_mysql</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;sparkSQL&quot;</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;admin&quot;</span>);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;Chaitin@123&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; jdbc = sparkSession.read().jdbc(<span class="string">&quot;jdbc:mysql://hadoop100:3306/metastore?useSSL=false&quot;</span>,<span class="string">&quot;TYPES&quot;</span>,properties);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        jdbc.write().mode(<span class="string">&quot;append&quot;</span>).jdbc(<span class="string">&quot;jdbc:mysql://hadoop100:3306/metastore?useSSL=false&quot;</span>,<span class="string">&quot;TYPES_123&quot;</span>,properties);</span><br><span class="line">        jdbc.show();</span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="hive交互"><a class="markdownIt-Anchor" href="#hive交互">#</a> hive 交互</h5>
<p>复制 hive-site.yaml 到 resources 中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-hive_2<span class="number">.12</span>&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">3.3</span><span class="number">.1</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">sql_hive</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 编码前设定hadoop访问用户</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>,<span class="string">&quot;root&quot;</span>);</span><br><span class="line">        <span class="keyword">final</span> <span class="type">SparkSession</span> <span class="variable">sparkSession</span> <span class="operator">=</span> SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                .appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                .getOrCreate();</span><br><span class="line"></span><br><span class="line">        sparkSession.sql(<span class="string">&quot;show tables&quot;</span>).show();</span><br><span class="line"></span><br><span class="line">        sparkSession.sql(<span class="string">&quot;create table user_info(name string, age int)&quot;</span>);</span><br><span class="line">        sparkSession.sql(<span class="string">&quot;insert into  table user_info values (&#x27;haha&#x27;,100)&quot;</span>);</span><br><span class="line">        sparkSession.sql(<span class="string">&quot;select * from user_info&quot;</span>).show();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        sparkSession.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果 reources 中有 hive-site.xml 但是 target/classes 中没有，需要手工拷贝到改目录</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224020783.png" alt="image-20240804224020783"></p>
<h4 id="spark-streaming"><a class="markdownIt-Anchor" href="#spark-streaming">#</a> spark streaming</h4>
<p>有界数据流</p>
<p>无界数据流</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224044892.png" alt="image-20240804224044892"></p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224058079.png" alt="image-20240804224058079"></p>
<p>spark streaming 底层还是 spark core，在流式数据处理中进行的封装</p>
<p>从数据处理方式的角度</p>
<p>流式数据处理：一个数据一个数据的处理</p>
<p>微批量数据处理：一小批数据处理</p>
<p>批量数据处理：一批数据一批数据的处理</p>
<p>从数据处理延迟的角度</p>
<p>实时数据处理：数据处理的延迟以毫秒为单位</p>
<p>准实时处理：以秒和分钟为单位</p>
<p>离线数据处理：数据处理的延迟以小时，天为单位</p>
<p>Spark 是批量，离线数据处理框架</p>
<p>spark streaming 是个 微批量 准实时数据处理框架</p>
<p>streaming 按照时间来定义一小批</p>
<p><img data-src="https://kbshire-1308981697.cos.ap-shanghai.myqcloud.com/img/image-20240804224112151.png" alt="image-20240804224112151"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">env</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkstreaming&quot;</span>);</span><br><span class="line">        <span class="comment">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class="line">        <span class="type">JavaStreamingContext</span> <span class="variable">javaStreamingContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(sparkConf, <span class="keyword">new</span> <span class="title class_">Duration</span>(<span class="number">3000</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动数据采集器</span></span><br><span class="line">        javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class="line">        javaStreamingContext.awaitTermination();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据采集器是长期执行的任务，不能停止，也不能释放资源</span></span><br><span class="line">        <span class="comment">// javaStreamingContext.close();</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>socket</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">python -m http.server <span class="number">8001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">socket</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkstreaming&quot;</span>);</span><br><span class="line">        <span class="comment">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class="line">        <span class="type">JavaStreamingContext</span> <span class="variable">javaStreamingContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(sparkConf, <span class="keyword">new</span> <span class="title class_">Duration</span>(<span class="number">3000</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过环境对象对接socket数据源</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        socketTextStream.print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动数据采集器</span></span><br><span class="line">        javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class="line">        javaStreamingContext.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>kafka</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">kafka</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkstreaming&quot;</span>);</span><br><span class="line">        <span class="comment">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class="line">        <span class="type">JavaStreamingContext</span> <span class="variable">javaStreamingContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(sparkConf, <span class="keyword">new</span> <span class="title class_">Duration</span>(<span class="number">3000</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建配置参数</span></span><br><span class="line">        HashMap&lt;String, Object&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;hadoop102:9092,hadoop103:9092,hadoop104:9092&quot;</span>);</span><br><span class="line">        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        map.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">&quot;latest&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 需要消费的主题</span></span><br><span class="line">        ArrayList&lt;String&gt; strings = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        strings.add(<span class="string">&quot;topic_db&quot;</span>);</span><br><span class="line"></span><br><span class="line">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));</span><br><span class="line"></span><br><span class="line">        directStream.map(<span class="keyword">new</span> <span class="title class_">Function</span>&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; v1)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1.value();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print(<span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动数据采集器</span></span><br><span class="line">        javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class="line">        javaStreamingContext.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Dstream</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">function</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkstreaming&quot;</span>);</span><br><span class="line">        <span class="comment">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class="line">        <span class="type">JavaStreamingContext</span> <span class="variable">javaStreamingContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(sparkConf, <span class="keyword">new</span> <span class="title class_">Duration</span>(<span class="number">3000</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过环境对象对接socket数据源</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        socketTextStream.print();</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; stringJavaDStream = socketTextStream.flatMap(</span><br><span class="line">                line -&gt; Arrays.asList(line.split(<span class="string">&quot; &quot;</span>)).iterator()</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; stringIntegerJavaPairDStream = stringJavaDStream.mapToPair(</span><br><span class="line">                word -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; stringIntegerJavaPairDStream1 = stringIntegerJavaPairDStream.reduceByKey(</span><br><span class="line">                Integer::sum</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        stringIntegerJavaPairDStream1.foreachRDD(</span><br><span class="line">                rdd -&gt; &#123;</span><br><span class="line">                    rdd.sortByKey().collect().forEach(System.out::println);</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 启动数据采集器</span></span><br><span class="line">        javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class="line">        javaStreamingContext.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>window</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">窗口是可以移动的，成为移动窗口，但是窗口移动是有复幅度的，默认移动幅度就是采集周期</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">窗口:其实就是数据的范围(时间)</span><br><span class="line">window方法可以改变窗口的数据范围(默认数据范围为采集周期</span><br><span class="line">window方法可以传递<span class="number">2</span>个参数</span><br><span class="line">第一个参数表示窗口的数据范围(时间)</span><br><span class="line">第二个参数表示窗口的移动幅度(时间),可以不用传递,默认使用的就是采集周期</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">数据窗口范围和窗口移动幅度一致(3s),数据不会有重复</span><br><span class="line"></span><br><span class="line">数据窗口范围比窗口移动幅度大,数据可能会有重复</span><br><span class="line"></span><br><span class="line">数据窗口范围比窗口移动幅度小,数据可能会有遗漏</span><br></pre></td></tr></table></figure>
<p>close</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">close</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkstreaming&quot;</span>);</span><br><span class="line">        <span class="comment">// Spark在流式数据的处理场景中对核心功能环境进行了封装</span></span><br><span class="line">        <span class="type">JavaStreamingContext</span> <span class="variable">javaStreamingContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(sparkConf, <span class="keyword">new</span> <span class="title class_">Duration</span>(<span class="number">3000</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过环境对象对接socket数据源</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; socketTextStream = javaStreamingContext.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        socketTextStream.print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动数据采集器</span></span><br><span class="line">        javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 不能在main中关闭</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 关闭SparkStreaming的时候,需要在程序运行的过程中,通过外部操作进行关闭</span></span><br><span class="line">                    Thread.sleep(<span class="number">000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">///javaStreamingContext.close();           // 强制关闭</span></span><br><span class="line">                javaStreamingContext.stop(<span class="literal">true</span>,<span class="literal">true</span>);       <span class="comment">//graceful stop</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class="line">        javaStreamingContext.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 关闭SparkStreaming的时候,需要在程序运行的过程中,通过外部操作进行关闭</span></span><br><span class="line">                    Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">                    <span class="comment">// 使用zk，redis，mysql，hdfs实现中转</span></span><br><span class="line">                    <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">///javaStreamingContext.close();           // 强制关闭</span></span><br><span class="line">                javaStreamingContext.stop(<span class="literal">true</span>,<span class="literal">true</span>);       <span class="comment">//graceful stop</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待数据采集器的结束,如果采集器停止运行,那么main线程会继续续执行</span></span><br><span class="line">        javaStreamingContext.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:8020&quot;</span>), <span class="keyword">new</span> <span class="title class_">Configuration</span>(), <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line">                <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">                    Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">                    <span class="type">boolean</span> <span class="variable">exists</span> <span class="operator">=</span> fs.exists(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;hdfs://hadoop102:8020/stopSpark&quot;</span>));</span><br><span class="line">                    <span class="keyword">if</span> (exists)&#123;</span><br><span class="line">                        <span class="type">StreamingContextState</span> <span class="variable">state</span> <span class="operator">=</span> javaStreamingContext.getState();</span><br><span class="line">                        <span class="comment">// 获取当前任务是否正在运行</span></span><br><span class="line">                        <span class="keyword">if</span> (state == StreamingContextState.ACTIVE)&#123;</span><br><span class="line">                            <span class="comment">// 优雅关闭</span></span><br><span class="line">                            javaStreamingContext.stop(<span class="literal">true</span>, <span class="literal">true</span>);</span><br><span class="line">                            System.exit(<span class="number">0</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      <div class="tags">
          <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag"><i class="ic i-tag"></i> 大数据</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">Edited on</span>
    <time title="Modified: 2024-08-04 23:29:49" itemprop="dateModified" datetime="2024-08-04T23:29:49+08:00">2024-08-04</time>
  </span>
</div>

      
<div class="reward">
  <button><i class="ic i-heartbeat"></i> Donate</button>
  <p>Give me a cup of [coffee]~(￣▽￣)~*</p>
  <div id="qr">
      
      <div>
        <img data-src="/images/wechatpay.png" alt="John Doe WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div>
        <img data-src="/images/alipay.png" alt="John Doe Alipay">
        <p>Alipay</p>
      </div>
      
      <div>
        <img data-src="/images/paypal.png" alt="John Doe PayPal">
        <p>PayPal</p>
      </div>
  </div>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>Post author:  </strong>John Doe <i class="ic i-at"><em>@</em></i>Hexo
  </li>
  <li class="link">
    <strong>Post link: </strong>
    <a href="http://example.com/2024/04/01/spark/" title="spark">http://example.com/2024/04/01/spark/</a>
  </li>
  <li class="license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/2024/03/01/hadoop_new/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;img.timelessq.com&#x2F;images&#x2F;2022&#x2F;07&#x2F;26&#x2F;f93f124fd90670f86b81841581c3698c.jpg" title="Hadoop">
  <span class="type">Previous Post</span>
  <span class="category"><i class="ic i-flag"></i> 大数据</span>
  <h3>Hadoop</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="/2024/05/01/hive_flink/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;img.timelessq.com&#x2F;images&#x2F;2022&#x2F;07&#x2F;26&#x2F;08909482bc207d17564c06ef2496e12f.jpg" title="hive_flink">
  <span class="type">Next Post</span>
  <span class="category"><i class="ic i-flag"></i> 大数据</span>
  <h3>hive_flink</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="Contents">
          <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#spark"><span class="toc-number">1.</span> <span class="toc-text"> spark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2spark%E9%9B%86%E7%BE%A4"><span class="toc-number">1.1.</span> <span class="toc-text"> 部署 spark 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2local"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 部署 local</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#yarn%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.2.</span> <span class="toc-text"> yarn 模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#standalone%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.3.</span> <span class="toc-text"> standalone 模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mesos%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.4.</span> <span class="toc-text"> mesos 模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="toc-number">1.1.5.</span> <span class="toc-text"> 模式对比</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="toc-number">1.1.6.</span> <span class="toc-text"> 端口号</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rdd"><span class="toc-number">1.2.</span> <span class="toc-text"> rdd</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#rdd%E7%BC%96%E7%A8%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text"> RDD 编程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%8E%A5%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BArdd%E5%AF%B9%E8%B1%A1"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 对接内存数据构建 RDD 对象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%8E%A5%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.3.</span> <span class="toc-text"> 对接磁盘数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 磁盘数据分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%86%E5%8C%BA%E6%95%B0%E6%8D%AE%E5%88%86%E9%85%8D"><span class="toc-number">1.2.5.</span> <span class="toc-text"> 内存数据源，分区数据分配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">1.2.6.</span> <span class="toc-text"> 磁盘数据源</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformation-%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="toc-number">1.2.7.</span> <span class="toc-text"> transformation 转换算子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#map"><span class="toc-number">1.2.7.1.</span> <span class="toc-text"> map</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B"><span class="toc-number">1.2.7.1.1.</span> <span class="toc-text"> 函数式编程</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#filter"><span class="toc-number">1.2.7.2.</span> <span class="toc-text"> filter</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#flatmap"><span class="toc-number">1.2.7.3.</span> <span class="toc-text"> flatmap</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#groupby"><span class="toc-number">1.2.7.4.</span> <span class="toc-text"> groupby</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#shuffle"><span class="toc-number">1.2.7.5.</span> <span class="toc-text"> shuffle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#distinct"><span class="toc-number">1.2.7.6.</span> <span class="toc-text"> distinct</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sortby"><span class="toc-number">1.2.7.7.</span> <span class="toc-text"> sortby</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#coalesce"><span class="toc-number">1.2.7.8.</span> <span class="toc-text"> coalesce</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#repartition"><span class="toc-number">1.2.7.9.</span> <span class="toc-text"> repartition</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kv"><span class="toc-number">1.2.8.</span> <span class="toc-text"> kv</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#mapvalue"><span class="toc-number">1.2.8.1.</span> <span class="toc-text"> mapvalue</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#groupbykey"><span class="toc-number">1.2.8.2.</span> <span class="toc-text"> groupbykey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#reducebykey"><span class="toc-number">1.2.8.3.</span> <span class="toc-text"> reducebykey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sortbykey"><span class="toc-number">1.2.8.4.</span> <span class="toc-text"> sortbykey</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rdd%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="toc-number">1.2.9.</span> <span class="toc-text"> RDD 行动算子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kryo"><span class="toc-number">1.2.10.</span> <span class="toc-text"> kryo</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96"><span class="toc-number">1.2.11.</span> <span class="toc-text"> 依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-number">1.2.12.</span> <span class="toc-text"> 持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#checkpoint"><span class="toc-number">1.2.12.1.</span> <span class="toc-text"> checkpoint</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">1.2.13.</span> <span class="toc-text"> 分区器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sparksql"><span class="toc-number">1.2.14.</span> <span class="toc-text"> sparksql</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E4%B9%8B%E9%97%B4%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.2.14.1.</span> <span class="toc-text"> 环境之间转换</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89udf"><span class="toc-number">1.2.14.2.</span> <span class="toc-text"> 自定义 UDF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-number">1.2.14.3.</span> <span class="toc-text"> 数据加载与保存</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#mysql%E4%BA%A4%E4%BA%92"><span class="toc-number">1.2.14.4.</span> <span class="toc-text"> mysql 交互</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hive%E4%BA%A4%E4%BA%92"><span class="toc-number">1.2.14.5.</span> <span class="toc-text"> hive 交互</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-streaming"><span class="toc-number">1.2.15.</span> <span class="toc-text"> spark streaming</span></a></li></ol></li></ol></li></ol>
      </div>
      <div class="related panel pjax" data-title="Related">
        <ul>
          <li><a href="/2024/03/01/hadoop_new/" rel="bookmark" title="Hadoop">Hadoop</a></li><li class="active"><a href="/2024/04/01/spark/" rel="bookmark" title="spark">spark</a></li><li><a href="/2024/05/01/hive_flink/" rel="bookmark" title="hive_flink">hive_flink</a></li><li><a href="/2024/10/01/clickhouse/" rel="bookmark" title="Clickhouse">Clickhouse</a></li><li><a href="/2024/11/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E6%A6%82%E8%AE%BA/" rel="bookmark" title="大数据生态概论">大数据生态概论</a></li>
        </ul>
      </div>
      <div class="overview panel" data-title="Overview">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="John Doe"
      data-src="/images/avatar.jpg">
  <p class="name" itemprop="name">John Doe</p>
  <div class="description" itemprop="description"></div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">54</span>
        <span class="name">posts</span>
      </a>
    </div>
    <div class="item categories">
      <a href="/categories/">
        <span class="count">7</span>
        <span class="name">categories</span>
      </a>
    </div>
    <div class="item tags">
      <a href="/tags/">
        <span class="count">7</span>
        <span class="name">tags</span>
      </a>
    </div>
</nav>

<div class="social">
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>Home</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="/2024/03/01/hadoop_new/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="/2024/05/01/hive_flink/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>Random Posts</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2022/02/28/XSS/" title="XSS">XSS</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E8%BF%90%E7%BB%B4/" title="In 运维">运维</a>
</div>

    <span><a href="/2024/01/25/%E8%99%9A%E6%8B%9F%E5%8C%96/" title="虚拟化">虚拟化</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E7%A7%8B%E6%8B%9B/" title="In 秋招">秋招</a>
</div>

    <span><a href="/2022/10/27/%E7%A7%8B%E6%8B%9B%E6%80%BB%E7%BB%93/" title="秋招总结">秋招总结</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" title="In 大数据">大数据</a>
</div>

    <span><a href="/2024/11/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E6%A6%82%E8%AE%BA/" title="大数据生态概论">大数据生态概论</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E8%BF%90%E7%BB%B4/" title="In 运维">运维</a>
</div>

    <span><a href="/2023/05/20/Kubernetes/" title="Kubernetes">Kubernetes</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/web%E5%AE%89%E5%85%A8/" title="In web安全">web安全</a>
</div>

    <span><a href="/2022/04/16/Upload/" title="File upload">File upload</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E8%BF%90%E7%BB%B4/" title="In 运维">运维</a>
</div>

    <span><a href="/2023/05/30/jenkins/" title="Jenkins">Jenkins</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E9%80%86%E5%90%91/" title="In 逆向">逆向</a>
</div>

    <span><a href="/2023/01/03/%E6%BB%B4%E6%B0%B4reverse/" title="水滴rev">水滴rev</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" title="In 大数据">大数据</a>
</div>

    <span><a href="/2024/03/01/hadoop_new/" title="Hadoop">Hadoop</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E7%BD%91%E7%BB%9C/" title="In 网络">网络</a>
</div>

    <span><a href="/2023/01/20/VXLAN/" title="VXLAN">VXLAN</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>Recent Comments</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">John Doe @ Yume Shoka</span>
  </div>
  <div class="powered-by">
    Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2024/04/01/spark/',
    favicon: {
      show: "（●´3｀●）Goooood",
      hide: "(´Д｀)Booooom"
    },
    search : {
      placeholder: "Search for Posts",
      empty: "We didn't find any results for the search: ${query}",
      stats: "${hits} results found in ${time} ms"
    },
    valine: true,fancybox: true,
    copyright: 'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




</body>
</html>
